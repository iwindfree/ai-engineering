{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RAG 평가(Evaluation) 튜토리얼 - Part 2: LLM-as-a-Judge\n\n## 왜 평가가 RAG 아키텍처보다 더 중요한가?\n\nRAG(Retrieval-Augmented Generation) 시스템을 구축할 때 많은 개발자들이 **아키텍처 선택**에 집중합니다:\n- 어떤 임베딩 모델을 사용할까?\n- 어떤 벡터 데이터베이스가 좋을까?\n- 청킹 전략은 어떻게 할까?\n\n하지만 진짜 중요한 질문은 따로 있습니다:\n\n> **\"우리 RAG 시스템이 실제로 잘 작동하는지 어떻게 알 수 있는가?\"**\n\n### 평가가 중요한 이유\n\n| 관점 | 평가 없이 | 평가 있으면 |\n|------|----------|------------|\n| **개발** | 감으로 튜닝 | 데이터 기반 개선 |\n| **배포** | 불안한 출시 | 자신있는 릴리즈 |\n| **비즈니스** | ROI 측정 불가 | 명확한 성과 지표 |\n| **유지보수** | 회귀 버그 발견 어려움 | 지속적인 품질 모니터링 |\n\n### 비즈니스 관점에서의 평가 필요성\n\n1. **비용 정당화**: RAG 시스템의 성능을 수치로 증명\n2. **리스크 관리**: 잘못된 정보 제공 시 발생하는 비용 최소화\n3. **지속적 개선**: A/B 테스트를 통한 점진적 향상\n4. **고객 신뢰**: 일관된 품질 보장\n\n---\n\n## 학습 목표\n\n이 노트북에서는 다음을 배웁니다:\n\n1. **테스트 데이터 구조** 설계 방법\n2. **답변 평가 (Answer Evaluation)**: LLM-as-a-Judge 패러다임\n3. **실전 평가** 파이프라인 구현\n4. **종합 평가 리포트** 생성\n\n> **참고**: 검색 평가 지표(MRR, nDCG, Keyword Coverage)는 **Part 1 (13-1.rag_evaluation_part1.ipynb)**에서 다룹니다."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "# !pip install pydantic litellm python-dotenv numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API 키 확인\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API 키가 설정되었습니다.\")\n",
    "else:\n",
    "    print(\"경고: OPENAI_API_KEY가 설정되지 않았습니다. .env 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 테스트 데이터 구조\n",
    "\n",
    "### 왜 테스트 데이터가 필요한가?\n",
    "\n",
    "RAG 시스템을 평가하려면 **정답이 있는 테스트 셋**이 필요합니다:\n",
    "\n",
    "```\n",
    "테스트 데이터 = 질문 + 예상 답변 + 관련 문서 정보\n",
    "```\n",
    "\n",
    "좋은 테스트 데이터의 특징:\n",
    "- **다양한 유형**의 질문 포함\n",
    "- **현실적인** 시나리오 반영\n",
    "- **명확한 정답** 존재\n",
    "- **충분한 양** (최소 50-100개 권장)\n",
    "\n",
    "### TestQuestion 클래스 (Pydantic 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 카테고리 타입 정의\n",
    "QuestionCategory = Literal[\n",
    "    \"direct_fact\",    # 직접적인 사실 질문\n",
    "    \"temporal\",       # 시간 관련 질문\n",
    "    \"spanning\",       # 여러 문서에 걸친 질문\n",
    "    \"comparative\",    # 비교 질문\n",
    "    \"numerical\",      # 숫자/수치 질문\n",
    "    \"relationship\",   # 관계 질문\n",
    "    \"holistic\"        # 종합적 이해 질문\n",
    "]\n",
    "\n",
    "\n",
    "class TestQuestion(BaseModel):\n",
    "    \"\"\"\n",
    "    RAG 평가를 위한 테스트 질문 모델\n",
    "    \n",
    "    각 필드 설명:\n",
    "    - id: 질문 고유 식별자\n",
    "    - question: 실제 질문 텍스트\n",
    "    - expected_answer: 예상되는 정답 (참조 답변)\n",
    "    - category: 질문 카테고리\n",
    "    - source_docs: 정답을 찾을 수 있는 문서 ID들\n",
    "    - keywords: 답변에 포함되어야 할 핵심 키워드들\n",
    "    - difficulty: 난이도 (1: 쉬움, 2: 보통, 3: 어려움)\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"질문 고유 식별자\")\n",
    "    question: str = Field(..., description=\"질문 텍스트\")\n",
    "    expected_answer: str = Field(..., description=\"예상 정답\")\n",
    "    category: QuestionCategory = Field(..., description=\"질문 카테고리\")\n",
    "    source_docs: List[str] = Field(default_factory=list, description=\"정답 소스 문서 ID들\")\n",
    "    keywords: List[str] = Field(default_factory=list, description=\"핵심 키워드들\")\n",
    "    difficulty: int = Field(default=1, ge=1, le=3, description=\"난이도 (1-3)\")\n",
    "\n",
    "\n",
    "# 예시 테스트 질문 생성\n",
    "sample_question = TestQuestion(\n",
    "    id=\"q001\",\n",
    "    question=\"Python에서 리스트를 정렬하는 방법은 무엇인가요?\",\n",
    "    expected_answer=\"Python에서 리스트를 정렬하려면 sort() 메서드 또는 sorted() 함수를 사용합니다. sort()는 원본 리스트를 직접 수정하고, sorted()는 새로운 정렬된 리스트를 반환합니다.\",\n",
    "    category=\"direct_fact\",\n",
    "    source_docs=[\"python_basics_001\", \"python_list_002\"],\n",
    "    keywords=[\"sort\", \"sorted\", \"리스트\", \"정렬\"],\n",
    "    difficulty=1\n",
    ")\n",
    "\n",
    "print(\"테스트 질문 예시:\")\n",
    "print(sample_question.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 카테고리 설명\n",
    "\n",
    "다양한 카테고리의 질문으로 테스트해야 RAG 시스템의 전체적인 성능을 파악할 수 있습니다.\n",
    "\n",
    "| 카테고리 | 설명 | 예시 | 권장 비율 |\n",
    "|----------|------|------|----------|\n",
    "| **direct_fact** | 직접적인 사실 질문 | \"Python의 창시자는?\" | 45-50% |\n",
    "| **temporal** | 시간 관련 질문 | \"Python 3.0은 언제 출시됐나?\" | 10-15% |\n",
    "| **spanning** | 여러 문서 연결 | \"Django와 Flask의 차이점은?\" | 10-15% |\n",
    "| **comparative** | 비교 질문 | \"어떤 방법이 더 빠른가?\" | 5-10% |\n",
    "| **numerical** | 수치 관련 | \"최대 연결 수는?\" | 5-10% |\n",
    "| **relationship** | 관계 질문 | \"A는 B와 어떤 관계?\" | 5-10% |\n",
    "| **holistic** | 종합 이해 | \"전체 아키텍처를 설명해주세요\" | 5-10% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 테스트 데이터셋 생성\n",
    "sample_test_data = [\n",
    "    TestQuestion(\n",
    "        id=\"q001\",\n",
    "        question=\"Python에서 리스트를 정렬하는 방법은?\",\n",
    "        expected_answer=\"sort() 메서드 또는 sorted() 함수를 사용합니다.\",\n",
    "        category=\"direct_fact\",\n",
    "        source_docs=[\"doc_001\"],\n",
    "        keywords=[\"sort\", \"sorted\"],\n",
    "        difficulty=1\n",
    "    ),\n",
    "    TestQuestion(\n",
    "        id=\"q002\",\n",
    "        question=\"Python 3.0은 언제 출시되었나요?\",\n",
    "        expected_answer=\"Python 3.0은 2008년 12월 3일에 출시되었습니다.\",\n",
    "        category=\"temporal\",\n",
    "        source_docs=[\"doc_002\"],\n",
    "        keywords=[\"2008\", \"12월\"],\n",
    "        difficulty=1\n",
    "    ),\n",
    "    TestQuestion(\n",
    "        id=\"q003\",\n",
    "        question=\"Django와 Flask의 주요 차이점은 무엇인가요?\",\n",
    "        expected_answer=\"Django는 풀스택 프레임워크로 ORM, 인증 등을 내장하고, Flask는 마이크로 프레임워크로 최소한의 기능만 제공하여 유연성이 높습니다.\",\n",
    "        category=\"spanning\",\n",
    "        source_docs=[\"doc_003\", \"doc_004\"],\n",
    "        keywords=[\"풀스택\", \"마이크로\", \"ORM\", \"유연성\"],\n",
    "        difficulty=2\n",
    "    ),\n",
    "    TestQuestion(\n",
    "        id=\"q004\",\n",
    "        question=\"리스트 컴프리헨션과 for 루프 중 어떤 것이 더 빠른가요?\",\n",
    "        expected_answer=\"일반적으로 리스트 컴프리헨션이 for 루프보다 빠릅니다. 이는 내부적으로 최적화되어 있기 때문입니다.\",\n",
    "        category=\"comparative\",\n",
    "        source_docs=[\"doc_005\"],\n",
    "        keywords=[\"리스트 컴프리헨션\", \"빠르\", \"최적화\"],\n",
    "        difficulty=2\n",
    "    ),\n",
    "    TestQuestion(\n",
    "        id=\"q005\",\n",
    "        question=\"Python의 기본 재귀 깊이 제한은 얼마인가요?\",\n",
    "        expected_answer=\"Python의 기본 재귀 깊이 제한은 1000입니다.\",\n",
    "        category=\"numerical\",\n",
    "        source_docs=[\"doc_006\"],\n",
    "        keywords=[\"1000\", \"재귀\"],\n",
    "        difficulty=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"테스트 데이터셋: {len(sample_test_data)}개 질문\")\n",
    "for q in sample_test_data:\n",
    "    print(f\"  [{q.category}] {q.question[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. 검색 평가 (Retrieval Evaluation)\n\n> **참고**: 검색 평가 지표(MRR, nDCG, Keyword Coverage)와 RetrievalEval 클래스는 \n> **13-1.rag_evaluation_part1.ipynb**에서 상세히 다룹니다.\n> \n> Part1에서 다루는 내용:\n> - MRR (Mean Reciprocal Rank) - 이론용 및 실전용 함수\n> - nDCG (Normalized DCG) - 이론용 및 실전용 함수\n> - Keyword Coverage\n> - RetrievalEval 종합 클래스\n> - evaluate_retrieval 함수\n>\n> 아래 실습에서는 Part1에서 정의한 함수들을 사용합니다."
  },
  {
   "cell_type": "code",
   "source": "# Part1에서 정의된 함수들 (인라인 정의)\n# 이 셀은 Part1을 먼저 실행하지 않아도 이 노트북을 독립적으로 실행할 수 있도록 합니다.\n\ndef calculate_dcg(relevances: List[int], k: int = None) -> float:\n    \"\"\"DCG 계산\"\"\"\n    if k is None:\n        k = len(relevances)\n    \n    dcg = 0.0\n    for i, rel in enumerate(relevances[:k], start=1):\n        dcg += rel / np.log2(i + 1)\n    return dcg\n\n\ndef calculate_mrr_practical(retrieved_docs: List[str], relevant_docs: List[str]) -> float:\n    \"\"\"MRR 계산 - 문서 ID 기반 실전 버전\"\"\"\n    relevant_set = set(relevant_docs)\n    for rank, doc_id in enumerate(retrieved_docs, start=1):\n        if doc_id in relevant_set:\n            return 1.0 / rank\n    return 0.0\n\n\ndef calculate_ndcg_practical(retrieved_docs: List[str], relevant_docs: List[str], k: int = None) -> float:\n    \"\"\"nDCG 계산 - 문서 ID 기반 Binary Relevance 버전\"\"\"\n    if k is None:\n        k = len(retrieved_docs)\n    \n    relevant_set = set(relevant_docs)\n    relevances = [1 if doc in relevant_set else 0 for doc in retrieved_docs[:k]]\n    \n    dcg = calculate_dcg(relevances, k)\n    ideal_relevances = sorted(relevances, reverse=True)\n    idcg = calculate_dcg(ideal_relevances, k)\n    \n    if idcg == 0:\n        return 0.0\n    return dcg / idcg\n\n\ndef calculate_keyword_coverage(retrieved_content: str, keywords: List[str]) -> float:\n    \"\"\"키워드 커버리지 계산\"\"\"\n    if not keywords:\n        return 1.0\n    content_lower = retrieved_content.lower()\n    found_count = sum(1 for kw in keywords if kw.lower() in content_lower)\n    return found_count / len(keywords)\n\n\nclass RetrievalEval(BaseModel):\n    \"\"\"검색 평가 결과 모델\"\"\"\n    mrr: float = Field(..., ge=0, le=1, description=\"Mean Reciprocal Rank\")\n    ndcg: float = Field(..., ge=0, le=1, description=\"Normalized DCG\")\n    keyword_coverage: float = Field(..., ge=0, le=1, description=\"키워드 커버리지\")\n    retrieved_count: int = Field(..., ge=0, description=\"검색된 문서 수\")\n    relevant_found: int = Field(..., ge=0, description=\"찾은 관련 문서 수\")\n    \n    @property\n    def overall_score(self) -> float:\n        \"\"\"종합 점수 (가중 평균)\"\"\"\n        return 0.4 * self.mrr + 0.4 * self.ndcg + 0.2 * self.keyword_coverage\n\n\ndef evaluate_retrieval(\n    retrieved_docs: List[str],\n    relevant_docs: List[str],\n    retrieved_content: str,\n    keywords: List[str],\n    k: int = 5\n) -> RetrievalEval:\n    \"\"\"검색 평가 수행\"\"\"\n    relevant_set = set(relevant_docs)\n    relevant_found = sum(1 for doc in retrieved_docs if doc in relevant_set)\n    \n    return RetrievalEval(\n        mrr=calculate_mrr_practical(retrieved_docs, relevant_docs),\n        ndcg=calculate_ndcg_practical(retrieved_docs, relevant_docs, k),\n        keyword_coverage=calculate_keyword_coverage(retrieved_content, keywords),\n        retrieved_count=len(retrieved_docs),\n        relevant_found=relevant_found\n    )\n\n\nclass AnswerEval(BaseModel):\n    \"\"\"답변 평가 결과 모델\"\"\"\n    accuracy: int = Field(..., ge=1, le=5, description=\"정확도 (1-5)\")\n    completeness: int = Field(..., ge=1, le=5, description=\"완전성 (1-5)\")\n    relevance: int = Field(..., ge=1, le=5, description=\"관련성 (1-5)\")\n    feedback: str = Field(..., description=\"상세 피드백\")\n    \n    @property\n    def average_score(self) -> float:\n        \"\"\"평균 점수\"\"\"\n        return (self.accuracy + self.completeness + self.relevance) / 3\n    \n    @property\n    def normalized_score(self) -> float:\n        \"\"\"정규화 점수 (0-1)\"\"\"\n        return (self.average_score - 1) / 4\n\n\ndef evaluate_answer_with_llm(\n    question: str,\n    generated_answer: str,\n    reference_answer: str,\n    model: str = \"gpt-4o-mini\"\n) -> AnswerEval:\n    \"\"\"LLM을 사용하여 답변 평가\"\"\"\n    from litellm import completion\n    \n    prompt = f\"\"\"당신은 RAG 시스템의 답변 품질을 평가하는 전문 평가자입니다.\n\n질문: {question}\n\n생성된 답변: {generated_answer}\n\n참조 답변: {reference_answer}\n\n다음 기준으로 1-5점 (1: 매우 나쁨, 5: 매우 좋음) 평가해주세요:\n1. 정확도 (accuracy): 사실적으로 얼마나 정확한가?\n2. 완전성 (completeness): 질문의 모든 측면을 다루는가?\n3. 관련성 (relevance): 질문에 직접적으로 답변하는가?\n\n반드시 아래 JSON 형식으로만 응답하세요:\n{{\"accuracy\": 점수, \"completeness\": 점수, \"relevance\": 점수, \"feedback\": \"상세 피드백\"}}\"\"\"\n    \n    response = completion(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        response_format={\"type\": \"json_object\"}\n    )\n    \n    result = json.loads(response.choices[0].message.content)\n    return AnswerEval(**result)\n\n\nprint(\"평가 함수들이 로드되었습니다.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. 답변 평가 (Answer Evaluation)\n\n검색이 잘 되었더라도, **최종 답변의 품질**이 좋아야 합니다.\n\n### 3.1 LLM-as-a-Judge 패러다임\n\n**핵심 아이디어**: LLM을 평가자로 사용하여 답변 품질 측정\n\n#### 장점\n- **스케일러블**: 대량의 평가를 자동화\n- **일관성**: 동일 기준으로 평가\n- **세밀한 피드백**: 왜 좋은지/나쁜지 설명 가능\n- **다양한 기준**: 정확도, 완전성, 관련성 등 여러 측면 평가\n\n#### 단점\n- **비용**: API 호출 비용 발생\n- **편향 가능성**: LLM 자체의 편향 반영\n- **일관성 한계**: 같은 입력에 다른 결과 가능\n\n### 3.2 평가 기준 3가지\n\n| 기준 | 설명 | 예시 질문 |\n|------|------|----------|\n| **Accuracy (정확도)** | 사실적으로 얼마나 정확한가? | \"답변이 사실과 일치하나요?\" |\n| **Completeness (완전성)** | 질문의 모든 측면을 다루는가? | \"모든 부분을 답변했나요?\" |\n| **Relevance (관련성)** | 질문에 직접적으로 답변하는가? | \"질문에 맞는 답변인가요?\" |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 LLM Judge 구현\n\nLiteLLM을 사용하여 다양한 LLM 제공자를 지원하는 Judge를 구현합니다."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. 실습: 평가 실행\n\n지금까지 배운 내용을 종합하여 실제 평가를 수행해봅니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 시나리오 설정\n",
    "test_case = {\n",
    "    \"question\": \"Python에서 리스트를 정렬하는 방법은 무엇인가요?\",\n",
    "    \"reference_answer\": \"Python에서 리스트를 정렬하려면 sort() 메서드 또는 sorted() 함수를 사용합니다. sort()는 원본 리스트를 직접 수정하고, sorted()는 새로운 정렬된 리스트를 반환합니다.\",\n",
    "    \"keywords\": [\"sort\", \"sorted\", \"리스트\", \"정렬\"],\n",
    "    \"relevant_docs\": [\"python_list_001\", \"python_basics_002\"]\n",
    "}\n",
    "\n",
    "# 가상의 RAG 시스템 출력 (실제로는 RAG 파이프라인에서 생성)\n",
    "rag_output = {\n",
    "    \"retrieved_docs\": [\"python_list_001\", \"python_intro_003\", \"python_basics_002\", \"django_001\", \"flask_002\"],\n",
    "    \"retrieved_content\": \"\"\"Python에서 리스트 정렬은 sort() 메서드를 사용합니다. \n",
    "    이 메서드는 원본 리스트를 직접 수정합니다. sorted() 함수도 있습니다.\"\"\",\n",
    "    \"generated_answer\": \"Python에서 리스트를 정렬하려면 sort() 메서드를 사용합니다. 이 메서드는 원본 리스트를 직접 수정합니다.\"\n",
    "}\n",
    "\n",
    "print(\"테스트 케이스:\")\n",
    "print(f\"  질문: {test_case['question']}\")\n",
    "print(f\"  정답 문서: {test_case['relevant_docs']}\")\n",
    "print(f\"\\nRAG 출력:\")\n",
    "print(f\"  검색된 문서: {rag_output['retrieved_docs']}\")\n",
    "print(f\"  생성된 답변: {rag_output['generated_answer'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 평가 실행\n",
    "retrieval_eval = evaluate_retrieval(\n",
    "    retrieved_docs=rag_output[\"retrieved_docs\"],\n",
    "    relevant_docs=test_case[\"relevant_docs\"],\n",
    "    retrieved_content=rag_output[\"retrieved_content\"],\n",
    "    keywords=test_case[\"keywords\"],\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"검색 평가 결과 (Retrieval Evaluation)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMRR: {retrieval_eval.mrr:.3f}\")\n",
    "print(f\"  -> 첫 번째 정답(python_list_001)이 1번째에 있어 최고 점수\")\n",
    "\n",
    "print(f\"\\nnDCG@5: {retrieval_eval.ndcg:.3f}\")\n",
    "print(f\"  -> 정답 2개(1,3번째)가 상위에 있어 좋은 점수\")\n",
    "\n",
    "print(f\"\\nKeyword Coverage: {retrieval_eval.keyword_coverage:.1%}\")\n",
    "print(f\"  -> 4개 키워드 중 {int(retrieval_eval.keyword_coverage * 4)}개 발견\")\n",
    "\n",
    "print(f\"\\n관련 문서 발견: {retrieval_eval.relevant_found}/{len(test_case['relevant_docs'])}개\")\n",
    "print(f\"\\n종합 점수: {retrieval_eval.overall_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 평가 실행 (LLM API 호출)\n",
    "# 주의: 이 셀은 API 키가 설정되어 있어야 실행됩니다.\n",
    "\n",
    "try:\n",
    "    answer_eval = evaluate_answer_with_llm(\n",
    "        question=test_case[\"question\"],\n",
    "        generated_answer=rag_output[\"generated_answer\"],\n",
    "        reference_answer=test_case[\"reference_answer\"],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"답변 평가 결과 (Answer Evaluation by LLM Judge)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n정확도 (Accuracy): {answer_eval.accuracy}/5\")\n",
    "    print(f\"완전성 (Completeness): {answer_eval.completeness}/5\")\n",
    "    print(f\"관련성 (Relevance): {answer_eval.relevance}/5\")\n",
    "    print(f\"\\n평균 점수: {answer_eval.average_score:.2f}/5\")\n",
    "    print(f\"정규화 점수: {answer_eval.normalized_score:.2f}\")\n",
    "    print(f\"\\n피드백: {answer_eval.feedback}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"API 호출 실패: {e}\")\n",
    "    print(\"\\nAPI 키를 확인하거나, 아래 모의 결과를 참고하세요:\")\n",
    "    \n",
    "    # 모의 결과\n",
    "    mock_eval = AnswerEval(\n",
    "        accuracy=4,\n",
    "        completeness=3,\n",
    "        relevance=5,\n",
    "        feedback=\"답변이 정확하고 관련성이 높지만, sorted() 함수에 대한 설명이 누락되어 완전성이 다소 낮습니다.\"\n",
    "    )\n",
    "    print(f\"\\n[모의 결과]\")\n",
    "    print(f\"정확도: {mock_eval.accuracy}/5\")\n",
    "    print(f\"완전성: {mock_eval.completeness}/5\")\n",
    "    print(f\"관련성: {mock_eval.relevance}/5\")\n",
    "    print(f\"피드백: {mock_eval.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. 종합 평가 리포트 클래스"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluationReport(BaseModel):\n",
    "    \"\"\"\n",
    "    RAG 시스템 종합 평가 리포트\n",
    "    \"\"\"\n",
    "    question_id: str\n",
    "    question: str\n",
    "    category: QuestionCategory\n",
    "    retrieval: RetrievalEval\n",
    "    answer: Optional[AnswerEval] = None\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"평가 리포트 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"RAG 평가 리포트: {self.question_id}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(f\"\\n질문: {self.question}\")\n",
    "        print(f\"카테고리: {self.category}\")\n",
    "        \n",
    "        print(\"\\n[검색 평가]\")\n",
    "        print(f\"  MRR: {self.retrieval.mrr:.3f}\")\n",
    "        print(f\"  nDCG: {self.retrieval.ndcg:.3f}\")\n",
    "        print(f\"  Keyword Coverage: {self.retrieval.keyword_coverage:.1%}\")\n",
    "        print(f\"  종합: {self.retrieval.overall_score:.3f}\")\n",
    "        \n",
    "        if self.answer:\n",
    "            print(\"\\n[답변 평가]\")\n",
    "            print(f\"  정확도: {self.answer.accuracy}/5\")\n",
    "            print(f\"  완전성: {self.answer.completeness}/5\")\n",
    "            print(f\"  관련성: {self.answer.relevance}/5\")\n",
    "            print(f\"  평균: {self.answer.average_score:.2f}/5\")\n",
    "            print(f\"\\n  피드백: {self.answer.feedback}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "# 종합 리포트 생성 예시\n",
    "report = RAGEvaluationReport(\n",
    "    question_id=\"q001\",\n",
    "    question=test_case[\"question\"],\n",
    "    category=\"direct_fact\",\n",
    "    retrieval=retrieval_eval,\n",
    "    answer=AnswerEval(\n",
    "        accuracy=4,\n",
    "        completeness=3,\n",
    "        relevance=5,\n",
    "        feedback=\"답변이 정확하고 관련성이 높지만, sorted() 함수 설명이 누락됨\"\n",
    "    )\n",
    ")\n",
    "\n",
    "report.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. 결론 및 Best Practices\n\n### 핵심 정리\n\n| 평가 단계 | 주요 지표 | 목적 |\n|----------|----------|------|\n| **검색 평가** | MRR, nDCG, Coverage | 올바른 문서를 찾았는가? |\n| **답변 평가** | Accuracy, Completeness, Relevance | 올바른 답변을 생성했는가? |\n\n### Best Practices\n\n1. **평가 자동화**\n   - CI/CD 파이프라인에 평가 포함\n   - 배포 전 성능 회귀 테스트\n\n2. **다양한 질문 카테고리**\n   - direct_fact만으로는 부족\n   - comparative, spanning 등 어려운 질문도 테스트\n\n3. **정기적인 평가**\n   - 문서 업데이트 시 재평가\n   - 모델 변경 시 비교 평가\n\n4. **평가 데이터 품질**\n   - 전문가 검토된 테스트 셋\n   - 실제 사용자 질문 반영\n\n5. **개선 루프 구축**\n   - 평가 -> 분석 -> 개선 -> 재평가\n   - 실패 케이스 상세 분석\n\n### 다음 단계\n\n1. 더 큰 테스트 셋 구축 (100개 이상)\n2. Ragas, DeepEval 등 전문 평가 프레임워크 활용\n3. Human evaluation과 LLM evaluation 비교\n4. 평가 대시보드 구축\n\n---\n\n## 참고 자료\n\n- [Ragas: RAG 평가 프레임워크](https://github.com/explodinggradients/ragas)\n- [DeepEval: LLM 평가 도구](https://github.com/confident-ai/deepeval)\n- [LangChain Evaluation](https://python.langchain.com/docs/guides/evaluation)\n- [BEIR Benchmark](https://github.com/beir-cellar/beir)\n\n---\n\n**이 노트북을 완료하셨습니다!**\n\n이제 여러분은:\n- 테스트 데이터를 설계할 수 있습니다\n- 검색 품질을 MRR, nDCG로 평가할 수 있습니다\n- LLM-as-a-Judge로 답변 품질을 평가할 수 있습니다\n- 종합 평가 리포트를 생성할 수 있습니다"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}