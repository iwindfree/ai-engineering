# RAG 성능 평가 지표 가이드

RAG(Retrieval-Augmented Generation) 시스템의 검색 성능을 측정하는 주요 지표들을 정리합니다.

---

## 1. MRR (Mean Reciprocal Rank)

### 핵심 아이디어
"정답이 몇 번째에 나왔나?"를 점수로 바꾼 것입니다.

### 점수 계산 방식

| 정답 위치 | 점수 |
|-----------|------|
| 1등 | 1/1 = **1.0** |
| 2등 | 1/2 = **0.5** |
| 3등 | 1/3 = **0.33** |
| 10등 | 1/10 = **0.1** |

정답이 위에 있을수록 점수가 높습니다.

### 공식

```
MRR = (1/Q) × Σ(1/rank_i)
```

- Q: 전체 쿼리 수
- rank_i: i번째 쿼리에서 첫 번째 정답의 순위

### 예시

RAG 시스템에 3번 질문했을 때:

| 질문 | 정답 위치 | 점수 |
|------|-----------|------|
| "김치 만드는 법" | 1등 | 1.0 |
| "서울 날씨" | 3등 | 0.33 |
| "파이썬 설치" | 2등 | 0.5 |

**MRR** = (1.0 + 0.33 + 0.5) / 3 = **0.61**

### 해석

- **1.0**: 완벽함 (항상 정답이 1등)
- **0.5 이상**: 괜찮은 성능
- **0.5 미만**: 개선 필요

---

## 2. nDCG (Normalized Discounted Cumulative Gain)

### 핵심 아이디어
"관련성 높은 결과가 위에 있을수록 좋다"를 점수로 바꾼 것입니다.

### MRR과의 차이점

| | MRR | nDCG |
|---|-----|------|
| 정답 개수 | 1개만 봄 | **여러 개** 봄 |
| 관련성 | 정답/오답 (0 or 1) | **등급** (0, 1, 2, 3...) |

### 계산 과정 (3단계)

#### 1단계: 관련성 점수 매기기

검색 결과 5개가 있다고 가정:

| 순위 | 문서 | 관련성 |
|------|------|--------|
| 1등 | 문서A | 3 (매우 관련) |
| 2등 | 문서B | 1 (조금 관련) |
| 3등 | 문서C | 2 (관련) |
| 4등 | 문서D | 0 (무관) |
| 5등 | 문서E | 1 (조금 관련) |

#### 2단계: DCG 계산

아래로 갈수록 점수를 **깎습니다** (할인 적용):

```
DCG = 3/log₂(2) + 1/log₂(3) + 2/log₂(4) + 0/log₂(5) + 1/log₂(6)
    = 3 + 0.63 + 1 + 0 + 0.39 = 5.02
```

#### 3단계: 정규화 (nDCG)

**이상적인 순서** (3, 2, 1, 1, 0)로 정렬했을 때의 DCG(IDCG)를 구하고 나눕니다:

```
nDCG = DCG / IDCG = 5.02 / 6.39 = 0.79
```

### 해석

- **1.0**: 완벽한 순서 (가장 관련 높은 것이 맨 위)
- **0.8 이상**: 좋은 성능
- **0.5 미만**: 순서 개선 필요

### 사용 시점

- 검색 결과가 **여러 개 필요**할 때
- 관련성에 **등급**이 있을 때 (매우 관련 / 관련 / 약간 관련)
- RAG에서 Top-K 문서의 **전체 품질**을 평가할 때

---

## 3. Precision@K (정밀도)

### 핵심 아이디어
"가져온 K개 중에 정답이 몇 개?"

### 공식

```
Precision@K = 상위 K개 중 정답 수 / K
```

### 예시 (K=5)

검색 결과 상위 5개: ✓ ✗ ✓ ✓ ✗ (정답 3개)

```
Precision@5 = 3/5 = 0.6 (60%)
```

### 관점
**품질** 중심 - 쓸모없는 결과 줄이기

---

## 4. Recall@K (재현율)

### 핵심 아이디어
"전체 정답 중에 K개 안에 몇 개 들어왔나?"

### 공식

```
Recall@K = 상위 K개 중 정답 수 / 전체 정답 수
```

### 예시 (K=5, 전체 정답 10개)

검색 결과 상위 5개 중 정답 3개

```
Recall@5 = 3/10 = 0.3 (30%)
```

### 관점
**커버리지** 중심 - 정답 놓치지 않기

---

## 5. Precision@K vs Recall@K 비교

### 개념 비교

| | Precision@K | Recall@K |
|---|-------------|----------|
| 질문 | K개 중 정답 비율? | 전체 정답 중 찾은 비율? |
| 분모 | K (고정) | 전체 정답 수 |
| 관점 | **품질** (쓸모없는 것 줄이기) | **커버리지** (놓치지 않기) |

### 실제 예시

전체 관련 문서가 **8개**인 질문에 대해 상위 5개를 가져왔을 때:

| 순위 | 결과 |
|------|------|
| 1 | ✓ 정답 |
| 2 | ✓ 정답 |
| 3 | ✗ 오답 |
| 4 | ✓ 정답 |
| 5 | ✗ 오답 |

- **Precision@5** = 3/5 = 0.6 → "가져온 것 중 60%가 쓸모있음"
- **Recall@5** = 3/8 = 0.375 → "전체 정답의 37.5%를 찾음"

### 트레이드오프

K를 늘리면 Recall은 올라가지만 Precision은 내려가는 경향이 있습니다.

---

## 6. 지표 선택 가이드

| 지표 | 사용 상황 |
|------|-----------|
| **MRR** | 첫 번째 정답 위치가 중요할 때 (단일 정답) |
| **nDCG** | 여러 결과의 전체 품질과 순서가 중요할 때 |
| **Precision@K** | 검색 결과에 노이즈가 적어야 할 때 |
| **Recall@K** | 정답을 놓치면 안 될 때 |

---

## 7. 요약

| 지표 | 핵심 질문 | 범위 |
|------|-----------|------|
| MRR | 첫 정답이 몇 번째? | 0 ~ 1 |
| nDCG | 관련성 높은 게 위에 있나? | 0 ~ 1 |
| Precision@K | K개 중 정답 비율? | 0 ~ 1 |
| Recall@K | 전체 정답 중 찾은 비율? | 0 ~ 1 |

모든 지표는 **1에 가까울수록 좋은 성능**을 의미합니다.
