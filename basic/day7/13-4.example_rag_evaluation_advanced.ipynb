{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c26c848",
   "metadata": {},
   "source": [
    "# ê³ ê¸‰ RAG ê¸°ë²• (Advanced RAG Techniques)\n",
    "\n",
    "ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ê³ ê¸‰ RAG ê¸°ë²•**ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "ê¸°ë³¸ RAGë¥¼ ë„˜ì–´ì„œ ì‹¤ë¬´ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ìµí™ë‹ˆë‹¤.\n",
    "\n",
    "## ì˜¤ëŠ˜ ë°°ìš¸ ë‚´ìš©: Ingest (ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬) ê³ ê¸‰ ê¸°ë²•\n",
    "\n",
    "1. **LangChain ì—†ì´ ìˆœìˆ˜ Pythonìœ¼ë¡œ êµ¬í˜„** - ìµœëŒ€í•œì˜ ìœ ì—°ì„±ì„ ìœ„í•´ í”„ë ˆì„ì›Œí¬ ì˜ì¡´ì„± ì œê±°\n",
    "2. **LLMì„ í™œìš©í•œ ì§€ëŠ¥í˜• ì²­í‚¹** - ë‹¨ìˆœ ë¬¸ì ìˆ˜ ê¸°ë°˜ì´ ì•„ë‹Œ, LLMì´ ë¬¸ë§¥ì„ ì´í•´í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "3. **ìµœì ì˜ ì²­í¬ í¬ê¸°ì™€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©** - ì´ì „ ì‹¤ìŠµì—ì„œ ì°¾ì€ ìµœì ì˜ ì„¤ì • í™œìš©\n",
    "4. **ë¬¸ì„œ ì „ì²˜ë¦¬ (Document Pre-processing)** - LLMì´ ê²€ìƒ‰ì— ìœ ë¦¬í•˜ë„ë¡ ì²­í¬ë¥¼ ì¬ì‘ì„±\n",
    "\n",
    "## ì™œ ê³ ê¸‰ RAGê°€ í•„ìš”í•œê°€?\n",
    "\n",
    "ê¸°ë³¸ RAGì˜ í•œê³„:\n",
    "- ê³ ì • í¬ê¸° ì²­í‚¹ìœ¼ë¡œ ì¸í•œ ë¬¸ë§¥ ì†ì‹¤\n",
    "- ë‹¨ìˆœ ìœ ì‚¬ë„ ê²€ìƒ‰ì˜ ì •í™•ë„ í•œê³„\n",
    "- ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬¸ì„œ í‘œí˜„ ë°©ì‹ì˜ ë¶ˆì¼ì¹˜\n",
    "\n",
    "ê³ ê¸‰ RAGì˜ í•´ê²°ì±…:\n",
    "- **Semantic Chunking**: ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì„œ ë¶„í• \n",
    "- **Reranking**: ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬ë¡œ ì •í™•ë„ í–¥ìƒ\n",
    "- **Query Rewriting**: ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from chromadb import PersistentClient\n",
    "from tqdm import tqdm\n",
    "from litellm import completion\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "DB_NAME = \"preprocessed_db\"\n",
    "collection_name = \"docs\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "KNOWLEDGE_BASE_PATH = Path(\"knowledge-base\")\n",
    "AVERAGE_CHUNK_SIZE = 500\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfac66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChainì˜ Document í´ë˜ìŠ¤ì—ì„œ ì˜ê°ì„ ë°›ì•„ ìœ ì‚¬í•œ êµ¬ì¡° ì •ì˜\n",
    "\n",
    "class Result(BaseModel):\n",
    "    page_content: str\n",
    "    metadata: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunkë¥¼ í‘œí˜„í•˜ëŠ” Pydantic ëª¨ë¸\n",
    "# LLMì˜ Structured Outputì„ ìœ„í•´ Field descriptionì„ ìƒì„¸íˆ ì‘ì„±\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    headline: str = Field(description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\")\n",
    "    summary: str = Field(description=\"A few sentences summarizing the content of this chunk to answer common questions\")\n",
    "    original_text: str = Field(description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\")\n",
    "\n",
    "    def as_result(self, document):\n",
    "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
    "        return Result(page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text,metadata=metadata)\n",
    "\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    chunks: list[Chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b64c1",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ 3ë‹¨ê³„\n",
    "\n",
    "RAG ì‹œìŠ¤í…œì˜ ë°ì´í„° ìˆ˜ì§‘(Ingest) ê³¼ì •ì€ ë‹¤ìŒ 3ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:\n",
    "\n",
    "1. **ë¬¸ì„œ ë¡œë“œ (Fetch)** - Knowledge Baseì—ì„œ ë¬¸ì„œë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (LangChainì˜ DirectoryLoaderì™€ ìœ ì‚¬)\n",
    "2. **LLM ê¸°ë°˜ ì²­í‚¹** - LLMì„ í˜¸ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” Chunkë¡œ ë¶„í• í•©ë‹ˆë‹¤\n",
    "3. **ë²¡í„° ì €ì¥** - ìƒì„±ëœ Chunkë“¤ì„ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤\n",
    "\n",
    "ì´ê²ƒì´ ì „ë¶€ì…ë‹ˆë‹¤! ê°„ë‹¨í•˜ì£ ?\n",
    "\n",
    "### Step 1: ë¬¸ì„œ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "ì•„ë˜ í•¨ìˆ˜ëŠ” LangChainì˜ `DirectoryLoader`ë¥¼ ìˆœìˆ˜ Pythonìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "í´ë” êµ¬ì¡°ì—ì„œ ë¬¸ì„œ íƒ€ì…ì„ ì¶”ì¶œí•˜ê³ , ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5abdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents():\n",
    "    \"\"\"LangChain DirectoryLoaderì˜ ìˆœìˆ˜ Python êµ¬í˜„\"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for folder in KNOWLEDGE_BASE_PATH.iterdir():\n",
    "        doc_type = folder.name\n",
    "        for file in folder.rglob(\"*.md\"):\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                documents.append({\"type\": doc_type, \"source\": file.as_posix(), \"text\": f.read()})\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = fetch_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "za5bg094zl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ë¬¸ì„œì˜ êµ¬ì¡° í™•ì¸\n",
    "print(f\"ë¬¸ì„œ ê°œìˆ˜: {len(documents)}\")\n",
    "print(f\"\\nì²« ë²ˆì§¸ ë¬¸ì„œ êµ¬ì¡°:\")\n",
    "print(f\"  - type: {documents[0]['type']}\")\n",
    "print(f\"  - source: {documents[0]['source']}\")\n",
    "print(f\"  - text ê¸¸ì´: {len(documents[0]['text'])} ê¸€ì\")\n",
    "print(f\"  - text ë¯¸ë¦¬ë³´ê¸°: {documents[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "du1g1z0d6fk",
   "metadata": {},
   "source": [
    "### ë¬¸ì„œ(Document) êµ¬ì¡° ì´í•´í•˜ê¸°\n",
    "\n",
    "`fetch_documents()` í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” `documents`ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ê° ë¬¸ì„œì˜ êµ¬ì¡°:**\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"type\": str,    # ë¬¸ì„œ íƒ€ì… (í´ë” ì´ë¦„ì—ì„œ ì¶”ì¶œ)\n",
    "    \"source\": str,  # íŒŒì¼ ê²½ë¡œ (ì¶œì²˜ ì¶”ì ìš©)\n",
    "    \"text\": str     # íŒŒì¼ ì „ì²´ ë‚´ìš©\n",
    "}\n",
    "```\n",
    "\n",
    "**Knowledge Base í´ë” êµ¬ì¡° ì˜ˆì‹œ:**\n",
    "\n",
    "```\n",
    "knowledge-base/\n",
    "â”œâ”€â”€ products/           # type = \"products\"\n",
    "â”‚   â”œâ”€â”€ product1.md\n",
    "â”‚   â””â”€â”€ product2.md\n",
    "â”œâ”€â”€ employees/          # type = \"employees\"\n",
    "â”‚   â”œâ”€â”€ ceo.md\n",
    "â”‚   â””â”€â”€ cto.md\n",
    "â”œâ”€â”€ contracts/          # type = \"contracts\"\n",
    "â”‚   â””â”€â”€ contract1.md\n",
    "â””â”€â”€ company/            # type = \"company\"\n",
    "    â””â”€â”€ about.md\n",
    "```\n",
    "\n",
    "**ì™œ ì´ êµ¬ì¡°ì¸ê°€?**\n",
    "\n",
    "| í•„ë“œ | ìš©ë„ |\n",
    "|-----|------|\n",
    "| `type` | ë¬¸ì„œ ë¶„ë¥˜, í•„í„°ë§ ê²€ìƒ‰, ì‹œê°í™” ìƒ‰ìƒ êµ¬ë¶„ì— í™œìš© |\n",
    "| `source` | ë‹µë³€ ì‹œ ì¶œì²˜ í‘œì‹œ, ë””ë²„ê¹…, ì¶”ì ì— í™œìš© |\n",
    "| `text` | LLM ì²­í‚¹ì˜ ì…ë ¥ ë°ì´í„° |\n",
    "\n",
    "ì´ ë©”íƒ€ë°ì´í„°ëŠ” ë‚˜ì¤‘ì— ChromaDBì— ì €ì¥ë˜ì–´ ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ë°˜í™˜ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa1c68",
   "metadata": {},
   "source": [
    "### Step 2: LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì²­í‚¹\n",
    "\n",
    "ì´ì œ í•µì‹¬ì¸ **LLM ê¸°ë°˜ ì²­í‚¹**ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê¸°ì¡´ ë°©ì‹ vs LLM ê¸°ë°˜ ë°©ì‹:\n",
    "\n",
    "| ê¸°ì¡´ ë°©ì‹ | LLM ê¸°ë°˜ ë°©ì‹ |\n",
    "|----------|--------------|\n",
    "| ê³ ì • ë¬¸ì ìˆ˜ë¡œ ë¶„í•  | ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  |\n",
    "| ë¬¸ë§¥ ë¬´ì‹œ | ë¬¸ë§¥ ì´í•´ |\n",
    "| ë‹¨ìˆœ ì˜¤ë²„ë© | ì§€ëŠ¥ì  ì˜¤ë²„ë© |\n",
    "| ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ ì €ì¥ | headline + summary + ì›ë³¸ ì €ì¥ |\n",
    "\n",
    "LLMì—ê²Œ ë‹¤ìŒì„ ìš”ì²­í•©ë‹ˆë‹¤:\n",
    "- **headline**: ê²€ìƒ‰ì— ìœ ë¦¬í•œ ì§§ì€ ì œëª©\n",
    "- **summary**: í•µì‹¬ ë‚´ìš© ìš”ì•½\n",
    "- **original_text**: ì›ë³¸ í…ìŠ¤íŠ¸ (ë³€ê²½ ì—†ì´)\n",
    "\n",
    "ì´ë ‡ê²Œ í•˜ë©´ ê²€ìƒ‰ ì‹œ headlineê³¼ summaryê°€ ë§¤ì¹­ í™•ë¥ ì„ ë†’ì—¬ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(document):\n",
    "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
    "    return f\"\"\"\n",
    "You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
    "\n",
    "The document is from the shared drive of a company called Insurellm.\n",
    "The document is of type: {document[\"type\"]}\n",
    "The document has been retrieved from: {document[\"source\"]}\n",
    "\n",
    "A chatbot will use these chunks to answer questions about the company.\n",
    "You should divide up the document as you see fit, being sure that the entire document is returned in the chunks - don't leave anything out.\n",
    "This document should probably be split into {how_many} chunks, but you can have more or less as appropriate.\n",
    "There should be overlap between the chunks as appropriate; typically about 25% overlap or about 50 words, so you have the same text in multiple chunks for best retrieval results.\n",
    "\n",
    "For each chunk, you should provide a headline, a summary, and the original text of the chunk.\n",
    "Together your chunks should represent the entire document with overlap.\n",
    "\n",
    "Here is the document:\n",
    "\n",
    "{document[\"text\"]}\n",
    "\n",
    "Respond with the chunks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_prompt(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f58850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_messages(document):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab04779",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_messages(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9qxtvuyh5z8",
   "metadata": {},
   "source": [
    "### process_document í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…\n",
    "\n",
    "`process_document`ëŠ” ì´ ë…¸íŠ¸ë¶ì˜ **í•µì‹¬ í•¨ìˆ˜**ì…ë‹ˆë‹¤. ë‹¨ì¼ ë¬¸ì„œë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ì§€ëŠ¥í˜• ì²­í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "**í•¨ìˆ˜ ë™ì‘ íë¦„:**\n",
    "\n",
    "```\n",
    "ë¬¸ì„œ (Document)\n",
    "    â†“\n",
    "make_messages() - í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    â†“\n",
    "LLM API í˜¸ì¶œ (litellm.completion)\n",
    "    â†“\n",
    "Structured Output (JSON) íŒŒì‹±\n",
    "    â†“\n",
    "Chunk ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    â†“\n",
    "Result ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "```\n",
    "\n",
    "**í•µì‹¬ í¬ì¸íŠ¸:**\n",
    "\n",
    "1. **Structured Output í™œìš©**\n",
    "   - `response_format=Chunks`ë¥¼ ì§€ì •í•˜ì—¬ LLMì´ ì •í•´ì§„ JSON ìŠ¤í‚¤ë§ˆë¡œ ì‘ë‹µ\n",
    "   - Pydantic ëª¨ë¸ë¡œ ìë™ ê²€ì¦ ë° íŒŒì‹±\n",
    "\n",
    "2. **LLMì´ ê²°ì •í•˜ëŠ” ê²ƒë“¤**\n",
    "   - ì²­í¬ ë¶„í•  ìœ„ì¹˜ (ì˜ë¯¸ ë‹¨ìœ„ë¡œ)\n",
    "   - ê° ì²­í¬ì˜ headline (ê²€ìƒ‰ìš© ì œëª©)\n",
    "   - ê° ì²­í¬ì˜ summary (í•µì‹¬ ìš”ì•½)\n",
    "   - ì˜¤ë²„ë© ë²”ìœ„ (ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´)\n",
    "\n",
    "3. **ì™œ ì´ ë°©ì‹ì´ íš¨ê³¼ì ì¸ê°€?**\n",
    "   - ë‹¨ìˆœ ë¬¸ì ìˆ˜ ë¶„í• ì€ ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ëŠê¸¸ ìˆ˜ ìˆìŒ\n",
    "   - LLMì€ ë¬¸ë‹¨, ì£¼ì œ, ë…¼ë¦¬ì  êµ¬ë¶„ì„ ì´í•´í•¨\n",
    "   - headlineê³¼ summaryê°€ ê²€ìƒ‰ ë§¤ì¹­ë¥ ì„ ë†’ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ë¬¸ì„œë¥¼ LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì²­í‚¹ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        document: dict - {\"type\": str, \"source\": str, \"text\": str} í˜•íƒœì˜ ë¬¸ì„œ\n",
    "        \n",
    "    Returns:\n",
    "        list[Result] - headline, summary, original_textê°€ ê²°í•©ëœ Result ê°ì²´ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    ì²˜ë¦¬ ê³¼ì •:\n",
    "        1. make_messages()ë¡œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        2. litellm.completion()ìœ¼ë¡œ LLM í˜¸ì¶œ (Structured Output ì‚¬ìš©)\n",
    "        3. Pydanticìœ¼ë¡œ JSON ì‘ë‹µì„ Chunks ëª¨ë¸ë¡œ íŒŒì‹±\n",
    "        4. ê° Chunkë¥¼ Result ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    # 1. ë¬¸ì„œë¥¼ LLM ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    messages = make_messages(document)\n",
    "    \n",
    "    # 2. LLM í˜¸ì¶œ - response_formatìœ¼ë¡œ Pydantic ëª¨ë¸ì„ ì§€ì •í•˜ë©´\n",
    "    #    LLMì´ í•´ë‹¹ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” JSONìœ¼ë¡œ ì‘ë‹µí•¨ (Structured Output)\n",
    "    response = completion(model=MODEL, messages=messages, response_format=Chunks)\n",
    "    \n",
    "    # 3. LLM ì‘ë‹µì—ì„œ JSON ë¬¸ìì—´ ì¶”ì¶œ\n",
    "    reply = response.choices[0].message.content\n",
    "    \n",
    "    # 4. JSONì„ Pydantic ëª¨ë¸ë¡œ íŒŒì‹±í•˜ì—¬ Chunk ë¦¬ìŠ¤íŠ¸ íšë“\n",
    "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "    \n",
    "    # 5. ê° Chunkë¥¼ Result í˜•íƒœë¡œ ë³€í™˜ (headline + summary + original_text ê²°í•©)\n",
    "    return [chunk.as_result(document) for chunk in doc_as_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480494d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_document(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccab1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm: ì§„í–‰ë¥  í‘œì‹œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "# - ë°˜ë³µë¬¸ì„ tqdm()ìœ¼ë¡œ ê°ì‹¸ë©´ ìë™ìœ¼ë¡œ í”„ë¡œê·¸ë ˆìŠ¤ ë°” í‘œì‹œ\n",
    "# - LLM API í˜¸ì¶œì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì§„í–‰ ìƒí™© í™•ì¸ì— ìœ ìš©\n",
    "# - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„(ETA), ì²˜ë¦¬ ì†ë„(it/s) ë“±ë„ í•¨ê»˜ í‘œì‹œë¨\n",
    "\n",
    "def create_chunks(documents):\n",
    "    \"\"\"ëª¨ë“  ë¬¸ì„œë¥¼ ìˆœíšŒí•˜ë©° LLM ê¸°ë°˜ ì²­í‚¹ ìˆ˜í–‰\"\"\"\n",
    "    chunks = []\n",
    "    for doc in tqdm(documents):  # tqdm(documents): ì§„í–‰ë¥  ë°”ì™€ í•¨ê»˜ ë°˜ë³µ\n",
    "        chunks.extend(process_document(doc))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93115f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = create_chunks(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uip8hs5wc7e",
   "metadata": {},
   "source": [
    "**tqdm ì¶œë ¥ ì˜ˆì‹œ:**\n",
    "```\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:30<00:00,  3.62s/it]\n",
    "```\n",
    "\n",
    "| í‘œì‹œ | ì˜ë¯¸ |\n",
    "|-----|------|\n",
    "| `100%` | ì§„í–‰ë¥  |\n",
    "| `â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ` | í”„ë¡œê·¸ë ˆìŠ¤ ë°” |\n",
    "| `25/25` | ì™„ë£Œ/ì „ì²´ ê°œìˆ˜ |\n",
    "| `01:30` | ê²½ê³¼ ì‹œê°„ |\n",
    "| `<00:00` | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ |\n",
    "| `3.62s/it` | í•­ëª©ë‹¹ ì²˜ë¦¬ ì‹œê°„ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750104c",
   "metadata": {},
   "source": [
    "### Step 2 ì™„ë£Œ! \n",
    "\n",
    "LLM ê¸°ë°˜ ì²­í‚¹ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì†Œ ì‹œê°„ì´ ê±¸ë¦¬ì§€ë§Œ, ê²°ê³¼ì˜ í’ˆì§ˆì€ í›¨ì”¬ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì„±ëŠ¥ ìµœì í™” íŒ:**\n",
    "- ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” `multiprocessing.Pool`ì„ ì‚¬ìš©í•´ ë³‘ë ¬ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "- Rate Limit ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ë„ê±°ë‚˜ ì§€ì—° ì‹œê°„ì„ ì¶”ê°€í•˜ì„¸ìš”\n",
    "- ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤\n",
    "\n",
    "### Step 3: ì„ë² ë”© ìƒì„± ë° ë²¡í„° DB ì €ì¥\n",
    "\n",
    "ì´ì œ ìƒì„±ëœ ì²­í¬ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì´ ë‹¨ê³„ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼:**\n",
    "1. ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ (ì¬ìƒì„±)\n",
    "2. ëª¨ë“  ì²­í¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜\n",
    "3. ChromaDB ì»¬ë ‰ì…˜ì— ë²¡í„°, ë¬¸ì„œ, ë©”íƒ€ë°ì´í„° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks):\n",
    "    chroma = PersistentClient(path=DB_NAME)\n",
    "    if collection_name in [c.name for c in chroma.list_collections()]:\n",
    "        chroma.delete_collection(collection_name)\n",
    "\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    emb = openai.embeddings.create(model=embedding_model, input=texts).data\n",
    "    vectors = [e.embedding for e in emb]\n",
    "\n",
    "    collection = chroma.get_or_create_collection(collection_name)\n",
    "\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    metas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metas)\n",
    "    print(f\"Vectorstore created with {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f52038",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf738d0",
   "metadata": {},
   "source": [
    "# ë²¡í„° ì‹œê°í™”\n",
    "\n",
    "ì ê¹! ì‹œê°í™”ë¥¼ ë¹¼ë¨¹ì„ ë»”í–ˆë„¤ìš”!\n",
    "\n",
    "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ì„ë² ë”©ë“¤ì„ ì‹œê°í™”í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "## t-SNEë¥¼ ì´ìš©í•œ ì°¨ì› ì¶•ì†Œ\n",
    "\n",
    "ê³ ì°¨ì› ì„ë² ë”© ë²¡í„°(3072ì°¨ì›)ë¥¼ 2D/3Dë¡œ ì¶•ì†Œí•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "**t-SNE (t-distributed Stochastic Neighbor Embedding)ë€?**\n",
    "- ê³ ì°¨ì› ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì €ì°¨ì›ì—ì„œ ë³´ì¡´í•˜ëŠ” ê¸°ë²•\n",
    "- ë¹„ìŠ·í•œ ë°ì´í„° í¬ì¸íŠ¸ëŠ” ê°€ê¹ê²Œ, ë‹¤ë¥¸ ë°ì´í„° í¬ì¸íŠ¸ëŠ” ë©€ë¦¬ ë°°ì¹˜\n",
    "- í´ëŸ¬ìŠ¤í„°ë§ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "ìƒ‰ìƒ ì½”ë“œ:\n",
    "- ğŸ”µ íŒŒë€ìƒ‰: products (ì œí’ˆ)\n",
    "- ğŸŸ¢ ë…¹ìƒ‰: employees (ì§ì›)\n",
    "- ğŸ”´ ë¹¨ê°„ìƒ‰: contracts (ê³„ì•½)\n",
    "- ğŸŸ  ì£¼í™©ìƒ‰: company (íšŒì‚¬ ì •ë³´)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = PersistentClient(path=DB_NAME)\n",
    "collection = chroma.get_or_create_collection(collection_name)\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['type'] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4683c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=10, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72b54a",
   "metadata": {},
   "source": [
    "## ê³ ê¸‰ RAG ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "\n",
    "ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ **ê³ ê¸‰ RAG**ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤!\n",
    "\n",
    "### ì ìš©í•  ê³ ê¸‰ ê¸°ë²•ë“¤:\n",
    "\n",
    "#### 1. Reranking (ì¬ìˆœìœ„í™”)\n",
    "- ë²¡í„° ìœ ì‚¬ë„ë¡œ ê²€ìƒ‰í•œ ê²°ê³¼ë¥¼ LLMì´ ë‹¤ì‹œ ì •ë ¬\n",
    "- ë‹¨ìˆœ ìœ ì‚¬ë„ë³´ë‹¤ **ì˜ë¯¸ì  ê´€ë ¨ì„±**ì„ ë” ì •í™•í•˜ê²Œ íŒë‹¨\n",
    "- ìƒìœ„ Kê°œ ê²°ê³¼ì˜ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚´\n",
    "\n",
    "**Rerankingì´ í•„ìš”í•œ ì´ìœ :**\n",
    "- ì„ë² ë”© ìœ ì‚¬ë„ëŠ” \"ë¹„ìŠ·í•¨\"ì„ ì¸¡ì •í•˜ì§€ë§Œ, \"ê´€ë ¨ì„±\"ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ\n",
    "- ì˜ˆ: \"ì‚¬ê³¼\"ì™€ \"ë°°\"ëŠ” ìœ ì‚¬í•˜ì§€ë§Œ, \"ì‚¬ê³¼ ê°€ê²©\"ì„ ë¬¼ìœ¼ë©´ \"ë°° ê°€ê²©\" ë¬¸ì„œëŠ” ê´€ë ¨ ì—†ìŒ\n",
    "- LLMì€ ì§ˆë¬¸ì˜ **ì˜ë„**ë¥¼ ì´í•´í•˜ì—¬ ë” ì •í™•í•œ ìˆœìœ„ ë¶€ì—¬ ê°€ëŠ¥\n",
    "\n",
    "#### 2. Query Rewriting (ì¿¼ë¦¬ ì¬ì‘ì„±)\n",
    "- ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜\n",
    "- ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ë…ë¦½ì ì¸ ì¿¼ë¦¬ ìƒì„±\n",
    "- ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "\n",
    "**Query Rewriting ì˜ˆì‹œ:**\n",
    "- ì›ë³¸: \"ê·¸ ì‚¬ëŒ ì–´ë”” ëŒ€í•™ ë‚˜ì™”ì–´?\" \n",
    "- ì¬ì‘ì„±: \"ì§ì› í•™ë ¥ ëŒ€í•™êµ ì¶œì‹ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankOrder(BaseModel):\n",
    "    order: list[int] = Field(\n",
    "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8446c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(question, chunks):\n",
    "    system_prompt = \"\"\"\n",
    "You are a document re-ranker.\n",
    "You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
    "The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance, but you may be able to improve on that.\n",
    "You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
    "Reply only with the list of ranked chunk ids, nothing else. Include all the chunk ids you are provided with, reranked.\n",
    "\"\"\"\n",
    "    user_prompt = f\"The user has asked the following question:\\n\\n{question}\\n\\nOrder all the chunks of text by relevance to the question, from most relevant to least relevant. Include all the chunk ids you are provided with, reranked.\\n\\n\"\n",
    "    user_prompt += \"Here are the chunks:\\n\\n\"\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        user_prompt += f\"# CHUNK ID: {index + 1}:\\n\\n{chunk.page_content}\\n\\n\"\n",
    "    user_prompt += \"Reply only with the list of ranked chunk ids, nothing else.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    response = completion(model=MODEL, messages=messages, response_format=RankOrder)\n",
    "    reply = response.choices[0].message.content\n",
    "    order = RankOrder.model_validate_json(reply).order\n",
    "    print(order)\n",
    "    return [chunks[i - 1] for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_K = 10\n",
    "\n",
    "def fetch_context_unranked(question):\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    \n",
    "    openai.embeddings.create() ë°˜í™˜ êµ¬ì¡°:\n",
    "        CreateEmbeddingResponse(\n",
    "            data=[Embedding(embedding=[0.0023, -0.009, ...], index=0)],\n",
    "            model='text-embedding-3-large',\n",
    "            usage=Usage(prompt_tokens=5, total_tokens=5)\n",
    "        )\n",
    "    \n",
    "    .data[0].embedding â†’ ì§ˆë¬¸ì˜ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ëŠ” 3072ì°¨ì› ë²¡í„°\n",
    "    \"\"\"\n",
    "    # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜ (ì§ˆë¬¸ì˜ ì˜ë¯¸ë¥¼ ìˆ«ìë¡œ í‘œí˜„)\n",
    "    # ì˜ˆ: \"Who won the IIOTY award?\" â†’ [0.0023, -0.009, 0.015, ...] 3072ê°œ ìˆ«ì\n",
    "    query = openai.embeddings.create(model=embedding_model, input=[question]).data[0].embedding\n",
    "    \n",
    "    # ChromaDBì—ì„œ ì§ˆë¬¸ ë²¡í„°ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ë²¡í„° ê²€ìƒ‰\n",
    "    # ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œì´ ìœ„ì¹˜í•¨\n",
    "    results = collection.query(query_embeddings=[query], n_results=RETRIEVAL_K)\n",
    "    \n",
    "    chunks = []\n",
    "    for result in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        chunks.append(Result(page_content=result[0], metadata=result[1]))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53f6de",
   "metadata": {},
   "source": [
    "### Reranking í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì´ì œ Rerankingì´ ì‹¤ì œë¡œ ê²€ìƒ‰ í’ˆì§ˆì„ ê°œì„ í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "**í…ŒìŠ¤íŠ¸ ë°©ë²•:**\n",
    "1. ì§ˆë¬¸ìœ¼ë¡œ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (unranked)\n",
    "2. ê²°ê³¼ ìˆœì„œ í™•ì¸\n",
    "3. Reranking ì ìš©\n",
    "4. ê°œì„ ëœ ìˆœì„œ í™•ì¸\n",
    "\n",
    "**ê¸°ëŒ€ ê²°ê³¼:**\n",
    "- ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì²­í¬ê°€ ìƒìœ„ë¡œ ì´ë™\n",
    "- ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ë³´ë‹¤ ì˜ë¯¸ì  ê´€ë ¨ì„±ì´ ë†’ì€ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ed5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the IIOTY award?\"\n",
    "chunks = fetch_context_unranked(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23594f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in reranked:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405de4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who went to Manchester University?\"\n",
    "RETRIEVAL_K = 20\n",
    "chunks = fetch_context_unranked(question)\n",
    "for index, c in enumerate(chunks):\n",
    "    if \"manchester\" in c.page_content.lower():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22948df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, c in enumerate(reranked):\n",
    "    if \"manchester\" in c.page_content.lower():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question):\n",
    "    chunks = fetch_context_unranked(question)\n",
    "    return rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
    "You are chatting with a user about Insurellm.\n",
    "Your answer will be evaluated for accuracy, relevance and completeness, so make sure it only answers the question and fully answers it.\n",
    "If you don't know the answer, say so.\n",
    "For context, here are specific extracts from the Knowledge Base that might be directly relevant to the user's question:\n",
    "{context}\n",
    "\n",
    "With this context, please answer the user's question. Be accurate, relevant and complete.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ë©”ì‹œì§€ ìƒì„± í•¨ìˆ˜\n",
    "# ì»¨í…ìŠ¤íŠ¸ì— ê° ì²­í¬ì˜ ì¶œì²˜(source)ë¥¼ í¬í•¨í•˜ì—¬ íˆ¬ëª…ì„± í™•ë³´\n",
    "\n",
    "def make_rag_messages(question, history, chunks):\n",
    "    context = \"\\n\\n\".join(f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\" for chunk in chunks)\n",
    "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(question, history=[]):\n",
    "    \"\"\"Rewrite the user's question to be a more specific question that is more likely to surface relevant content in the Knowledge Base.\"\"\"\n",
    "    message = f\"\"\"\n",
    "You are in a conversation with a user, answering questions about the company Insurellm.\n",
    "You are about to look up information in a Knowledge Base to answer the user's question.\n",
    "\n",
    "This is the history of your conversation so far with the user:\n",
    "{history}\n",
    "\n",
    "And this is the user's current question:\n",
    "{question}\n",
    "\n",
    "Respond only with a single, refined question that you will use to search the Knowledge Base.\n",
    "It should be a VERY short specific question most likely to surface content. Focus on the question details.\n",
    "Don't mention the company name unless it's a general question about the company.\n",
    "IMPORTANT: Respond ONLY with the knowledgebase query, nothing else.\n",
    "\"\"\"\n",
    "    response = completion(model=MODEL, messages=[{\"role\": \"system\", \"content\": message}])\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d050a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_query(\"Who won the IIOTY award?\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yfsbnkgldqj",
   "metadata": {},
   "source": [
    "### Query Rewritingì˜ ì£¼ì˜ì \n",
    "\n",
    "Query Rewritingì€ **ì–‘ë‚ ì˜ ê²€**ì…ë‹ˆë‹¤. ì˜ ì‘ë™í•˜ë©´ ê²€ìƒ‰ í’ˆì§ˆì´ í–¥ìƒë˜ì§€ë§Œ, ì˜ëª»ë˜ë©´ ì˜¤íˆë ¤ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì •í™•ì„±ì„ ë–¨ì–´ëœ¨ë¦¬ëŠ” ê²½ìš°:**\n",
    "\n",
    "| ë¬¸ì œ | ì˜ˆì‹œ |\n",
    "|-----|------|\n",
    "| ê³ ìœ ëª…ì‚¬ ì†ì‹¤ | \"IIOTY ìƒ\" â†’ \"íšŒì‚¬ ìˆ˜ìƒ ë‚´ì—­\" (IIOTYê°€ ì‚¬ë¼ì§) |\n",
    "| ê³¼ë„í•œ ì¼ë°˜í™” | \"Manchester University ì¶œì‹ \" â†’ \"ì§ì› í•™ë ¥\" |\n",
    "| ì˜ë„ ì˜¤í•´ | \"ê°€ê²©ì´ ì–¼ë§ˆì•¼?\" â†’ \"ì œí’ˆ ì •ë³´\" (ê°€ê²© í‚¤ì›Œë“œ ëˆ„ë½) |\n",
    "| í™˜ê° ì¶”ê°€ | ì—†ëŠ” ì •ë³´ë¥¼ LLMì´ ì¶”ê°€ |\n",
    "| ë§¥ë½ ì˜¤í•´ì„ | ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì˜ëª» í•´ì„ |\n",
    "\n",
    "**í•´ê²° ë°©ë²•ë“¤:**\n",
    "\n",
    "1. **Multi-query RAG**: ì›ë³¸ + ì¬ì‘ì„± ì¿¼ë¦¬ ë‘˜ ë‹¤ë¡œ ê²€ìƒ‰\n",
    "2. **ì¡°ê±´ë¶€ ì ìš©**: ë‹¨ìˆœ ì§ˆë¬¸ì€ ì¬ì‘ì„± ê±´ë„ˆë›°ê¸°\n",
    "3. **í’ˆì§ˆ ê²€ì¦**: ì¬ì‘ì„± ê²°ê³¼ê°€ ì›ë³¸ê³¼ ë„ˆë¬´ ë‹¤ë¥´ë©´ ì›ë³¸ ì‚¬ìš©\n",
    "4. **Fallback**: ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì‹¤í•˜ë©´ ì›ë³¸ìœ¼ë¡œ ì¬ì‹œë„\n",
    "5. **Query Expansion + Reranking ì¡°í•©**: ì›ë³¸ ì¿¼ë¦¬ì™€ ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ëª¨ë‘ë¡œ ê²€ìƒ‰í•˜ì—¬ ì²­í¬ë¥¼ ìˆ˜ì§‘í•œ ë’¤, ì „ì²´ ê²°ê³¼ë¥¼ Rerankingìœ¼ë¡œ ì¬ì •ë ¬\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Query Expansion + Reranking íŒŒì´í”„ë¼ì¸                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  ì›ë³¸ ì¿¼ë¦¬: \"IIOTY ìƒ ìˆ˜ìƒì?\"                           â”‚\n",
    "â”‚       â†“                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚ ì›ë³¸ ì¿¼ë¦¬ë¡œ  â”‚     â”‚ ì¬ì‘ì„± ì¿¼ë¦¬ë¡œâ”‚                   â”‚\n",
    "â”‚  â”‚ ê²€ìƒ‰        â”‚     â”‚ ê²€ìƒ‰        â”‚                   â”‚\n",
    "â”‚  â”‚ â†’ ì²­í¬ A,B,Câ”‚     â”‚ â†’ ì²­í¬ D,E,Fâ”‚                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚       â†“                    â†“                           â”‚\n",
    "â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚\n",
    "â”‚                â†“                                        â”‚\n",
    "â”‚         ì¤‘ë³µ ì œê±° í›„ ë³‘í•©                                â”‚\n",
    "â”‚         [A, B, C, D, E, F]                              â”‚\n",
    "â”‚                â†“                                        â”‚\n",
    "â”‚         Reranking (LLM)                                 â”‚\n",
    "â”‚                â†“                                        â”‚\n",
    "â”‚         ìµœì¢… ê²°ê³¼: [D, A, C, ...]                        â”‚\n",
    "â”‚                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "ì´ ë°©ì‹ì€ ì›ë³¸ ì¿¼ë¦¬ì˜ í‚¤ì›Œë“œ ì •í™•ì„±ê³¼ ì¬ì‘ì„± ì¿¼ë¦¬ì˜ ì˜ë¯¸ í™•ì¥ì„ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ë¬´ì—ì„œëŠ” Query Rewritingì„ **ì„ íƒì ìœ¼ë¡œ** ì ìš©í•˜ê±°ë‚˜, A/B í…ŒìŠ¤íŠ¸ë¡œ íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history: list[dict] = []) -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG and return the answer and the retrieved context\n",
    "    \"\"\"\n",
    "    query = rewrite_query(question, history)\n",
    "    print(query)\n",
    "    chunks = fetch_context(query)\n",
    "    messages = make_rag_messages(question, history, chunks)\n",
    "    response = completion(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Who won the IIOTY award?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Who went to Manchester University?\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c93b30",
   "metadata": {},
   "source": [
    "## ì •ë¦¬\n",
    "\n",
    "ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ í•™ìŠµí•œ ê³ ê¸‰ RAG ê¸°ë²•ë“¤:\n",
    "\n",
    "### 1. LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì²­í‚¹\n",
    "- ê³ ì • í¬ê¸° ë¶„í•  ëŒ€ì‹  LLMì´ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "- headline + summary + original_text êµ¬ì¡°ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ\n",
    "- Pydanticì„ í™œìš©í•œ Structured Output\n",
    "\n",
    "### 2. Reranking (ì¬ìˆœìœ„í™”)\n",
    "- ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ í›„ LLMì´ ì¬ì •ë ¬\n",
    "- ì˜ë¯¸ì  ê´€ë ¨ì„± ê¸°ë°˜ ìˆœìœ„ ë¶€ì—¬\n",
    "- ê²€ìƒ‰ ì •í™•ë„ ëŒ€í­ í–¥ìƒ\n",
    "\n",
    "### 3. Query Rewriting (ì¿¼ë¦¬ ì¬ì‘ì„±)\n",
    "- ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²€ìƒ‰ ìµœì í™” í˜•íƒœë¡œ ë³€í™˜\n",
    "- ëŒ€í™” ë§¥ë½ ê³ ë ¤\n",
    "- ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "- HyDE (Hypothetical Document Embeddings)\n",
    "- Multi-query RAG\n",
    "- RAG Fusion\n",
    "- Self-RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
