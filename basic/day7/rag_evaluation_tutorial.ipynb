{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 평가(Evaluation) 튜토리얼\n",
    "\n",
    "## 왜 평가가 RAG 아키텍처보다 더 중요한가?\n",
    "\n",
    "RAG(Retrieval-Augmented Generation) 시스템을 구축할 때 많은 개발자들이 **아키텍처 선택**에 집중합니다:\n",
    "- 어떤 임베딩 모델을 사용할까?\n",
    "- 어떤 벡터 데이터베이스가 좋을까?\n",
    "- 청킹 전략은 어떻게 할까?\n",
    "\n",
    "하지만 진짜 중요한 질문은 따로 있습니다:\n",
    "\n",
    "> **\"우리 RAG 시스템이 실제로 잘 작동하는지 어떻게 알 수 있는가?\"**\n",
    "\n",
    "### 평가가 중요한 이유\n",
    "\n",
    "| 관점 | 평가 없이 | 평가 있으면 |\n",
    "|------|----------|------------|\n",
    "| **개발** | 감으로 튜닝 | 데이터 기반 개선 |\n",
    "| **배포** | 불안한 출시 | 자신있는 릴리즈 |\n",
    "| **비즈니스** | ROI 측정 불가 | 명확한 성과 지표 |\n",
    "| **유지보수** | 회귀 버그 발견 어려움 | 지속적인 품질 모니터링 |\n",
    "\n",
    "### 비즈니스 관점에서의 평가 필요성\n",
    "\n",
    "1. **비용 정당화**: RAG 시스템의 성능을 수치로 증명\n",
    "2. **리스크 관리**: 잘못된 정보 제공 시 발생하는 비용 최소화\n",
    "3. **지속적 개선**: A/B 테스트를 통한 점진적 향상\n",
    "4. **고객 신뢰**: 일관된 품질 보장\n",
    "\n",
    "---\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "이 노트북에서는 다음을 배웁니다:\n",
    "\n",
    "1. **테스트 데이터 구조** 설계 방법\n",
    "2. **검색 평가 (Retrieval Evaluation)**: MRR, nDCG, Keyword Coverage\n",
    "3. **답변 평가 (Answer Evaluation)**: LLM-as-a-Judge 패러다임\n",
    "4. **실전 평가** 파이프라인 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "# !pip install pydantic litellm python-dotenv numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API 키 확인\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API 키가 설정되었습니다.\")\n",
    "else:\n",
    "    print(\"경고: OPENAI_API_KEY가 설정되지 않았습니다. .env 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 테스트 데이터 구조\n",
    "\n",
    "### 왜 테스트 데이터가 필요한가?\n",
    "\n",
    "RAG 시스템을 평가하려면 **정답이 있는 테스트 셋**이 필요합니다:\n",
    "\n",
    "```\n",
    "테스트 데이터 = 질문 + 예상 답변 + 관련 문서 정보\n",
    "```\n",
    "\n",
    "좋은 테스트 데이터의 특징:\n",
    "- **다양한 유형**의 질문 포함\n",
    "- **현실적인** 시나리오 반영\n",
    "- **명확한 정답** 존재\n",
    "- **충분한 양** (최소 50-100개 권장)\n",
    "\n",
    "### TestQuestion 클래스 (Pydantic 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 카테고리 타입 정의\n",
    "QuestionCategory = Literal[\n",
    "    \"direct_fact\",    # 직접적인 사실 질문\n",
    "    \"temporal\",       # 시간 관련 질문\n",
    "    \"spanning\",       # 여러 문서에 걸친 질문\n",
    "    \"comparative\",    # 비교 질문\n",
    "    \"numerical\",      # 숫자/수치 질문\n",
    "    \"relationship\",   # 관계 질문\n",
    "    \"holistic\"        # 종합적 이해 질문\n",
    "]\n",
    "\n",
    "\n",
    "class TestQuestion(BaseModel):\n",
    "    \"\"\"\n",
    "    RAG 평가를 위한 테스트 질문 모델\n",
    "    \n",
    "    각 필드 설명:\n",
    "    - id: 질문 고유 식별자\n",
    "    - question: 실제 질문 텍스트\n",
    "    - expected_answer: 예상되는 정답 (참조 답변)\n",
    "    - category: 질문 카테고리\n",
    "    - source_docs: 정답을 찾을 수 있는 문서 ID들\n",
    "    - keywords: 답변에 포함되어야 할 핵심 키워드들\n",
    "    - difficulty: 난이도 (1: 쉬움, 2: 보통, 3: 어려움)\n",
    "    \"\"\"\n",
    "    id: str = Field(..., description=\"질문 고유 식별자\")\n",
    "    question: str = Field(..., description=\"질문 텍스트\")\n",
    "    expected_answer: str = Field(..., description=\"예상 정답\")\n",
    "    category: QuestionCategory = Field(..., description=\"질문 카테고리\")\n",
    "    source_docs: List[str] = Field(default_factory=list, description=\"정답 소스 문서 ID들\")\n",
    "    keywords: List[str] = Field(default_factory=list, description=\"핵심 키워드들\")\n",
    "    difficulty: int = Field(default=1, ge=1, le=3, description=\"난이도 (1-3)\")\n",
    "\n",
    "\n",
    "# 예시 테스트 질문 생성\n",
    "sample_question = TestQuestion(\n",
    "    id=\"q001\",\n",
    "    question=\"Python에서 리스트를 정렬하는 방법은 무엇인가요?\",\n",
    "    expected_answer=\"Python에서 리스트를 정렬하려면 sort() 메서드 또는 sorted() 함수를 사용합니다. sort()는 원본 리스트를 직접 수정하고, sorted()는 새로운 정렬된 리스트를 반환합니다.\",\n",
    "    category=\"direct_fact\",\n",
    "    source_docs=[\"python_basics_001\", \"python_list_002\"],\n",
    "    keywords=[\"sort\", \"sorted\", \"리스트\", \"정렬\"],\n",
    "    difficulty=1\n",
    ")\n",
    "\n",
    "print(\"테스트 질문 예시:\")\n",
    "print(sample_question.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 카테고리 설명\n",
    "\n",
    "다양한 카테고리의 질문으로 테스트해야 RAG 시스템의 전체적인 성능을 파악할 수 있습니다.\n",
    "\n",
    "| 카테고리 | 설명 | 예시 | 권장 비율 |\n",
    "|----------|------|------|----------|\n",
    "| **direct_fact** | 직접적인 사실 질문 | \"Python의 창시자는?\" | 45-50% |\n",
    "| **temporal** | 시간 관련 질문 | \"Python 3.0은 언제 출시됐나?\" | 10-15% |\n",
    "| **spanning** | 여러 문서 연결 | \"Django와 Flask의 차이점은?\" | 10-15% |\n",
    "| **comparative** | 비교 질문 | \"어떤 방법이 더 빠른가?\" | 5-10% |\n",
    "| **numerical** | 수치 관련 | \"최대 연결 수는?\" | 5-10% |\n",
    "| **relationship** | 관계 질문 | \"A는 B와 어떤 관계?\" | 5-10% |\n",
    "| **holistic** | 종합 이해 | \"전체 아키텍처를 설명해주세요\" | 5-10% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 테스트 데이터셋 생성\n",
    "sample_test_data = [\n",
    "    TestQuestion(\n",
    "        id=\"q001\",\n",
    "        question=\"Python에서 리스트를 정렬하는 방법은?\",\n",
    "        expected_answer=\"sort() 메서드 또는 sorted() 함수를 사용합니다.\",\n",
    "        category=\"direct_fact\",\n",
    "        source_docs=[\"doc_001\"],\n",
    "        keywords=[\"sort\", \"sorted\"],\n",
    "        difficulty=1\n",
    "    ),\n",
    "    TestQuestion(\n",
    "        id=\"q002\",\n",
    "        question=\"Python 3.0은 언제 출시되었나요?\",\n",
    "        expected_answer=\"Python 3.0은 2008년 12월 3일에 출시되었습니다.\",\n",
    "        category=\"temporal\",\n",
    "        source_docs=[\"doc_002\"],\n",
    "        keywords=[\"2008\", \"12월\"],\n",
    "        difficulty=1\n",
    "    ),\n",
    "    TestQuestion(\n",
    "        id=\"q003\",\n",
    "        question=\"Django와 Flask의 주요 차이점은 무엇인가요?\",\n",
    "        expected_answer=\"Django는 풀스택 프레임워크로 ORM, 인증 등을 내장하고, Flask는 마이크로 프레임워크로 최소한의 기능만 제공하여 유연성이 높습니다.\",\n",
    "        category=\"spanning\",\n",
    "        source_docs=[\"doc_003\", \"doc_004\"],\n",
    "        keywords=[\"풀스택\", \"마이크로\", \"ORM\", \"유연성\"],\n",
    "        difficulty=2\n",
    "    ),\n",
    "    TestQuestion(\n",
    "        id=\"q004\",\n",
    "        question=\"리스트 컴프리헨션과 for 루프 중 어떤 것이 더 빠른가요?\",\n",
    "        expected_answer=\"일반적으로 리스트 컴프리헨션이 for 루프보다 빠릅니다. 이는 내부적으로 최적화되어 있기 때문입니다.\",\n",
    "        category=\"comparative\",\n",
    "        source_docs=[\"doc_005\"],\n",
    "        keywords=[\"리스트 컴프리헨션\", \"빠르\", \"최적화\"],\n",
    "        difficulty=2\n",
    "    ),\n",
    "    TestQuestion(\n",
    "        id=\"q005\",\n",
    "        question=\"Python의 기본 재귀 깊이 제한은 얼마인가요?\",\n",
    "        expected_answer=\"Python의 기본 재귀 깊이 제한은 1000입니다.\",\n",
    "        category=\"numerical\",\n",
    "        source_docs=[\"doc_006\"],\n",
    "        keywords=[\"1000\", \"재귀\"],\n",
    "        difficulty=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"테스트 데이터셋: {len(sample_test_data)}개 질문\")\n",
    "for q in sample_test_data:\n",
    "    print(f\"  [{q.category}] {q.question[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 검색 평가 (Retrieval Evaluation)\n",
    "\n",
    "검색 평가는 RAG의 첫 번째 단계인 **문서 검색**의 품질을 측정합니다.\n",
    "\n",
    "핵심 질문:\n",
    "> \"검색된 문서가 질문에 대한 답을 찾기에 충분한가?\"\n",
    "\n",
    "### 3.1 MRR (Mean Reciprocal Rank)\n",
    "\n",
    "**핵심 아이디어**: \"첫 번째 정답이 몇 번째에 나왔는가?\"\n",
    "\n",
    "#### 수식\n",
    "\n",
    "$$MRR = \\frac{1}{|Q|} \\times \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}$$\n",
    "\n",
    "- $|Q|$: 전체 쿼리 수\n",
    "- $rank_i$: i번째 쿼리에서 첫 번째 정답의 순위\n",
    "\n",
    "#### 점수 해석\n",
    "\n",
    "| 정답 위치 | 점수 |\n",
    "|-----------|------|\n",
    "| 1등 | 1/1 = **1.0** |\n",
    "| 2등 | 1/2 = **0.5** |\n",
    "| 3등 | 1/3 = **0.33** |\n",
    "| 10등 | 1/10 = **0.1** |\n",
    "| 정답 없음 | **0.0** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_docs: List[str], relevant_docs: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    MRR (Mean Reciprocal Rank) 계산\n",
    "    \n",
    "    단일 쿼리에 대한 Reciprocal Rank를 계산합니다.\n",
    "    여러 쿼리의 평균을 구하면 MRR이 됩니다.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: 검색된 문서 ID 리스트 (순서대로)\n",
    "        relevant_docs: 관련 문서(정답) ID 리스트\n",
    "    \n",
    "    Returns:\n",
    "        첫번째 정답에 대해서만 고려함.\n",
    "        Reciprocal Rank (0~1, 정답이 없으면 0)\n",
    "    \"\"\"\n",
    "    relevant_set = set(relevant_docs)\n",
    "    \n",
    "    for rank, doc_id in enumerate(retrieved_docs, start=1):\n",
    "        if doc_id in relevant_set:\n",
    "            return 1.0 / rank\n",
    "    \n",
    "    return 0.0  # 정답 문서를 찾지 못한 경우\n",
    "\n",
    "\n",
    "# 예시\n",
    "retrieved = [\"doc_A\", \"doc_B\", \"doc_C\", \"doc_D\", \"doc_E\"]\n",
    "relevant = [\"doc_C\", \"doc_F\"]  # 정답 문서들\n",
    "\n",
    "mrr = calculate_mrr(retrieved, relevant)\n",
    "print(f\"검색된 문서: {retrieved}\")\n",
    "print(f\"정답 문서: {relevant}\")\n",
    "print(f\"Reciprocal Rank: {mrr:.3f}\")\n",
    "print(f\"\\n해석: 첫 번째 정답(doc_C)이 3번째에 있으므로 RR = 1/3 = 0.333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 nDCG (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "**핵심 아이디어**: \"관련성 높은 문서가 위에 있을수록 좋다\"\n",
    "\n",
    "MRR과의 차이점:\n",
    "- MRR: 첫 번째 정답만 고려\n",
    "- nDCG: **여러 정답의 순서**까지 고려\n",
    "\n",
    "#### 계산 과정\n",
    "\n",
    "1. **DCG (Discounted Cumulative Gain)** 계산:\n",
    "$$DCG = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}$$\n",
    "\n",
    "2. **IDCG (Ideal DCG)** 계산:\n",
    "   - 관련성 점수를 내림차순 정렬 후 DCG 계산\n",
    "\n",
    "3. **nDCG** 계산:\n",
    "$$nDCG = \\frac{DCG}{IDCG}$$\n",
    "\n",
    "#### Binary Relevance\n",
    "\n",
    "RAG 평가에서는 주로 **binary relevance** (0 또는 1)를 사용:\n",
    "- 관련 문서: 1\n",
    "- 비관련 문서: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dcg(relevances: List[int], k: int = None) -> float:\n",
    "    \"\"\"\n",
    "    DCG (Discounted Cumulative Gain) 계산\n",
    "    \n",
    "    Args:\n",
    "        relevances: 각 문서의 관련성 점수 (순서대로)\n",
    "        k: 상위 k개만 고려 (None이면 전체)\n",
    "    \n",
    "    Returns:\n",
    "        DCG 점수\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(relevances)\n",
    "    \n",
    "    dcg = 0.0\n",
    "    for i in range(min(k, len(relevances))):\n",
    "        # 위치 i+1의 할인율 적용 (로그 베이스 2)\n",
    "        dcg += relevances[i] / np.log2(i + 2)  # i+2는 position이 1부터 시작하기 때문\n",
    "    \n",
    "    return dcg\n",
    "\n",
    "\n",
    "def calculate_ndcg(retrieved_docs: List[str], relevant_docs: List[str], k: int = None) -> float:\n",
    "    \"\"\"\n",
    "    nDCG (Normalized DCG) 계산 - Binary Relevance 버전\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: 검색된 문서 ID 리스트 (순서대로)\n",
    "        relevant_docs: 관련 문서(정답) ID 리스트\n",
    "        k: 상위 k개만 고려\n",
    "    \n",
    "    Returns:\n",
    "        nDCG 점수 (0~1)\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(retrieved_docs)\n",
    "    \n",
    "    relevant_set = set(relevant_docs)\n",
    "    \n",
    "    # Binary relevance 계산\n",
    "    relevances = [1 if doc in relevant_set else 0 for doc in retrieved_docs[:k]]\n",
    "    \n",
    "    # 실제 DCG 계산\n",
    "    dcg = calculate_dcg(relevances, k)\n",
    "    \n",
    "    # 이상적인 순서: 관련 문서가 모두 앞에 오는 경우\n",
    "    ideal_relevances = sorted(relevances, reverse=True)\n",
    "    idcg = calculate_dcg(ideal_relevances, k)\n",
    "    \n",
    "    # IDCG가 0이면 관련 문서가 없음\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "# 예시\n",
    "retrieved = [\"doc_A\", \"doc_B\", \"doc_C\", \"doc_D\", \"doc_E\"]\n",
    "relevant = [\"doc_A\", \"doc_C\", \"doc_E\"]  # 1, 3, 5번째가 정답\n",
    "\n",
    "ndcg = calculate_ndcg(retrieved, relevant, k=5)\n",
    "print(f\"검색된 문서: {retrieved}\")\n",
    "print(f\"정답 문서: {relevant}\")\n",
    "print(f\"\\nRelevance 패턴: [1, 0, 1, 0, 1]\")\n",
    "print(f\"이상적 패턴:    [1, 1, 1, 0, 0]\")\n",
    "print(f\"\\nnDCG@5: {ndcg:.3f}\")\n",
    "print(f\"\\n해석: 정답 문서들이 완벽한 순서가 아니므로 1.0보다 낮음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Keyword Coverage\n",
    "\n",
    "**핵심 아이디어**: \"검색된 문서에서 핵심 키워드가 얼마나 발견되는가?\"\n",
    "\n",
    "LLM 기반 평가 없이 빠르게 검색 품질을 확인할 수 있는 휴리스틱 지표입니다.\n",
    "\n",
    "$$Keyword\\ Coverage = \\frac{\\text{발견된 키워드 수}}{\\text{전체 키워드 수}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyword_coverage(retrieved_content: str, keywords: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    키워드 커버리지 계산\n",
    "    \n",
    "    Args:\n",
    "        retrieved_content: 검색된 문서들의 내용 (연결된 텍스트)\n",
    "        keywords: 찾아야 할 키워드 리스트\n",
    "    \n",
    "    Returns:\n",
    "        커버리지 비율 (0~1)\n",
    "    \"\"\"\n",
    "    if not keywords:\n",
    "        return 1.0  # 키워드가 없으면 100%로 간주\n",
    "    \n",
    "    content_lower = retrieved_content.lower()\n",
    "    found_count = sum(1 for kw in keywords if kw.lower() in content_lower)\n",
    "    \n",
    "    return found_count / len(keywords)\n",
    "\n",
    "\n",
    "# 예시\n",
    "document_content = \"\"\"\n",
    "Python에서 리스트를 정렬하는 방법은 두 가지가 있습니다.\n",
    "sort() 메서드는 원본 리스트를 직접 수정합니다.\n",
    "sorted() 함수는 새로운 정렬된 리스트를 반환합니다.\n",
    "\"\"\"\n",
    "\n",
    "keywords = [\"sort\", \"sorted\", \"리스트\", \"정렬\", \"reverse\"]  # reverse는 없음\n",
    "\n",
    "coverage = calculate_keyword_coverage(document_content, keywords)\n",
    "print(f\"키워드: {keywords}\")\n",
    "print(f\"Keyword Coverage: {coverage:.1%}\")\n",
    "print(f\"\\n해석: 5개 중 4개 키워드가 발견됨 (reverse 누락)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 RetrievalEval 종합 클래스\n",
    "\n",
    "모든 검색 평가 지표를 하나로 묶은 Pydantic 모델입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEval(BaseModel):\n",
    "    \"\"\"\n",
    "    검색 평가 결과 모델\n",
    "    \n",
    "    RAG 시스템의 검색 단계 성능을 종합적으로 평가합니다.\n",
    "    \"\"\"\n",
    "    mrr: float = Field(..., ge=0, le=1, description=\"Mean Reciprocal Rank\")\n",
    "    ndcg: float = Field(..., ge=0, le=1, description=\"Normalized DCG\")\n",
    "    keyword_coverage: float = Field(..., ge=0, le=1, description=\"키워드 커버리지\")\n",
    "    retrieved_count: int = Field(..., ge=0, description=\"검색된 문서 수\")\n",
    "    relevant_found: int = Field(..., ge=0, description=\"찾은 관련 문서 수\")\n",
    "    \n",
    "    @property\n",
    "    def overall_score(self) -> float:\n",
    "        \"\"\"종합 점수 (가중 평균)\"\"\"\n",
    "        return 0.4 * self.mrr + 0.4 * self.ndcg + 0.2 * self.keyword_coverage\n",
    "\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    retrieved_docs: List[str],\n",
    "    relevant_docs: List[str],\n",
    "    retrieved_content: str,\n",
    "    keywords: List[str],\n",
    "    k: int = 5\n",
    ") -> RetrievalEval:\n",
    "    \"\"\"\n",
    "    검색 평가 수행\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: 검색된 문서 ID 리스트\n",
    "        relevant_docs: 관련 문서(정답) ID 리스트\n",
    "        retrieved_content: 검색된 문서들의 내용\n",
    "        keywords: 핵심 키워드 리스트\n",
    "        k: nDCG 계산에 사용할 k\n",
    "    \n",
    "    Returns:\n",
    "        RetrievalEval 객체\n",
    "    \"\"\"\n",
    "    relevant_set = set(relevant_docs)\n",
    "    relevant_found = sum(1 for doc in retrieved_docs if doc in relevant_set)\n",
    "    \n",
    "    return RetrievalEval(\n",
    "        mrr=calculate_mrr(retrieved_docs, relevant_docs),\n",
    "        ndcg=calculate_ndcg(retrieved_docs, relevant_docs, k),\n",
    "        keyword_coverage=calculate_keyword_coverage(retrieved_content, keywords),\n",
    "        retrieved_count=len(retrieved_docs),\n",
    "        relevant_found=relevant_found\n",
    "    )\n",
    "\n",
    "\n",
    "# 종합 평가 예시\n",
    "eval_result = evaluate_retrieval(\n",
    "    retrieved_docs=[\"doc_A\", \"doc_B\", \"doc_C\", \"doc_D\", \"doc_E\"],\n",
    "    relevant_docs=[\"doc_A\", \"doc_C\"],\n",
    "    retrieved_content=\"Python에서 sort()와 sorted() 함수를 사용하여 리스트를 정렬할 수 있습니다.\",\n",
    "    keywords=[\"sort\", \"sorted\", \"리스트\", \"정렬\"],\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"검색 평가 결과:\")\n",
    "print(f\"  MRR: {eval_result.mrr:.3f}\")\n",
    "print(f\"  nDCG@5: {eval_result.ndcg:.3f}\")\n",
    "print(f\"  Keyword Coverage: {eval_result.keyword_coverage:.1%}\")\n",
    "print(f\"  관련 문서 발견: {eval_result.relevant_found}/{len(eval_result.retrieved_count)}개 중\")\n",
    "print(f\"\\n종합 점수: {eval_result.overall_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 답변 평가 (Answer Evaluation)\n",
    "\n",
    "검색이 잘 되었더라도, **최종 답변의 품질**이 좋아야 합니다.\n",
    "\n",
    "### 4.1 LLM-as-a-Judge 패러다임\n",
    "\n",
    "**핵심 아이디어**: LLM을 평가자로 사용하여 답변 품질 측정\n",
    "\n",
    "#### 장점\n",
    "- **스케일러블**: 대량의 평가를 자동화\n",
    "- **일관성**: 동일 기준으로 평가\n",
    "- **세밀한 피드백**: 왜 좋은지/나쁜지 설명 가능\n",
    "- **다양한 기준**: 정확도, 완전성, 관련성 등 여러 측면 평가\n",
    "\n",
    "#### 단점\n",
    "- **비용**: API 호출 비용 발생\n",
    "- **편향 가능성**: LLM 자체의 편향 반영\n",
    "- **일관성 한계**: 같은 입력에 다른 결과 가능\n",
    "\n",
    "### 4.2 평가 기준 3가지\n",
    "\n",
    "| 기준 | 설명 | 예시 질문 |\n",
    "|------|------|----------|\n",
    "| **Accuracy (정확도)** | 사실적으로 얼마나 정확한가? | \"답변이 사실과 일치하나요?\" |\n",
    "| **Completeness (완전성)** | 질문의 모든 측면을 다루는가? | \"모든 부분을 답변했나요?\" |\n",
    "| **Relevance (관련성)** | 질문에 직접적으로 답변하는가? | \"질문에 맞는 답변인가요?\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEval(BaseModel):\n",
    "    \"\"\"\n",
    "    답변 평가 결과 모델\n",
    "    \n",
    "    LLM-as-a-Judge가 생성한 평가 결과를 구조화합니다.\n",
    "    \"\"\"\n",
    "    accuracy: int = Field(..., ge=1, le=5, description=\"정확도 점수 (1-5)\")\n",
    "    completeness: int = Field(..., ge=1, le=5, description=\"완전성 점수 (1-5)\")\n",
    "    relevance: int = Field(..., ge=1, le=5, description=\"관련성 점수 (1-5)\")\n",
    "    feedback: str = Field(..., description=\"상세 피드백\")\n",
    "    \n",
    "    @property\n",
    "    def average_score(self) -> float:\n",
    "        \"\"\"평균 점수\"\"\"\n",
    "        return (self.accuracy + self.completeness + self.relevance) / 3\n",
    "    \n",
    "    @property\n",
    "    def normalized_score(self) -> float:\n",
    "        \"\"\"정규화된 점수 (0-1 범위)\"\"\"\n",
    "        return (self.average_score - 1) / 4  # 1-5를 0-1로 변환\n",
    "\n",
    "\n",
    "# 예시 평가 결과\n",
    "sample_eval = AnswerEval(\n",
    "    accuracy=4,\n",
    "    completeness=3,\n",
    "    relevance=5,\n",
    "    feedback=\"답변이 질문에 잘 맞지만, 추가 예제가 있으면 더 좋겠습니다.\"\n",
    ")\n",
    "\n",
    "print(\"답변 평가 예시:\")\n",
    "print(f\"  정확도: {sample_eval.accuracy}/5\")\n",
    "print(f\"  완전성: {sample_eval.completeness}/5\")\n",
    "print(f\"  관련성: {sample_eval.relevance}/5\")\n",
    "print(f\"  평균 점수: {sample_eval.average_score:.2f}/5\")\n",
    "print(f\"  정규화 점수: {sample_eval.normalized_score:.2f}\")\n",
    "print(f\"  피드백: {sample_eval.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Judge 프롬프트 설계\n",
    "\n",
    "LLM-as-a-Judge의 핵심은 **잘 설계된 프롬프트**입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "당신은 RAG 시스템의 답변 품질을 평가하는 전문 평가자입니다.\n",
    "\n",
    "주어진 질문, 생성된 답변, 참조 답변을 비교하여 다음 세 가지 기준으로 평가하세요:\n",
    "\n",
    "1. **정확도 (Accuracy)** [1-5점]\n",
    "   - 5점: 완전히 정확함, 사실적 오류 없음\n",
    "   - 4점: 대부분 정확함, 사소한 오류\n",
    "   - 3점: 부분적으로 정확함, 일부 오류\n",
    "   - 2점: 많은 오류 포함\n",
    "   - 1점: 대부분 부정확함\n",
    "\n",
    "2. **완전성 (Completeness)** [1-5점]\n",
    "   - 5점: 질문의 모든 측면을 완벽히 다룸\n",
    "   - 4점: 대부분의 측면을 다룸\n",
    "   - 3점: 핵심만 다룸, 일부 누락\n",
    "   - 2점: 많은 부분 누락\n",
    "   - 1점: 거의 답변하지 않음\n",
    "\n",
    "3. **관련성 (Relevance)** [1-5점]\n",
    "   - 5점: 질문에 직접적이고 정확하게 답변\n",
    "   - 4점: 대부분 관련있는 답변\n",
    "   - 3점: 부분적으로 관련있음\n",
    "   - 2점: 관련성 낮음\n",
    "   - 1점: 완전히 동떨어진 답변\n",
    "\n",
    "반드시 JSON 형식으로 응답하세요.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_judge_user_prompt(question: str, generated_answer: str, reference_answer: str) -> str:\n",
    "    \"\"\"Judge에게 전달할 User 프롬프트 생성\"\"\"\n",
    "    return f\"\"\"\n",
    "## 평가 대상\n",
    "\n",
    "### 질문\n",
    "{question}\n",
    "\n",
    "### 생성된 답변 (평가 대상)\n",
    "{generated_answer}\n",
    "\n",
    "### 참조 답변 (정답)\n",
    "{reference_answer}\n",
    "\n",
    "## 요청\n",
    "\n",
    "위 정보를 바탕으로 생성된 답변을 평가해주세요.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 프롬프트 예시 출력\n",
    "print(\"=\" * 60)\n",
    "print(\"System Prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(JUDGE_SYSTEM_PROMPT[:500] + \"...\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"User Prompt 예시:\")\n",
    "print(\"=\" * 60)\n",
    "print(create_judge_user_prompt(\n",
    "    question=\"Python에서 리스트 정렬 방법은?\",\n",
    "    generated_answer=\"sort() 함수를 사용합니다.\",\n",
    "    reference_answer=\"sort() 메서드와 sorted() 함수를 사용할 수 있습니다.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 LLM Judge 구현\n",
    "\n",
    "LiteLLM을 사용하여 다양한 LLM 제공자를 지원하는 Judge를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import json\n",
    "\n",
    "\n",
    "def evaluate_answer_with_llm(\n",
    "    question: str,\n",
    "    generated_answer: str,\n",
    "    reference_answer: str,\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> AnswerEval:\n",
    "    \"\"\"\n",
    "    LLM을 사용하여 답변 품질 평가\n",
    "    \n",
    "    Args:\n",
    "        question: 원본 질문\n",
    "        generated_answer: RAG 시스템이 생성한 답변\n",
    "        reference_answer: 정답 (기대 답변)\n",
    "        model: 사용할 LLM 모델\n",
    "    \n",
    "    Returns:\n",
    "        AnswerEval 객체\n",
    "    \"\"\"\n",
    "    user_prompt = create_judge_user_prompt(question, generated_answer, reference_answer)\n",
    "    \n",
    "    # Structured Output을 위한 response_format\n",
    "    response = completion(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"answer_evaluation\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"accuracy\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"정확도 점수 (1-5)\"\n",
    "                        },\n",
    "                        \"completeness\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"완전성 점수 (1-5)\"\n",
    "                        },\n",
    "                        \"relevance\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"관련성 점수 (1-5)\"\n",
    "                        },\n",
    "                        \"feedback\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"상세 피드백\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"accuracy\", \"completeness\", \"relevance\", \"feedback\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        temperature=0  # 일관된 평가를 위해 temperature 0\n",
    "    )\n",
    "    \n",
    "    # JSON 파싱\n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    return AnswerEval(**result)\n",
    "\n",
    "\n",
    "print(\"LLM Judge 함수가 정의되었습니다.\")\n",
    "print(\"\\n다음 섹션에서 실제 평가를 실행합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 실습: 평가 실행\n",
    "\n",
    "지금까지 배운 내용을 종합하여 실제 평가를 수행해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 시나리오 설정\n",
    "test_case = {\n",
    "    \"question\": \"Python에서 리스트를 정렬하는 방법은 무엇인가요?\",\n",
    "    \"reference_answer\": \"Python에서 리스트를 정렬하려면 sort() 메서드 또는 sorted() 함수를 사용합니다. sort()는 원본 리스트를 직접 수정하고, sorted()는 새로운 정렬된 리스트를 반환합니다.\",\n",
    "    \"keywords\": [\"sort\", \"sorted\", \"리스트\", \"정렬\"],\n",
    "    \"relevant_docs\": [\"python_list_001\", \"python_basics_002\"]\n",
    "}\n",
    "\n",
    "# 가상의 RAG 시스템 출력 (실제로는 RAG 파이프라인에서 생성)\n",
    "rag_output = {\n",
    "    \"retrieved_docs\": [\"python_list_001\", \"python_intro_003\", \"python_basics_002\", \"django_001\", \"flask_002\"],\n",
    "    \"retrieved_content\": \"\"\"Python에서 리스트 정렬은 sort() 메서드를 사용합니다. \n",
    "    이 메서드는 원본 리스트를 직접 수정합니다. sorted() 함수도 있습니다.\"\"\",\n",
    "    \"generated_answer\": \"Python에서 리스트를 정렬하려면 sort() 메서드를 사용합니다. 이 메서드는 원본 리스트를 직접 수정합니다.\"\n",
    "}\n",
    "\n",
    "print(\"테스트 케이스:\")\n",
    "print(f\"  질문: {test_case['question']}\")\n",
    "print(f\"  정답 문서: {test_case['relevant_docs']}\")\n",
    "print(f\"\\nRAG 출력:\")\n",
    "print(f\"  검색된 문서: {rag_output['retrieved_docs']}\")\n",
    "print(f\"  생성된 답변: {rag_output['generated_answer'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 평가 실행\n",
    "retrieval_eval = evaluate_retrieval(\n",
    "    retrieved_docs=rag_output[\"retrieved_docs\"],\n",
    "    relevant_docs=test_case[\"relevant_docs\"],\n",
    "    retrieved_content=rag_output[\"retrieved_content\"],\n",
    "    keywords=test_case[\"keywords\"],\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"검색 평가 결과 (Retrieval Evaluation)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMRR: {retrieval_eval.mrr:.3f}\")\n",
    "print(f\"  -> 첫 번째 정답(python_list_001)이 1번째에 있어 최고 점수\")\n",
    "\n",
    "print(f\"\\nnDCG@5: {retrieval_eval.ndcg:.3f}\")\n",
    "print(f\"  -> 정답 2개(1,3번째)가 상위에 있어 좋은 점수\")\n",
    "\n",
    "print(f\"\\nKeyword Coverage: {retrieval_eval.keyword_coverage:.1%}\")\n",
    "print(f\"  -> 4개 키워드 중 {int(retrieval_eval.keyword_coverage * 4)}개 발견\")\n",
    "\n",
    "print(f\"\\n관련 문서 발견: {retrieval_eval.relevant_found}/{len(test_case['relevant_docs'])}개\")\n",
    "print(f\"\\n종합 점수: {retrieval_eval.overall_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 평가 실행 (LLM API 호출)\n",
    "# 주의: 이 셀은 API 키가 설정되어 있어야 실행됩니다.\n",
    "\n",
    "try:\n",
    "    answer_eval = evaluate_answer_with_llm(\n",
    "        question=test_case[\"question\"],\n",
    "        generated_answer=rag_output[\"generated_answer\"],\n",
    "        reference_answer=test_case[\"reference_answer\"],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"답변 평가 결과 (Answer Evaluation by LLM Judge)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n정확도 (Accuracy): {answer_eval.accuracy}/5\")\n",
    "    print(f\"완전성 (Completeness): {answer_eval.completeness}/5\")\n",
    "    print(f\"관련성 (Relevance): {answer_eval.relevance}/5\")\n",
    "    print(f\"\\n평균 점수: {answer_eval.average_score:.2f}/5\")\n",
    "    print(f\"정규화 점수: {answer_eval.normalized_score:.2f}\")\n",
    "    print(f\"\\n피드백: {answer_eval.feedback}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"API 호출 실패: {e}\")\n",
    "    print(\"\\nAPI 키를 확인하거나, 아래 모의 결과를 참고하세요:\")\n",
    "    \n",
    "    # 모의 결과\n",
    "    mock_eval = AnswerEval(\n",
    "        accuracy=4,\n",
    "        completeness=3,\n",
    "        relevance=5,\n",
    "        feedback=\"답변이 정확하고 관련성이 높지만, sorted() 함수에 대한 설명이 누락되어 완전성이 다소 낮습니다.\"\n",
    "    )\n",
    "    print(f\"\\n[모의 결과]\")\n",
    "    print(f\"정확도: {mock_eval.accuracy}/5\")\n",
    "    print(f\"완전성: {mock_eval.completeness}/5\")\n",
    "    print(f\"관련성: {mock_eval.relevance}/5\")\n",
    "    print(f\"피드백: {mock_eval.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 종합 평가 리포트 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluationReport(BaseModel):\n",
    "    \"\"\"\n",
    "    RAG 시스템 종합 평가 리포트\n",
    "    \"\"\"\n",
    "    question_id: str\n",
    "    question: str\n",
    "    category: QuestionCategory\n",
    "    retrieval: RetrievalEval\n",
    "    answer: Optional[AnswerEval] = None\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"평가 리포트 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"RAG 평가 리포트: {self.question_id}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(f\"\\n질문: {self.question}\")\n",
    "        print(f\"카테고리: {self.category}\")\n",
    "        \n",
    "        print(\"\\n[검색 평가]\")\n",
    "        print(f\"  MRR: {self.retrieval.mrr:.3f}\")\n",
    "        print(f\"  nDCG: {self.retrieval.ndcg:.3f}\")\n",
    "        print(f\"  Keyword Coverage: {self.retrieval.keyword_coverage:.1%}\")\n",
    "        print(f\"  종합: {self.retrieval.overall_score:.3f}\")\n",
    "        \n",
    "        if self.answer:\n",
    "            print(\"\\n[답변 평가]\")\n",
    "            print(f\"  정확도: {self.answer.accuracy}/5\")\n",
    "            print(f\"  완전성: {self.answer.completeness}/5\")\n",
    "            print(f\"  관련성: {self.answer.relevance}/5\")\n",
    "            print(f\"  평균: {self.answer.average_score:.2f}/5\")\n",
    "            print(f\"\\n  피드백: {self.answer.feedback}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "# 종합 리포트 생성 예시\n",
    "report = RAGEvaluationReport(\n",
    "    question_id=\"q001\",\n",
    "    question=test_case[\"question\"],\n",
    "    category=\"direct_fact\",\n",
    "    retrieval=retrieval_eval,\n",
    "    answer=AnswerEval(\n",
    "        accuracy=4,\n",
    "        completeness=3,\n",
    "        relevance=5,\n",
    "        feedback=\"답변이 정확하고 관련성이 높지만, sorted() 함수 설명이 누락됨\"\n",
    "    )\n",
    ")\n",
    "\n",
    "report.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 결론 및 Best Practices\n",
    "\n",
    "### 핵심 정리\n",
    "\n",
    "| 평가 단계 | 주요 지표 | 목적 |\n",
    "|----------|----------|------|\n",
    "| **검색 평가** | MRR, nDCG, Coverage | 올바른 문서를 찾았는가? |\n",
    "| **답변 평가** | Accuracy, Completeness, Relevance | 올바른 답변을 생성했는가? |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **평가 자동화**\n",
    "   - CI/CD 파이프라인에 평가 포함\n",
    "   - 배포 전 성능 회귀 테스트\n",
    "\n",
    "2. **다양한 질문 카테고리**\n",
    "   - direct_fact만으로는 부족\n",
    "   - comparative, spanning 등 어려운 질문도 테스트\n",
    "\n",
    "3. **정기적인 평가**\n",
    "   - 문서 업데이트 시 재평가\n",
    "   - 모델 변경 시 비교 평가\n",
    "\n",
    "4. **평가 데이터 품질**\n",
    "   - 전문가 검토된 테스트 셋\n",
    "   - 실제 사용자 질문 반영\n",
    "\n",
    "5. **개선 루프 구축**\n",
    "   - 평가 -> 분석 -> 개선 -> 재평가\n",
    "   - 실패 케이스 상세 분석\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "1. 더 큰 테스트 셋 구축 (100개 이상)\n",
    "2. Ragas, DeepEval 등 전문 평가 프레임워크 활용\n",
    "3. Human evaluation과 LLM evaluation 비교\n",
    "4. 평가 대시보드 구축\n",
    "\n",
    "---\n",
    "\n",
    "## 참고 자료\n",
    "\n",
    "- [Ragas: RAG 평가 프레임워크](https://github.com/explodinggradients/ragas)\n",
    "- [DeepEval: LLM 평가 도구](https://github.com/confident-ai/deepeval)\n",
    "- [LangChain Evaluation](https://python.langchain.com/docs/guides/evaluation)\n",
    "- [BEIR Benchmark](https://github.com/beir-cellar/beir)\n",
    "\n",
    "---\n",
    "\n",
    "**이 노트북을 완료하셨습니다!**\n",
    "\n",
    "이제 여러분은:\n",
    "- 테스트 데이터를 설계할 수 있습니다\n",
    "- 검색 품질을 MRR, nDCG로 평가할 수 있습니다\n",
    "- LLM-as-a-Judge로 답변 품질을 평가할 수 있습니다\n",
    "- 종합 평가 리포트를 생성할 수 있습니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
