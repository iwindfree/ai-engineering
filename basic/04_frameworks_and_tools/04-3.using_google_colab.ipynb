{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab ì‚¬ìš©í•´ë³´ê¸°: ë¬´ë£Œë¡œ GPU í™˜ê²½ì—ì„œ AI ëª¨ë¸ ì‹¤í–‰í•˜ê¸°\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/google_colab_gpu_guide.ipynb)\n",
    "\n",
    "## ê°œìš”\n",
    "\n",
    "**Google Colab**ì€ Googleì´ ì œê³µí•˜ëŠ” ë¬´ë£Œ í´ë¼ìš°ë“œ Jupyter Notebook í™˜ê²½ì…ë‹ˆë‹¤. ë³„ë„ì˜ ì„¤ì¹˜ ì—†ì´ ë¸Œë¼ìš°ì €ì—ì„œ Python ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, **ë¬´ë£Œ GPU**ë¥¼ ì œê³µí•©ë‹ˆë‹¤!\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "- âœ… Google Colab GPU ëŸ°íƒ€ì„ ì„¤ì •í•˜ê¸°\n",
    "- âœ… Colab Secretsìœ¼ë¡œ API í‚¤ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ê¸°\n",
    "- âœ… Hugging Face ëª¨ë¸ì„ GPUë¡œ ì‹¤í–‰í•˜ê¸°\n",
    "- âœ… OpenAI APIë¥¼ Colabì—ì„œ ì‚¬ìš©í•˜ê¸°\n",
    "- âœ… CPU vs GPU ì„±ëŠ¥ ë¹„êµí•˜ê¸°\n",
    "\n",
    "## Google Colabì´ë€?\n",
    "\n",
    "| íŠ¹ì§• | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **ë¬´ë£Œ GPU** | NVIDIA Tesla T4 GPU ë¬´ë£Œ ì œê³µ |\n",
    "| **ì„¤ì¹˜ ë¶ˆí•„ìš”** | ë¸Œë¼ìš°ì €ë§Œ ìˆìœ¼ë©´ ì¦‰ì‹œ ì‹œì‘ |\n",
    "| **Google Drive ì—°ë™** | íŒŒì¼ ì €ì¥ ë° ê³µìœ  ê°„í¸ |\n",
    "| **í˜‘ì—…** | ë§í¬ ê³µìœ ë¡œ ì‹¤ì‹œê°„ í˜‘ì—… ê°€ëŠ¥ |\n",
    "| **ì‚¬ì „ ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬** | TensorFlow, PyTorch ë“± ì´ë¯¸ ì„¤ì¹˜ë¨ |\n",
    "\n",
    "### ë¬´ë£Œ vs Colab Pro ë¹„êµ\n",
    "\n",
    "| êµ¬ë¶„ | ë¬´ë£Œ | Colab Pro | Colab Pro+ |\n",
    "|------|------|-----------|------------|\n",
    "| **ê°€ê²©** | $0 | $9.99/ì›” | $49.99/ì›” |\n",
    "| **GPU** | T4 (ì œí•œì ) | T4, V100, A100 | V100, A100 (ìš°ì„  í• ë‹¹) |\n",
    "| **ì„¸ì…˜ ì‹œê°„** | 12ì‹œê°„ | 24ì‹œê°„ | 24ì‹œê°„ |\n",
    "| **ìœ íœ´ ì‹œê°„** | 90ë¶„ | ë” ê¸´ ìœ íœ´ í—ˆìš© | ë” ê¸´ ìœ íœ´ í—ˆìš© |\n",
    "| **ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰** | âŒ | âœ… | âœ… |\n",
    "\n",
    "> ğŸ’¡ **í•™ìŠµ ë° ì‹¤í—˜ ëª©ì **ì´ë¼ë©´ ë¬´ë£Œ ë²„ì „ìœ¼ë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. GPU ëŸ°íƒ€ì„ ì„¤ì •í•˜ê¸°\n",
    "\n",
    "Colabì€ ê¸°ë³¸ì ìœ¼ë¡œ CPU ëŸ°íƒ€ì„ìœ¼ë¡œ ì‹œì‘ë©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ëŸ°íƒ€ì„ ìœ í˜•ì„ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“Œ GPU í™œì„±í™” ë‹¨ê³„\n",
    "\n",
    "1. ìƒë‹¨ ë©”ë‰´: **ëŸ°íƒ€ì„(Runtime)** â†’ **ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½(Change runtime type)**\n",
    "2. **í•˜ë“œì›¨ì–´ ê°€ì†ê¸°(Hardware accelerator)**: **GPU** ì„ íƒ\n",
    "3. **GPU ìœ í˜•**: T4 (ë¬´ë£Œ ë²„ì „ì€ ìë™ ì„ íƒ)\n",
    "4. **ì €ì¥(Save)** í´ë¦­\n",
    "\n",
    "ëŸ°íƒ€ì„ì´ ì¬ì‹œì‘ë˜ë©° GPUê°€ í• ë‹¹ë©ë‹ˆë‹¤.\n",
    "\n",
    "### GPU í• ë‹¹ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ì •ë³´ í™•ì¸ (NVIDIA System Management Interface)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¶œë ¥ ì˜ˆì‹œ:**\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   36C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "```\n",
    "\n",
    "- **GPU ì´ë¦„**: Tesla T4\n",
    "- **VRAM**: ì•½ 15GB\n",
    "- **í˜„ì¬ ì‚¬ìš©ëŸ‰**: 0MiB (ì´ˆê¸° ìƒíƒœ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchë¡œ GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥!\")\n",
    "    print(f\"ğŸ“¦ GPU ì´ë¦„: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ VRAM ì´ëŸ‰: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŸ°íƒ€ì„ ìœ í˜•ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Colab Secretsìœ¼ë¡œ API í‚¤ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ê¸° ğŸ”‘\n",
    "\n",
    "API í‚¤ë¥¼ ì½”ë“œì— ì§ì ‘ ì…ë ¥í•˜ë©´ **ë³´ì•ˆ ìœ„í—˜**ì´ ìˆìŠµë‹ˆë‹¤. Google Colabì˜ **Secrets** ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ì•ˆì „í•˜ê²Œ í‚¤ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“Œ Colab Secrets ì‚¬ìš© ë°©ë²•\n",
    "\n",
    "#### 1ë‹¨ê³„: Secrets ì¶”ê°€í•˜ê¸°\n",
    "\n",
    "1. **ì¢Œì¸¡ ì‚¬ì´ë“œë°”**ì˜ **ì—´ì‡  ì•„ì´ì½˜(ğŸ”‘)** í´ë¦­\n",
    "2. **+ Add new secret** ë²„íŠ¼ í´ë¦­\n",
    "3. ë‹¤ìŒ ì •ë³´ ì…ë ¥:\n",
    "\n",
    "**Hugging Face Token:**\n",
    "- **Name**: `HF_TOKEN`\n",
    "- **Value**: `hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx` (ë³¸ì¸ì˜ í† í°)\n",
    "\n",
    "**OpenAI API Key:**\n",
    "- **Name**: `OPENAI_API_KEY`\n",
    "- **Value**: `sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx` (ë³¸ì¸ì˜ í‚¤)\n",
    "\n",
    "4. **ì €ì¥ í›„ ì¤‘ìš”!** â†’ ê° Secret ì˜†ì˜ **\"ë…¸íŠ¸ë¶ ì•¡ì„¸ìŠ¤\" í† ê¸€ì„ ì¼œê¸°** âš ï¸\n",
    "\n",
    "#### 2ë‹¨ê³„: Pythonì—ì„œ Secrets ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "```python\n",
    "from google.colab import userdata\n",
    "\n",
    "# Secretsì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "openai_key = userdata.get('OPENAI_API_KEY')\n",
    "```\n",
    "\n",
    "### ì™œ Secretsì„ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?\n",
    "\n",
    "| ë°©ë²• | ì¥ì  | ë‹¨ì  |\n",
    "|------|------|------|\n",
    "| **ì½”ë“œì— ì§ì ‘ ì…ë ¥** | ê°„í¸í•¨ | âŒ ë…¸íŠ¸ë¶ ê³µìœ  ì‹œ í‚¤ ë…¸ì¶œ |\n",
    "| **í™˜ê²½ ë³€ìˆ˜ (.env)** | ë¡œì»¬ì—ì„œ ì•ˆì „ | âŒ Colabì—ì„œ íŒŒì¼ ì—…ë¡œë“œ í•„ìš” |\n",
    "| **Colab Secrets** | âœ… ì•ˆì „ + ê°„í¸ | Colab ì „ìš© |\n",
    "\n",
    "> ğŸ’¡ **SecretsëŠ” ëŸ°íƒ€ì„ì´ ì¬ì‹œì‘ë˜ì–´ë„ ìœ ì§€**ë˜ë©°, ë…¸íŠ¸ë¶ì„ ê³µìœ í•´ë„ í‚¤ëŠ” ë…¸ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secretsì—ì„œ API í‚¤ ë¡œë“œí•˜ê¸°\n",
    "from google.colab import userdata\n",
    "\n",
    "# Hugging Face Token ê°€ì ¸ì˜¤ê¸°\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"âœ… HF_TOKEN ë¡œë“œ ì„±ê³µ!\")\n",
    "    print(f\"   í† í° ì• 10ì: {HF_TOKEN[:10]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ HF_TOKENì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Secretsì— ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    print(f\"   ì—ëŸ¬: {e}\")\n",
    "\n",
    "# OpenAI API Key ê°€ì ¸ì˜¤ê¸°\n",
    "try:\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"âœ… OPENAI_API_KEY ë¡œë“œ ì„±ê³µ!\")\n",
    "    print(f\"   í‚¤ ì• 10ì: {OPENAI_API_KEY[:10]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   ì—ëŸ¬: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "\n",
    "Colabì—ëŠ” ë§ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì‚¬ì „ ì„¤ì¹˜ë˜ì–´ ìˆì§€ë§Œ, ìµœì‹  ë²„ì „ì´ í•„ìš”í•˜ê±°ë‚˜ ì¶”ê°€ íŒ¨í‚¤ì§€ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš° ì—…ê·¸ë ˆì´ë“œ)\n",
    "!pip install -q transformers accelerate huggingface_hub openai pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì¹˜ëœ ë²„ì „ í™•ì¸\n",
    "import transformers\n",
    "import torch\n",
    "import openai\n",
    "\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "print(f\"Transformers ë²„ì „: {transformers.__version__}\")\n",
    "print(f\"OpenAI ë²„ì „: {openai.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ì‹¤ìŠµ ì˜ˆì œ 1: Hugging Face ëª¨ë¸ GPUë¡œ ì‹¤í–‰í•˜ê¸°\n",
    "\n",
    "Hugging Faceì˜ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì„ GPUì—ì„œ ì‹¤í–‰í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "### GPT-2ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face ë¡œê·¸ì¸ (ì„ íƒì‚¬í•­, ê³µê°œ ëª¨ë¸ì€ ë¶ˆí•„ìš”)\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"âœ… Hugging Face ë¡œê·¸ì¸ ì„±ê³µ!\")\n",
    "except:\n",
    "    print(\"âš ï¸ HF_TOKENì´ ì—†ìŠµë‹ˆë‹¤. ê³µê°œ ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# GPUë¥¼ ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "# device=0: GPU ì‚¬ìš© (CPUëŠ” device=-1)\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device=0  # GPU ì‚¬ìš©\n",
    ")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ì´ GPUì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"   ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {generator.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ìƒì„± ì‹¤í–‰\n",
    "prompt = \"Artificial intelligence in healthcare will\"\n",
    "\n",
    "print(f\"í”„ë¡¬í”„íŠ¸: {prompt}\\n\")\n",
    "print(\"ìƒì„± ì¤‘...\\n\")\n",
    "\n",
    "results = generator(\n",
    "    prompt,\n",
    "    max_length=100,\n",
    "    num_return_sequences=3,\n",
    "    temperature=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"=== ìƒì„± ê²°ê³¼ {i} ===\")\n",
    "    print(result['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU vs GPU ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# CPU íŒŒì´í”„ë¼ì¸\n",
    "generator_cpu = pipeline(\"text-generation\", model=\"gpt2\", device=-1)\n",
    "\n",
    "# GPU íŒŒì´í”„ë¼ì¸\n",
    "generator_gpu = pipeline(\"text-generation\", model=\"gpt2\", device=0)\n",
    "\n",
    "prompt = \"The future of technology is\"\n",
    "\n",
    "# CPU ì„±ëŠ¥ ì¸¡ì •\n",
    "start_cpu = time.time()\n",
    "_ = generator_cpu(prompt, max_length=50, num_return_sequences=1)\n",
    "cpu_time = time.time() - start_cpu\n",
    "\n",
    "# GPU ì„±ëŠ¥ ì¸¡ì •\n",
    "start_gpu = time.time()\n",
    "_ = generator_gpu(prompt, max_length=50, num_return_sequences=1)\n",
    "gpu_time = time.time() - start_gpu\n",
    "\n",
    "print(\"â±ï¸ ì„±ëŠ¥ ë¹„êµ (í…ìŠ¤íŠ¸ ìƒì„±)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"CPU ì†Œìš” ì‹œê°„: {cpu_time:.3f}ì´ˆ\")\n",
    "print(f\"GPU ì†Œìš” ì‹œê°„: {gpu_time:.3f}ì´ˆ\")\n",
    "print(f\"ì†ë„ í–¥ìƒ: {cpu_time/gpu_time:.2f}ë°° ë¹ ë¦„\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ì‹¤ìŠµ ì˜ˆì œ 2: OpenAI API ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "Colabì—ì„œ OpenAI APIë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤. Secretsì— ì €ì¥ëœ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "# Secretsì—ì„œ OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-minië¡œ ëŒ€í™”í•˜ê¸°\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain Google Colab in one sentence.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"ğŸ’¬ AI ì‘ë‹µ:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ ë°›ê¸°\n",
    "print(\"ğŸ’¬ AI ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°):\")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a short poem about machine learning.\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "print()  # ì¤„ë°”ê¿ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ì‹¤ìŠµ ì˜ˆì œ 3: ì´ë¯¸ì§€ ë¶„ë¥˜ (Vision Transformer)\n",
    "\n",
    "GPUë¥¼ í™œìš©í•´ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# ì´ë¯¸ì§€ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ (GPU ì‚¬ìš©)\n",
    "image_classifier = pipeline(\"image-classification\", device=0)\n",
    "\n",
    "# ìƒ˜í”Œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# ì´ë¯¸ì§€ í‘œì‹œ\n",
    "display(image)\n",
    "\n",
    "# ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹¤í–‰\n",
    "predictions = image_classifier(image)\n",
    "\n",
    "print(\"\\nğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ë¥˜ ê²°ê³¼:\")\n",
    "for pred in predictions[:5]:\n",
    "    print(f\"  - {pred['label']}: {pred['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Google Drive ì—°ë™ (ì„ íƒì‚¬í•­)\n",
    "\n",
    "ì„¸ì…˜ì´ ì¢…ë£Œë˜ë©´ Colabì˜ ë°ì´í„°ëŠ” ì‚¬ë¼ì§‘ë‹ˆë‹¤. Google Driveë¥¼ ë§ˆìš´íŠ¸í•˜ë©´ íŒŒì¼ì„ ì˜êµ¬ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"âœ… Google Driveê°€ /content/driveì— ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"   íŒŒì¼ì€ /content/drive/MyDrive/ ê²½ë¡œì— ì €ì¥í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driveì— íŒŒì¼ ì €ì¥ ì˜ˆì œ\n",
    "import os\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "save_dir = \"/content/drive/MyDrive/colab_projects\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# íŒŒì¼ ì €ì¥\n",
    "with open(f\"{save_dir}/test.txt\", \"w\") as f:\n",
    "    f.write(\"Hello from Colab!\")\n",
    "\n",
    "print(f\"âœ… íŒŒì¼ì´ {save_dir}/test.txtì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "ëŒ€í˜• ëª¨ë¸ì„ ì‹¤í–‰í•  ë•ŒëŠ” GPU ë©”ëª¨ë¦¬ë¥¼ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        print(\"ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\")\n",
    "        print(f\"  í• ë‹¹ë¨: {allocated:.2f} GB\")\n",
    "        print(f\"  ì˜ˆì•½ë¨: {reserved:.2f} GB\")\n",
    "        print(f\"  ì „ì²´: {total:.2f} GB\")\n",
    "        print(f\"  ì‚¬ìš©ë¥ : {(reserved/total)*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# ë³€ìˆ˜ ì‚­ì œ\n",
    "# del model, tokenizer  # ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì‚­ì œ\n",
    "\n",
    "# ê°€ë¹„ì§€ ì»¬ë ‰ì…˜\n",
    "gc.collect()\n",
    "\n",
    "# PyTorch ìºì‹œ ë¹„ìš°ê¸°\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"âœ… GPU ë©”ëª¨ë¦¬ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. ì£¼ì˜ì‚¬í•­ ë° íŒ ğŸ’¡\n",
    "\n",
    "### âš ï¸ Colab ì‚¬ìš© ì œí•œì‚¬í•­\n",
    "\n",
    "| ì œí•œ | ë‚´ìš© |\n",
    "|------|------|\n",
    "| **ì„¸ì…˜ ì‹œê°„** | ìµœëŒ€ 12ì‹œê°„ (Pro: 24ì‹œê°„) |\n",
    "| **ìœ íœ´ ì‹œê°„** | 90ë¶„ ë™ì•ˆ í™œë™ ì—†ìœ¼ë©´ ìë™ ì¢…ë£Œ |\n",
    "| **GPU í• ë‹¹** | ì‚¬ìš©ëŸ‰ì´ ë§ìœ¼ë©´ GPUë¥¼ í• ë‹¹ë°›ì§€ ëª»í•  ìˆ˜ ìˆìŒ |\n",
    "| **ë™ì‹œ ì„¸ì…˜** | ë¬´ë£ŒëŠ” 1ê°œ, ProëŠ” ì—¬ëŸ¬ ê°œ ê°€ëŠ¥ |\n",
    "| **íŒŒì¼ ë³´ì¡´** | ëŸ°íƒ€ì„ ì¢…ë£Œ ì‹œ `/content` ë°ì´í„° ì‚­ì œ |\n",
    "\n",
    "### ğŸ’¡ ìœ ìš©í•œ íŒ\n",
    "\n",
    "#### 1. ì„¸ì…˜ ìœ ì§€í•˜ê¸°\n",
    "\n",
    "```javascript\n",
    "// ë¸Œë¼ìš°ì € ì½˜ì†”ì—ì„œ ì‹¤í–‰ (F12)\n",
    "function KeepAlive() {\n",
    "  console.log(\"Keeping session alive...\");\n",
    "  document.querySelector(\"colab-connect-button\").click();\n",
    "}\n",
    "setInterval(KeepAlive, 60000); // 1ë¶„ë§ˆë‹¤ ì‹¤í–‰\n",
    "```\n",
    "\n",
    "âš ï¸ **ì£¼ì˜**: Google ì •ì±… ìœ„ë°˜ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "\n",
    "#### 2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ í•´ê²°\n",
    "\n",
    "```python\n",
    "# ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©\n",
    "batch_size = 1  # ê¸°ë³¸ê°’ë³´ë‹¤ ì¤„ì´ê¸°\n",
    "\n",
    "# Mixed Precision ì‚¬ìš© (FP16)\n",
    "# torch.cuda.amp í™œìš©\n",
    "\n",
    "# Gradient Checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "```\n",
    "\n",
    "#### 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì†ë„ í–¥ìƒ\n",
    "\n",
    "```python\n",
    "# ë¯¸ëŸ¬ ì‚¬ì´íŠ¸ ì‚¬ìš©\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "```\n",
    "\n",
    "#### 4. ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ë¹ ë¥¸ ë³µêµ¬\n",
    "\n",
    "```python\n",
    "# ë…¸íŠ¸ë¶ ìƒë‹¨ì— ì´ ì…€ì„ ë°°ì¹˜\n",
    "# ëŸ°íƒ€ì„ ì¬ì‹œì‘ ì‹œ ì´ ì…€ë§Œ ì‹¤í–‰í•˜ë©´ í™˜ê²½ ë³µêµ¬\n",
    "\n",
    "!pip install -q transformers accelerate huggingface_hub openai\n",
    "from google.colab import userdata\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "```\n",
    "\n",
    "#### 5. í° ëª¨ë¸ ì‚¬ìš© ì‹œ ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "```python\n",
    "# 8bit ë˜ëŠ” 4bit ì–‘ìí™” ì‚¬ìš©\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    # ë˜ëŠ” load_in_4bit=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"model_name\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. ê²°ë¡  ë° ìš”ì•½\n",
    "\n",
    "### ğŸ“ ë°°ìš´ ë‚´ìš©\n",
    "\n",
    "âœ… Google Colab GPU ëŸ°íƒ€ì„ ì„¤ì •í•˜ê¸°  \n",
    "âœ… Colab Secretsìœ¼ë¡œ API í‚¤ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ê¸°  \n",
    "âœ… Hugging Face ëª¨ë¸ì„ GPUë¡œ ì‹¤í–‰í•˜ê¸°  \n",
    "âœ… OpenAI APIë¥¼ Colabì—ì„œ ì‚¬ìš©í•˜ê¸°  \n",
    "âœ… CPU vs GPU ì„±ëŠ¥ ë¹„êµí•˜ê¸°  \n",
    "âœ… GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬í•˜ê¸°  \n",
    "\n",
    "### ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸\n",
    "\n",
    "1. **Colab Secrets ì‚¬ìš©**\n",
    "   - ì¢Œì¸¡ ì—´ì‡  ì•„ì´ì½˜(ğŸ”‘) í´ë¦­\n",
    "   - API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ì €ì¥\n",
    "   - `userdata.get()ìœ¼ë¡œ ì ‘ê·¼`\n",
    "\n",
    "2. **GPU í™œì„±í™”**\n",
    "   - ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU ì„ íƒ\n",
    "   - `device=0` ë˜ëŠ” `device=\"cuda\"`ë¡œ GPU ì‚¬ìš©\n",
    "\n",
    "3. **ì„¸ì…˜ ê´€ë¦¬**\n",
    "   - 12ì‹œê°„ ì œí•œ (ë¬´ë£Œ)\n",
    "   - Google Drive ë§ˆìš´íŠ¸ë¡œ ë°ì´í„° ë³´ì¡´\n",
    "   - ì¤‘ìš”í•œ íŒŒì¼ì€ Driveì— ì €ì¥\n",
    "\n",
    "### ğŸ“š ì¶”ê°€ í•™ìŠµ ë¦¬ì†ŒìŠ¤\n",
    "\n",
    "- [Google Colab ê³µì‹ ë¬¸ì„œ](https://colab.research.google.com/notebooks/)\n",
    "- [Hugging Face ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/models)\n",
    "- [PyTorch GPU íŠœí† ë¦¬ì–¼](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#cuda-tensors)\n",
    "- [Colab Pro êµ¬ë…](https://colab.research.google.com/signup)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!**\n",
    "\n",
    "ì´ì œ Google Colabì˜ ë¬´ë£Œ GPUë¡œ ë‹¤ì–‘í•œ AI ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ì¦ê¸°ì„¸ìš”!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
