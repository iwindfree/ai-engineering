{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API 중급 (Part 2/3)\n",
    "\n",
    "이 노트북은 LLM API 시리즈의 두 번째 파트로, 실무에서 필요한 다양한 기법들을 다룹니다.\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "| 목표 | 설명 |\n",
    "|------|------|\n",
    "| 다양한 활용 | 코드 생성, 감정 분석 등 고급 활용 |\n",
    "| API 파라미터 | temperature, max_tokens 등 제어 |\n",
    "| 스트리밍 | 실시간 응답 출력 구현 |\n",
    "| 에러 처리 | 안정적인 API 호출 패턴 |\n",
    "| 다중 LLM | OpenAI, Claude, Ollama 비교 |\n",
    "| 비용 관리 | 토큰 사용량 기반 비용 계산 |\n",
    "\n",
    "## 시리즈 구성\n",
    "\n",
    "- **Part 1**: LLM API 기초 - 환경설정, 메시지 구조, 기본 호출\n",
    "- **Part 2 (현재)**: LLM API 중급 - 파라미터, 스트리밍, 에러처리, 다중 LLM\n",
    "- **Part 3**: LLM API 고급 - 대화 이력, 캐싱, 에이전트, 프레임워크\n",
    "\n",
    "## 사전 요구사항\n",
    "\n",
    "- Part 1 완료\n",
    "- OpenAI API 키 (필수)\n",
    "- Anthropic API 키 (선택)\n",
    "- Ollama 설치 (선택, 로컬 LLM용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, APIError, RateLimitError, AuthenticationError\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 클라이언트 초기화\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 다양한 활용 예시\n",
    "\n",
    "Part 1에서 다룬 요약, 번역, Q&A 외에 더 다양한 활용 사례를 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 헬퍼 함수\n",
    "def ask_llm(system_prompt: str, user_message: str, model: str = \"gpt-4o-mini\") -> str:\n",
    "    \"\"\"시스템 프롬프트와 사용자 메시지로 LLM을 호출합니다.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 코드 생성\n",
    "\n",
    "프로그래밍 언어와 스타일을 지정하여 코드를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 코드 생성 결과 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "아래는 두 개의 리스트를 받아서 공통 요소만 반환하는 함수입니다. 이 함수는 집합(set)을 사용하여 중복을 제거하고 효율적으로 공통 요소를 찾습니다.\n",
       "\n",
       "```python\n",
       "def common_elements(list1, list2):\n",
       "    \"\"\"\n",
       "    두 개의 리스트를 받아서 공통 요소를 반환하는 함수입니다.\n",
       "    \n",
       "    :param list1: 첫 번째 리스트\n",
       "    :param list2: 두 번째 리스트\n",
       "    :return: 공통 요소 리스트\n",
       "    \"\"\"\n",
       "    return list(set(list1) & set(list2))\n",
       "\n",
       "# 사용 예시\n",
       "list_a = [1, 2, 3, 4, 5]\n",
       "list_b = [4, 5, 6, 7, 8]\n",
       "print(common_elements(list_a, list_b))  # 출력: [4, 5]\n",
       "```\n",
       "\n",
       "위 코드에서 `set(list1)`과 `set(list2)`를 사용하여 두 리스트를 집합으로 변환한 후, `&` 연산자를 사용하여 공통 요소를 찾습니다. 그 결과를 다시 리스트로 변환하여 반환합니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 코드 생성\n",
    "code_prompt = \"\"\"당신은 Python 전문 프로그래머입니다.\n",
    "요청된 기능을 구현하는 깔끔하고 효율적인 Python 코드를 작성해주세요.\n",
    "코드에 간단한 docstring을 포함해주세요.\"\"\"\n",
    "\n",
    "code_request = \"두 개의 리스트를 받아서 공통 요소만 반환하는 함수를 작성해주세요.\"\n",
    "\n",
    "code = ask_llm(code_prompt, code_request)\n",
    "print(\"=== 코드 생성 결과 ===\")\n",
    "display(Markdown(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 감정 분석 (텍스트 분류)\n",
    "\n",
    "텍스트를 정해진 카테고리로 분류합니다. 출력 형식을 명확히 지정하면 후처리가 쉬워집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 감정 분석 결과 ===\n",
      "\n",
      "리뷰: \"이 제품 정말 최고예요! 배송도 빠르고 품질도 훌륭합니다.\"\n",
      "- 감정: 긍정\n",
      "- 신뢰도: 높음\n",
      "- 근거: 제품에 대한 칭찬과 만족감을 나타내는 명확한 표현이 포함되어 있어 긍정적인 감정을 나타냅니다.\n",
      "--------------------------------------------------\n",
      "리뷰: \"그냥 그래요. 나쁘진 않은데 특별히 좋지도 않네요.\"\n",
      "- 감정: 중립\n",
      "- 신뢰도: 높음\n",
      "- 근거: \"나쁘진 않지만 특별히 좋지도 않다\"는 중간적인 감정을 나타냅니다.\n",
      "--------------------------------------------------\n",
      "리뷰: \"최악이에요. 돈 아깝습니다.\"\n",
      "- 감정: 부정\n",
      "- 신뢰도: 높음\n",
      "- 근거: '최악이에요'와 '돈 아깝습니다'라는 표현은 강한 부정적인 감정을 나타냅니다.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 감정 분석\n",
    "sentiment_prompt = \"\"\"당신은 감정 분석 전문가입니다.\n",
    "주어진 텍스트의 감정을 분석하고 다음 형식으로만 응답하세요:\n",
    "- 감정: [긍정/부정/중립]\n",
    "- 신뢰도: [높음/중간/낮음]\n",
    "- 근거: [1문장 설명]\"\"\"\n",
    "\n",
    "reviews = [\n",
    "    \"이 제품 정말 최고예요! 배송도 빠르고 품질도 훌륭합니다.\",\n",
    "    \"그냥 그래요. 나쁘진 않은데 특별히 좋지도 않네요.\",\n",
    "    \"최악이에요. 돈 아깝습니다.\"\n",
    "]\n",
    "\n",
    "print(\"=== 감정 분석 결과 ===\\n\")\n",
    "for review in reviews:\n",
    "    result = ask_llm(sentiment_prompt, review)\n",
    "    print(f'리뷰: \"{review}\"')\n",
    "    print(result)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. API 파라미터\n",
    "\n",
    "Chat Completion API에서 자주 사용되는 파라미터들을 알아봅니다.\n",
    "\n",
    "| 파라미터 | 설명 | 기본값 | 범위 |\n",
    "|---------|------|--------|------|\n",
    "| `model` | 사용할 모델 | 필수 | - |\n",
    "| `messages` | 대화 메시지 배열 | 필수 | - |\n",
    "| `temperature` | 창의성 조절 | 1.0 | 0.0~2.0 |\n",
    "| `max_tokens` | 최대 출력 토큰 수 | 모델별 상이 | - |\n",
    "| `top_p` | 누적 확률 기반 샘플링 | 1.0 | 0.0~1.0 |\n",
    "| `stream` | 스트리밍 응답 | False | True/False |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 temperature 파라미터\n",
    "\n",
    "- **낮은 값 (0.0)**: 일관되고 결정론적인 응답\n",
    "- **높은 값 (1.5+)**: 창의적이지만 예측하기 어려운 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== temperature 비교 ===\n",
      "temperature=0.0: 랜덤한 색상으로 \"코발트 블루\"를 추천합니다! 이 색상은 깊고 선명한 파란색으로, 시원하고 활기찬 느낌을 줍니다. 다양한 디자인에 잘 어울리\n",
      "\n",
      "temperature=1.0: 다양한 색상 중에서 하나를 추천드리자면, \"티파니 블루(Tiffany Blue)\"를 추천합니다. 이 색상은 부드러운 파란색과 녹색의 조합으로, 우아\n",
      "\n",
      "temperature=1.8: 추천하는 랜덤한 색상은 \"코랄(Coral)\"입니다. 이 색상은 산호와 같은 부드러운 따뜻한 오렌지색 оттенhyruder Quint911 گرفته refer重 اعلام single وظН لمطعمة bağlant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temperature 비교\n",
    "def compare_temperature(prompt: str):\n",
    "    \"\"\"temperature 값에 따른 응답 차이를 비교합니다.\"\"\"\n",
    "    for temp in [0.0, 1.0, 1.8]:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        print(f\"temperature={temp}: {response.choices[0].message.content}\\n\")\n",
    "\n",
    "# 테스트\n",
    "print(\"=== temperature 비교 ===\")\n",
    "compare_temperature(\"랜덤한 색상 하나를 추천해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 max_tokens 파라미터\n",
    "\n",
    "출력의 최대 길이를 제한합니다. 비용 관리와 응답 시간 단축에 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_tokens=20: 파이썬은 여러 가지 장점으로 인해 많은 개발자와 데이터 과학자들...\n",
      "  (실제 출력: 20 토큰)\n",
      "\n",
      "max_tokens=50: 파이썬은 여러 가지 이유로 매우 인기 있는 프로그래밍 언어입니다. 그 장점은 다음과 같습니다:\n",
      "\n",
      "1. **간결하고 읽기 쉬운 문법**: 파이썬은...\n",
      "  (실제 출력: 50 토큰)\n",
      "\n",
      "max_tokens=100: 파이썬은 다양한 분야에서 널리 사용되는 프로그래밍 언어로, 여러 가지 장점이 있습니다. 주요 장점은 다음과 같습니다:\n",
      "\n",
      "1. **간결하고 읽기 ...\n",
      "  (실제 출력: 100 토큰)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_tokens 비교\n",
    "for max_tok in [20, 50, 100]:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"파이썬의 장점을 설명해주세요.\"}],\n",
    "        max_tokens=max_tok\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    print(f\"max_tokens={max_tok}: {content[:80]}...\")\n",
    "    print(f\"  (실제 출력: {response.usage.completion_tokens} 토큰)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 스트리밍 응답\n",
    "\n",
    "긴 응답의 경우 스트리밍을 사용하면 토큰이 생성되는 대로 실시간으로 출력받을 수 있습니다.\n",
    "\n",
    "| 방식 | 특징 | 사용 사례 |\n",
    "|------|------|----------|\n",
    "| 일반 | 전체 응답 완료 후 수신 | 짧은 응답, 후처리 필요 시 |\n",
    "| 스트리밍 | 토큰 단위로 실시간 수신 | 긴 응답, 챗봇 UI |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(user_message: str, system_prompt: str = \"You are helpful.\"):\n",
    "    \"\"\"스트리밍 방식으로 응답을 받아 실시간 출력합니다.\"\"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or \"\"\n",
    "        full_response += delta\n",
    "        update_display(Markdown(full_response), display_id=display_handle.display_id)\n",
    "    \n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 스트리밍 응답 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "파이썬의 장점은 다음과 같습니다:\n",
       "\n",
       "1. **쉬운 문법**: 파이썬은 읽기 쉬운 코드 스타일을 가지고 있어 배우기 쉽고, 코드 작성이 간결합니다. 이는 개발자들이 더 빠르게 코드를 이해하고 유지보수할 수 있도록 도와줍니다.\n",
       "\n",
       "2. **풍부한 라이브러리**: 파이썬은 다양한 분야를 위한 수많은 라이브러리와 프레임워크를 제공합니다. 데이터 과학, 웹 개발, 머신러닝 등 많은 작업을 손쉽게 할 수 있도록 지원합니다.\n",
       "\n",
       "3. **활발한 커뮤니티**: 파이썬은 전 세계적으로 활발한 개발자 커뮤니티를 가지고 있어, 문제 해결이나 자료 공유가 용이합니다. 이러한 커뮤니티는 사용자들이 필요할 때 도움을 받을 수 있는 훌륭한 자원입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 스트리밍 테스트\n",
    "print(\"=== 스트리밍 응답 ===\")\n",
    "_ = stream_response(\"파이썬의 장점을 3가지만 간단히 설명해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 에러 처리\n",
    "\n",
    "API 호출 시 다양한 오류가 발생할 수 있습니다.\n",
    "\n",
    "| 에러 | 원인 | 해결책 |\n",
    "|------|------|--------|\n",
    "| `AuthenticationError` | 잘못된 API 키 | API 키 확인 |\n",
    "| `RateLimitError` | 요청 한도 초과 | 대기 후 재시도 |\n",
    "| `APIError` | 서버 오류 | 재시도 |\n",
    "| `APIConnectionError` | 네트워크 오류 | 연결 확인 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_with_retry(messages: list, max_retries: int = 3):\n",
    "    \"\"\"에러 처리와 재시도 로직이 포함된 API 호출 함수\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except AuthenticationError as e:\n",
    "            print(f\"인증 오류: API 키를 확인하세요.\")\n",
    "            raise  # 재시도 불필요\n",
    "        \n",
    "        except RateLimitError as e:\n",
    "            wait_time = 2 ** attempt  # 지수 백오프: 1, 2, 4초\n",
    "            print(f\"요청 한도 초과. {wait_time}초 후 재시도... ({attempt + 1}/{max_retries})\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        except APIError as e:\n",
    "            print(f\"API 오류: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    raise Exception(\"최대 재시도 횟수 초과\")\n",
    "\n",
    "# 테스트\n",
    "result = call_with_retry([{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
    "print(f\"응답: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 다중 LLM 활용\n",
    "\n",
    "여러 LLM 제공업체의 API를 비교해봅니다.\n",
    "\n",
    "### 5.1 OpenAI vs Anthropic 비교\n",
    "\n",
    "| 항목 | OpenAI | Anthropic |\n",
    "|------|--------|----------|\n",
    "| 라이브러리 | `openai` | `anthropic` |\n",
    "| 메서드 | `chat.completions.create()` | `messages.create()` |\n",
    "| system 전달 | messages 배열에 포함 | 별도 `system` 파라미터 |\n",
    "| 응답 접근 | `response.choices[0].message.content` | `response.content[0].text` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Claude 응답 ===\n",
      "Content: 파이썬의 주요 장점 2가지를 말씀드리겠습니다:\n",
      "\n",
      "1. **간결하고 읽기 쉬운 문법**: 파이썬은 영어와 유사한 직관적인 문법을 가지고 있어 코드를 작성하고 이해하기가 매우 쉽습니다. 들여쓰기로 코드 블록을 구분하여 가독성이 뛰어납니다.\n",
      "\n",
      "2. **풍부한 라이브러리 생태계**: 데이터 분석, 웹 개발, 머신러닝, 자동화 등 다양한 분야의 강력한 라이브러리들이 풍부하게 제공되어 개발 효율성을 크게 높일 수 있습니다.\n",
      "\n",
      "--- 메타데이터 ---\n",
      "Model: claude-sonnet-4-20250514\n",
      "Input: 38, Output: 243 tokens\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Anthropic 클라이언트 초기화\n",
    "claude_client = anthropic.Anthropic()\n",
    "\n",
    "def call_claude(system_prompt: str, user_message: str, model: str = \"claude-sonnet-4-20250514\"):\n",
    "    \"\"\"Anthropic Claude API를 호출합니다.\"\"\"\n",
    "    response = claude_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        system=system_prompt,  # system은 별도 파라미터\n",
    "        messages=[{\"role\": \"user\", \"content\": user_message}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Claude 호출 테스트\n",
    "claude_response = call_claude(\n",
    "    system_prompt=\"You are helpful. Respond in Korean.\",\n",
    "    user_message=\"파이썬의 장점을 2가지만 알려주세요.\"\n",
    ")\n",
    "\n",
    "print(\"=== Claude 응답 ===\")\n",
    "print(f\"Content: {claude_response.content[0].text}\")\n",
    "print(f\"\\n--- 메타데이터 ---\")\n",
    "print(f\"Model: {claude_response.model}\")\n",
    "print(f\"Input: {claude_response.usage.input_tokens}, Output: {claude_response.usage.output_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ollama 로컬 LLM\n",
    "\n",
    "[Ollama](https://ollama.ai)를 사용하면 로컬에서 오픈소스 LLM을 실행할 수 있습니다.\n",
    "\n",
    "| 장점 | 설명 |\n",
    "|------|------|\n",
    "| 무료 | API 비용 없음 |\n",
    "| 프라이버시 | 데이터가 로컬에서만 처리 |\n",
    "| 오프라인 | 인터넷 연결 불필요 |\n",
    "\n",
    "```bash\n",
    "# 설치 후 모델 다운로드\n",
    "ollama pull llama3.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama 응답 ===\n",
      "Python is a high-level, interpreted programming language that is widely used for various purposes such as web development, data analysis, machine learning, and scripting due to its simplicity, readability, and large community of users and developers.\n"
     ]
    }
   ],
   "source": [
    "# Ollama 클라이언트 (OpenAI 호환 API)\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"  # Ollama는 인증 불필요\n",
    ")\n",
    "\n",
    "def call_ollama(user_message: str, model: str = \"llama3.2\"):\n",
    "    \"\"\"Ollama 로컬 LLM을 호출합니다.\"\"\"\n",
    "    try:\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_message}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"오류: {e} (Ollama가 실행 중인지 확인하세요)\"\n",
    "\n",
    "# Ollama 테스트 (Ollama 서버가 실행 중이어야 함)\n",
    "ollama_result = call_ollama(\"What is Python? Answer in one sentence.\")\n",
    "print(f\"=== Ollama 응답 ===\")\n",
    "print(ollama_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. OpenAI 호환 인터페이스\n",
    "\n",
    "OpenAI의 API 형식이 사실상 표준이 되어, 많은 제공업체가 호환 API를 제공합니다.\n",
    "`base_url`만 변경하면 동일한 코드로 다양한 LLM에 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 서비스 엔드포인트\n",
    "ENDPOINTS = {\n",
    "    \"gemini\": \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    \"groq\": \"https://api.groq.com/openai/v1\",\n",
    "    \"ollama\": \"http://localhost:11434/v1\",\n",
    "}\n",
    "\n",
    "# Gemini 클라이언트 예시\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if google_api_key:\n",
    "    gemini_client = OpenAI(\n",
    "        api_key=google_api_key,\n",
    "        base_url=ENDPOINTS[\"gemini\"]\n",
    "    )\n",
    "    \n",
    "    response = gemini_client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n",
    "    )\n",
    "    print(f\"Gemini 응답: {response.choices[0].message.content}\")\n",
    "else:\n",
    "    print(\"GOOGLE_API_KEY가 설정되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 비용 계산\n",
    "\n",
    "API 사용 시 토큰 수에 따라 비용이 발생합니다.\n",
    "\n",
    "### 주요 모델 가격 (2025년 기준, 1M 토큰당)\n",
    "\n",
    "| 모델 | Input | Output | 특징 |\n",
    "|------|-------|--------|------|\n",
    "| gpt-4o | $2.50 | $10.00 | 고성능 |\n",
    "| gpt-4o-mini | $0.15 | $0.60 | 가성비 |\n",
    "| claude-sonnet-4 | $3.00 | $15.00 | 균형 |\n",
    "\n",
    "> 가격은 변동될 수 있으니 공식 문서를 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 계산 함수\n",
    "PRICING = {\n",
    "    \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "    \"gpt-4.1\": {\"input\": 2.00, \"output\": 8.00},\n",
    "}\n",
    "\n",
    "def calculate_cost(usage, model: str = \"gpt-4o-mini\") -> dict:\n",
    "    \"\"\"토큰 사용량을 기반으로 비용을 계산합니다.\"\"\"\n",
    "    if model not in PRICING:\n",
    "        return {\"error\": f\"Unknown model: {model}\"}\n",
    "    \n",
    "    price = PRICING[model]\n",
    "    input_cost = (usage.prompt_tokens / 1_000_000) * price[\"input\"]\n",
    "    output_cost = (usage.completion_tokens / 1_000_000) * price[\"output\"]\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": usage.prompt_tokens,\n",
    "        \"output_tokens\": usage.completion_tokens,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": input_cost + output_cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 계산 테스트\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
    "        {\"role\": \"user\", \"content\": \"파이썬의 장점을 5가지 알려주세요.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "cost = calculate_cost(response.usage, \"gpt-4o-mini\")\n",
    "print(\"=== 비용 계산 결과 ===\")\n",
    "print(f\"입력 토큰: {cost['input_tokens']:,}\")\n",
    "print(f\"출력 토큰: {cost['output_tokens']:,}\")\n",
    "print(f\"입력 비용: ${cost['input_cost']:.6f}\")\n",
    "print(f\"출력 비용: ${cost['output_cost']:.6f}\")\n",
    "print(f\"총 비용: ${cost['total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 요약\n",
    "\n",
    "이번 노트북에서 학습한 내용:\n",
    "\n",
    "| 주제 | 핵심 내용 |\n",
    "|------|----------|\n",
    "| **활용 예시** | 코드 생성, 감정 분석 등 system prompt로 역할 지정 |\n",
    "| **파라미터** | temperature(창의성), max_tokens(길이) 등 제어 |\n",
    "| **스트리밍** | stream=True로 실시간 응답 수신 |\n",
    "| **에러 처리** | try-except와 지수 백오프로 안정성 확보 |\n",
    "| **다중 LLM** | OpenAI, Claude, Ollama 상황에 맞게 선택 |\n",
    "| **호환 API** | base_url 변경으로 다양한 LLM 접근 |\n",
    "| **비용 관리** | 토큰 사용량 추적으로 비용 최적화 |\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "**Part 3 (고급)** 에서 다룰 내용:\n",
    "- 대화 이력 관리 (멀티턴 대화)\n",
    "- 프롬프트 캐싱\n",
    "- LiteLLM 통합 인터페이스\n",
    "- 다중 에이전트 시스템\n",
    "- LangChain 프레임워크"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
