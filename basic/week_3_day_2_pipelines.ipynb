{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRuFso38jxFd"
      },
      "source": [
        "# Welcome to Pipelines!\n",
        "\n",
        "The HuggingFace transformers library provides APIs at two different levels.\n",
        "\n",
        "The High Level API for using open-source models for typical inference tasks is called \"pipelines\". It's incredibly easy to use.\n",
        "\n",
        "You create a pipeline using something like:\n",
        "\n",
        "`my_pipeline = pipeline(\"the_task_I_want_to_do\")`\n",
        "\n",
        "Followed by\n",
        "\n",
        "`result = my_pipeline(my_input)`\n",
        "\n",
        "And that's it!\n",
        "\n",
        "See end of this colab for a list of all pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tORRYLozax7"
      },
      "source": [
        "## Before we start: 2 important pro-tips for using Colab:\n",
        "\n",
        "**Pro-tip 1:**\n",
        "\n",
        "Data Science code often gives warnings and messages. They can mostly be safely ignored! Glance over them, and if something goes wrong later, perhaps they can give you a clue.\n",
        "\n",
        "**Pro-tip 2:**\n",
        "\n",
        "In the middle of running a Colab, you might get an error like this:\n",
        "\n",
        "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "This is a super-misleading error message! Please don't try changing versions of packages...\n",
        "\n",
        "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
        "\n",
        "1. Kernel menu >> Disconnect and delete runtime\n",
        "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
        "3. Connect to a new T4 using the button at the top right\n",
        "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
        "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
        "\n",
        "And all should work great - otherwise, ask me!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyvdfSSv4Cwo"
      },
      "source": [
        "## A sidenote:\n",
        "\n",
        "You may already know this, but just in case you're not familiar with the word \"inference\" that I use here:\n",
        "\n",
        "When working with Data Science models, you could be carrying out 2 very different activities: **training** and **inference**.\n",
        "\n",
        "### 1. Training  \n",
        "\n",
        "**Training** is when you provide a model with data for it to adapt to get better at a task in the future. It does this by updating its internal settings - the parameters or weights of the model. If you're Training a model that's already had some training, the activity is called \"fine-tuning\".\n",
        "\n",
        "### 2. Inference\n",
        "\n",
        "**Inference** is when you are working with a model that has _already been trained_. You are using that model to produce new outputs on new inputs, taking advantage of everything it learned while it was being trained. Inference is also sometimes referred to as \"Execution\" or \"Running a model\".\n",
        "\n",
        "All of our use of APIs for GPT, Claude and Gemini in the last weeks are examples of **inference**. The \"P\" in GPT stands for \"Pre-trained\", meaning that it has already been trained with data (lots of it!) In week 6 we will try fine-tuning GPT ourselves.\n",
        "  \n",
        "The pipelines API in HuggingFace is only for use for **inference** - running a model that has already been trained. In week 7 we will be training our own model, and we will need to use the more advanced HuggingFace APIs that we look at in the up-coming lecture.\n",
        "\n",
        "I recorded this playlist on YouTube with more on parameters, training and inference:  \n",
        "https://www.youtube.com/playlist?list=PLWHe-9GP9SMMdl6SLaovUQF2abiLGbMjs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sQ03dQDl2h0D"
      },
      "outputs": [],
      "source": [
        "# Pip installs should come at the top line.\n",
        "# If your Kernel ever resets, you need to run this again.\n",
        "\n",
        "!pip install -q --upgrade datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Bj_PlzfSd5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n",
            "NOT CONNECTED TO A T4\n"
          ]
        }
      ],
      "source": [
        "# Let's check the GPU - it should be a Tesla T4\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  if gpu_info.find('Tesla T4') >= 0:\n",
        "    print(\"Success - Connected to a T4\")\n",
        "  else:\n",
        "    print(\"NOT CONNECTED TO A T4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTm7gpG7qhB7",
        "outputId": "18d51969-72b9-4413-bc4c-ee104ec24817"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90GeDKCG6c1v"
      },
      "source": [
        "# Important Note - Hugging Face account\n",
        "\n",
        "In Day 1, we set up a FREE account on https://huggingface.co\n",
        "\n",
        "### If you skipped this:\n",
        "\n",
        "Please go back and do it! Then go to the Avatar menu, Tokens, and create an API token.. And make sure it has WRITE permissions! And then add it to the secrets on the left by pressing the key button.\n",
        "\n",
        "### If you did this (thank you!)\n",
        "\n",
        "Click on the Key button and turn on the switch so that this notebook gets access to your Hugging Face key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7GoH-tT6-xD",
        "outputId": "dbe3e330-347f-4703-f89f-c6d9d8a27c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF key looks good so far\n"
          ]
        }
      ],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token and hf_token.startswith(\"hf_\"):\n",
        "  print(\"HF key looks good so far\")\n",
        "else:\n",
        "  print(\"HF key is not set - please click the key in the left sidebar\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIdFxC_CUnRY"
      },
      "source": [
        "## Using Pipelines from Hugging Face\n",
        "\n",
        "A simple way to run inference for common tasks, without worrying about all the plumbing, picking reasonable defaults.\n",
        "\n",
        "\n",
        "### How it works:\n",
        "\n",
        "STEP 1: Create a pipeline - a function you can then call\n",
        "\n",
        "```python\n",
        "my_pipeline = pipeline(task, model=xx, device=xx)\n",
        "```\n",
        "\n",
        "If you don't specify a model, then Hugging Face picks one for you that's the default for the task. Specify \"cuda\" for the device to use an NVIDIA GPU like the one on the T4. Specify \"mps\" on a Mac.\n",
        "\n",
        "\n",
        "STEP 2: Then call it as many times as you want:\n",
        "\n",
        "```python\n",
        "my_pipeline(input1)\n",
        "my_pipeline(input2)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzurQ1d12mBU"
      },
      "outputs": [],
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "my_simple_sentiment_analyzer = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "result = my_simple_sentiment_analyzer(\"I'm super excited to be on the way to LLM mastery!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6PzFEHzWPSo"
      },
      "outputs": [],
      "source": [
        "result = my_simple_sentiment_analyzer(\"I should be more excited to be on the way to LLM mastery!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-OAalHVT8ED"
      },
      "outputs": [],
      "source": [
        "\n",
        "better_sentiment = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", device=\"cuda\")\n",
        "result = better_sentiment(\"I should be more excited to be on the way to LLM mastery!!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeSJeFAh21Ra"
      },
      "outputs": [],
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "ner = pipeline(\"ner\", device=\"cuda\")\n",
        "result = ner(\"AI Engineers are learning about the amazing pipelines from HuggingFace in Google Colab from Ed Donner\")\n",
        "for entity in result:\n",
        "  print(entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1fnF2yJ3o6O"
      },
      "outputs": [],
      "source": [
        "# Question Answering with Context\n",
        "\n",
        "question=\"What are Hugging Face pipelines?\"\n",
        "context=\"Pipelines are a high level API for inference of LLMs with common tasks\"\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=question, context=context)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjiiWRj231ME"
      },
      "outputs": [],
      "source": [
        "# Text Summarization\n",
        "\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"\n",
        "The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
        "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
        "It's an extremely popular library that's widely used by the open-source data science community.\n",
        "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7UMfw324AdO"
      },
      "outputs": [],
      "source": [
        "# Translation\n",
        "\n",
        "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGU7ANVaRIkR"
      },
      "outputs": [],
      "source": [
        "# Another translation, showing a model being specified\n",
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSZR309b4IP8"
      },
      "outputs": [],
      "source": [
        "# Classification\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_vynLSH4YQ7"
      },
      "outputs": [],
      "source": [
        "# Text Generation\n",
        "\n",
        "generator = pipeline(\"text-generation\", device=\"cuda\")\n",
        "result = generator(\"If there's one thing I want you to remember about using HuggingFace pipelines, it's\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgG4kcT_4lO_"
      },
      "outputs": [],
      "source": [
        "# Image Generation - remember this?! Now you know what's going on\n",
        "# Pipelines can be used for diffusion models as well as transformers\n",
        "\n",
        "from IPython.display import display\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "pipe.to(\"cuda\")\n",
        "prompt = \"A class of students learning AI engineering in a vibrant pop-art style\"\n",
        "image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCPBE0i4pAAO"
      },
      "outputs": [],
      "source": [
        "# Audio Generation\n",
        "\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from IPython.display import Audio\n",
        "\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "embeddings_dataset = load_dataset(\"matthijs/cmu-arctic-xvectors\", split=\"validation\", trust_remote_code=True)\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "speech = synthesiser(\"Hi to an artificial intelligence engineer, on the way to mastery!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "Audio(speech[\"audio\"], rate=speech[\"sampling_rate\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdMBtNNp3FwC"
      },
      "source": [
        "# All the available pipelines\n",
        "\n",
        "Here are all the pipelines available from Transformers and Diffusers.\n",
        "\n",
        "With thanks to student Lucky P for suggesting I include this!\n",
        "\n",
        "There's a list pipelines under the Tasks on this page (you have to scroll down a bit, then expand the parameters to see the Tasks):\n",
        "\n",
        "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
        "\n",
        "There's also this list of Tasks for Diffusion models instead of Transformers, following the image generation example where I use DiffusionPipeline above.\n",
        "\n",
        "https://huggingface.co/docs/diffusers/en/api/pipelines/overview\n",
        "\n",
        "If you come up with some cool examples of other pipelines, please share them with me! It's wonderful how HuggingFace makes this advanced AI functionality available for inference with such a simple API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLjkPyBs06KU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
