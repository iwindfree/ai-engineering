{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API 중급 (Part 2/3)\n",
    "\n",
    "이 노트북은 LLM API 시리즈의 두 번째 파트로, 실무에서 필요한 다양한 기법들을 다룹니다.\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "| 목표 | 설명 |\n",
    "|------|------|\n",
    "| 다양한 활용 | 코드 생성, 감정 분석 등 고급 활용 |\n",
    "| API 파라미터 | temperature, max_tokens 등 제어 |\n",
    "| 스트리밍 | 실시간 응답 출력 구현 |\n",
    "| 에러 처리 | 안정적인 API 호출 패턴 |\n",
    "| 다중 LLM | OpenAI, Claude, Ollama 비교 |\n",
    "| 비용 관리 | 토큰 사용량 기반 비용 계산 |\n",
    "\n",
    "## 시리즈 구성\n",
    "\n",
    "- **Part 1**: LLM API 기초 - 환경설정, 메시지 구조, 기본 호출\n",
    "- **Part 2 (현재)**: LLM API 중급 - 파라미터, 스트리밍, 에러처리, 다중 LLM\n",
    "- **Part 3**: LLM API 고급 - 대화 이력, 캐싱, 에이전트, 프레임워크\n",
    "\n",
    "## 사전 요구사항\n",
    "\n",
    "- Part 1 완료\n",
    "- OpenAI API 키 (필수)\n",
    "- Anthropic API 키 (선택)\n",
    "- Ollama 설치 (선택, 로컬 LLM용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, APIError, RateLimitError, AuthenticationError\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 클라이언트 초기화\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 다양한 활용 예시\n",
    "\n",
    "Part 1에서 다룬 요약, 번역, Q&A 외에 더 다양한 활용 사례를 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 헬퍼 함수\n",
    "def ask_llm(system_prompt: str, user_message: str, model: str = \"gpt-4o-mini\") -> str:\n",
    "    \"\"\"시스템 프롬프트와 사용자 메시지로 LLM을 호출합니다.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 코드 생성\n",
    "\n",
    "프로그래밍 언어와 스타일을 지정하여 코드를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 코드 생성 결과 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "다음은 두 개의 리스트를 받아서 공통 요소만 반환하는 Python 함수입니다. 이 함수는 set 자료형을 사용하여 효율적으로 공통 요소를 찾아 반환합니다.\n",
       "\n",
       "```python\n",
       "def find_common_elements(list1, list2):\n",
       "    \"\"\"\n",
       "    두 개의 리스트를 받아서 공통 요소만 반환합니다.\n",
       "\n",
       "    :param list1: 첫 번째 리스트\n",
       "    :param list2: 두 번째 리스트\n",
       "    :return: 두 리스트의 공통 요소를 포함하는 리스트\n",
       "    \"\"\"\n",
       "    return list(set(list1) & set(list2))\n",
       "\n",
       "# 예시 사용\n",
       "list_a = [1, 2, 3, 4, 5]\n",
       "list_b = [4, 5, 6, 7, 8]\n",
       "common_elements = find_common_elements(list_a, list_b)\n",
       "print(common_elements)  # 출력: [4, 5]\n",
       "```\n",
       "\n",
       "위 코드에서는 `set`을 사용하여 두 리스트의 공통 요소를 찾고, 결과를 다시 리스트로 변환하여 반환합니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 코드 생성\n",
    "code_prompt = \"\"\"당신은 Python 전문 프로그래머입니다.\n",
    "요청된 기능을 구현하는 깔끔하고 효율적인 Python 코드를 작성해주세요.\n",
    "코드에 간단한 docstring을 포함해주세요.\"\"\"\n",
    "\n",
    "code_request = \"두 개의 리스트를 받아서 공통 요소만 반환하는 함수를 작성해주세요.\"\n",
    "\n",
    "code = ask_llm(code_prompt, code_request)\n",
    "print(\"=== 코드 생성 결과 ===\")\n",
    "display(Markdown(code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 감정 분석 (텍스트 분류)\n",
    "\n",
    "텍스트를 정해진 카테고리로 분류합니다. 출력 형식을 명확히 지정하면 후처리가 쉬워집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 감정 분석 결과 ===\n",
      "\n",
      "리뷰: \"이 제품 정말 최고예요! 배송도 빠르고 품질도 훌륭합니다.\"\n",
      "- 감정: 긍정\n",
      "- 신뢰도: 높음\n",
      "- 근거: 제품에 대한 높은 만족감과 긍정적인 경험이 명확하게 표현되고 있습니다.\n",
      "--------------------------------------------------\n",
      "리뷰: \"그냥 그래요. 나쁘진 않은데 특별히 좋지도 않네요.\"\n",
      "- 감정: 중립\n",
      "- 신뢰도: 높음\n",
      "- 근거: 긍정과 부정이 혼재되어 있지 않고, 평범함을 나타내는 표현이 사용되었다.\n",
      "--------------------------------------------------\n",
      "리뷰: \"최악이에요. 돈 아깝습니다.\"\n",
      "- 감정: 부정\n",
      "- 신뢰도: 높음\n",
      "- 근거: \"최악이에요\"와 \"돈 아깝습니다\"는 명백한 부정적인 감정을 표현하고 있습니다.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 감정 분석\n",
    "sentiment_prompt = \"\"\"당신은 감정 분석 전문가입니다.\n",
    "주어진 텍스트의 감정을 분석하고 다음 형식으로만 응답하세요:\n",
    "- 감정: [긍정/부정/중립]\n",
    "- 신뢰도: [높음/중간/낮음]\n",
    "- 근거: [1문장 설명]\"\"\"\n",
    "\n",
    "reviews = [\n",
    "    \"이 제품 정말 최고예요! 배송도 빠르고 품질도 훌륭합니다.\",\n",
    "    \"그냥 그래요. 나쁘진 않은데 특별히 좋지도 않네요.\",\n",
    "    \"최악이에요. 돈 아깝습니다.\"\n",
    "]\n",
    "\n",
    "print(\"=== 감정 분석 결과 ===\\n\")\n",
    "for review in reviews:\n",
    "    result = ask_llm(sentiment_prompt, review)\n",
    "    print(f'리뷰: \"{review}\"')\n",
    "    print(result)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. API 파라미터\n",
    "\n",
    "Chat Completion API에서 자주 사용되는 파라미터들을 알아봅니다.\n",
    "\n",
    "| 파라미터 | 설명 | 기본값 | 범위 |\n",
    "|---------|------|--------|------|\n",
    "| `model` | 사용할 모델 | 필수 | - |\n",
    "| `messages` | 대화 메시지 배열 | 필수 | - |\n",
    "| `temperature` | 창의성 조절 | 1.0 | 0.0~2.0 |\n",
    "| `max_tokens` | 최대 출력 토큰 수 | 모델별 상이 | - |\n",
    "| `top_p` | 누적 확률 기반 샘플링 | 1.0 | 0.0~1.0 |\n",
    "| `stream` | 스트리밍 응답 | False | True/False |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 temperature 파라미터\n",
    "\n",
    "- **낮은 값 (0.0)**: 일관되고 결정론적인 응답\n",
    "- **높은 값 (1.5+)**: 창의적이지만 예측하기 어려운 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== temperature 비교 ===\n",
      "temperature=0.0: 랜덤한 색상으로 \"코발트 블루\"를 추천합니다! 이 색상은 깊고 선명한 파란색으로, 시원하고 활기찬 느낌을 줍니다. 다양한 디자인에 잘 어울리\n",
      "\n",
      "temperature=1.0: 랜덤한 색상으로 \"청록색\"을 추천합니다. 청록색은 청색과 녹색이 조화를 이루는 색상으로, 시원하고 자연적인 느낌을 줍니다. 디자인이나 인테리\n",
      "\n",
      "temperature=1.8: 쇼킹 핑크(pure pink)를 추천드립니다! 신나는 에너지를 주는 색상으로, 다양한 요소와 잘 어울리며 튀는 느낌을 줄 수 있습니다. 또한, 시각적으로도 Legendary(백발\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temperature 비교\n",
    "def compare_temperature(prompt: str):\n",
    "    \"\"\"temperature 값에 따른 응답 차이를 비교합니다.\"\"\"\n",
    "    for temp in [0.0, 1.0, 1.8]:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        print(f\"temperature={temp}: {response.choices[0].message.content}\\n\")\n",
    "\n",
    "# 테스트\n",
    "print(\"=== temperature 비교 ===\")\n",
    "compare_temperature(\"랜덤한 색상 하나를 추천해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 max_tokens 파라미터\n",
    "\n",
    "출력의 최대 길이를 제한합니다. 비용 관리와 응답 시간 단축에 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_tokens=20: 파이썬은 여러 가지 장점을 가진 널리 사용되는 프로그래밍 언어...\n",
      "  (실제 출력: 20 토큰)\n",
      "\n",
      "max_tokens=50: 파이썬은 다양한 이유로 인기가 높은 프로그래밍 언어입니다. 아래는 파이썬의 주요 장점들입니다:\n",
      "\n",
      "1. **문법이 간단하고 가독성이 높음**: 파...\n",
      "  (실제 출력: 50 토큰)\n",
      "\n",
      "max_tokens=100: 파이썬은 다양한 장점 덕분에 많은 개발자와 기업에서 널리 사용되고 있는 프로그래밍 언어입니다. 다음은 파이썬의 주요 장점입니다:\n",
      "\n",
      "1. **쉬운...\n",
      "  (실제 출력: 100 토큰)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_tokens 비교\n",
    "for max_tok in [20, 50, 100]:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"파이썬의 장점을 설명해주세요.\"}],\n",
    "        max_tokens=max_tok\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    print(f\"max_tokens={max_tok}: {content[:80]}...\")\n",
    "    print(f\"  (실제 출력: {response.usage.completion_tokens} 토큰)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 스트리밍 응답\n",
    "\n",
    "긴 응답의 경우 스트리밍을 사용하면 토큰이 생성되는 대로 실시간으로 출력받을 수 있습니다.\n",
    "\n",
    "| 방식 | 특징 | 사용 사례 |\n",
    "|------|------|----------|\n",
    "| 일반 | 전체 응답 완료 후 수신 | 짧은 응답, 후처리 필요 시 |\n",
    "| 스트리밍 | 토큰 단위로 실시간 수신 | 긴 응답, 챗봇 UI |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(user_message: str, system_prompt: str = \"You are helpful.\"):\n",
    "    \"\"\"스트리밍 방식으로 응답을 받아 실시간 출력합니다.\"\"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or \"\"\n",
    "        full_response += delta\n",
    "        update_display(Markdown(full_response), display_id=display_handle.display_id)\n",
    "    \n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 스트리밍 응답 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "물론입니다! 파이썬의 장점을 세 가지 간단히 설명해 드리겠습니다.\n",
       "\n",
       "1. **간결하고 읽기 쉬운 문법**: 파이썬은 코드가 매우 직관적이고 간결하여, 초보자가 배우기 쉽습니다. 가독성이 높은 코드 덕분에 유지 보수와 협업이 용이합니다.\n",
       "\n",
       "2. **광범위한 라이브러리와 프레임워크**: 파이썬은 수많은 라이브러리와 프레임워크(예: NumPy, Pandas, Django, Flask 등)를 제공하여 데이터 분석, 웹 개발, 머신러닝 등 다양한 분야에서 쉽게 활용할 수 있습니다.\n",
       "\n",
       "3. **다양한 커뮤니티 지원**: 전 세계적으로 활발한 사용자 및 개발자 커뮤니티가 있어, 문제 해결 및 자료 공유가 용이합니다. 풍부한 문서와 튜토리얼이 제공되어 학습에 큰 도움이 됩니다. \n",
       "\n",
       "이러한 장점들 덕분에 파이썬은 많은 분야에서 인기가 높은 프로그래밍 언어로 자리잡고 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 스트리밍 테스트\n",
    "print(\"=== 스트리밍 응답 ===\")\n",
    "_ = stream_response(\"파이썬의 장점을 3가지만 간단히 설명해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 에러 처리\n",
    "\n",
    "API 호출 시 다양한 오류가 발생할 수 있습니다.\n",
    "\n",
    "| 에러 | 원인 | 해결책 |\n",
    "|------|------|--------|\n",
    "| `AuthenticationError` | 잘못된 API 키 | API 키 확인 |\n",
    "| `RateLimitError` | 요청 한도 초과 | 대기 후 재시도 |\n",
    "| `APIError` | 서버 오류 | 재시도 |\n",
    "| `APIConnectionError` | 네트워크 오류 | 연결 확인 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_with_retry(messages: list, max_retries: int = 3):\n",
    "    \"\"\"에러 처리와 재시도 로직이 포함된 API 호출 함수\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except AuthenticationError as e:\n",
    "            print(f\"인증 오류: API 키를 확인하세요.\")\n",
    "            raise  # 재시도 불필요\n",
    "        \n",
    "        except RateLimitError as e:\n",
    "            wait_time = 2 ** attempt  # 지수 백오프: 1, 2, 4초\n",
    "            print(f\"요청 한도 초과. {wait_time}초 후 재시도... ({attempt + 1}/{max_retries})\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        except APIError as e:\n",
    "            print(f\"API 오류: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    raise Exception(\"최대 재시도 횟수 초과\")\n",
    "\n",
    "# 테스트\n",
    "result = call_with_retry([{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
    "print(f\"응답: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 다중 LLM 활용\n",
    "\n",
    "여러 LLM 제공업체의 API를 비교해봅니다.\n",
    "\n",
    "### 5.1 OpenAI vs Anthropic 비교\n",
    "\n",
    "| 항목 | OpenAI | Anthropic |\n",
    "|------|--------|----------|\n",
    "| 라이브러리 | `openai` | `anthropic` |\n",
    "| 메서드 | `chat.completions.create()` | `messages.create()` |\n",
    "| system 전달 | messages 배열에 포함 | 별도 `system` 파라미터 |\n",
    "| 응답 접근 | `response.choices[0].message.content` | `response.content[0].text` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:156\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mstart_tls\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     stream = \u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_tls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:154\u001b[39m, in \u001b[36mSyncStream.start_tls\u001b[39m\u001b[34m(self, ssl_context, server_hostname, timeout)\u001b[39m\n\u001b[32m    150\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    151\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    152\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    153\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1028)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/anthropic/_base_client.py:1049\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.2/lib/python3.13/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1028)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Claude 호출 테스트\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m claude_response = \u001b[43mcall_claude\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are helpful. Respond in Korean.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m파이썬의 장점을 2가지만 알려주세요.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Claude 응답 ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mContent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclaude_response.content[\u001b[32m0\u001b[39m].text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcall_claude\u001b[39m\u001b[34m(system_prompt, user_message, model)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_claude\u001b[39m(system_prompt: \u001b[38;5;28mstr\u001b[39m, user_message: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mclaude-sonnet-4-20250514\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Anthropic Claude API를 호출합니다.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     response = \u001b[43mclaude_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# system은 별도 파라미터\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/anthropic/_utils/_utils.py:282\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/anthropic/resources/messages/messages.py:930\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    924\u001b[39m     warnings.warn(\n\u001b[32m    925\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    926\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    927\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    928\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/anthropic/_base_client.py:1326\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1314\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1321\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1322\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1323\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1324\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1325\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/anthropic/_base_client.py:1081\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1080\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1083\u001b[39m log.debug(\n\u001b[32m   1084\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1085\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1089\u001b[39m     response.headers,\n\u001b[32m   1090\u001b[39m )\n\u001b[32m   1091\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mrequest-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Anthropic 클라이언트 초기화\n",
    "claude_client = anthropic.Anthropic()\n",
    "\n",
    "def call_claude(system_prompt: str, user_message: str, model: str = \"claude-sonnet-4-20250514\"):\n",
    "    \"\"\"Anthropic Claude API를 호출합니다.\"\"\"\n",
    "    response = claude_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        system=system_prompt,  # system은 별도 파라미터\n",
    "        messages=[{\"role\": \"user\", \"content\": user_message}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Claude 호출 테스트\n",
    "claude_response = call_claude(\n",
    "    system_prompt=\"You are helpful. Respond in Korean.\",\n",
    "    user_message=\"파이썬의 장점을 2가지만 알려주세요.\"\n",
    ")\n",
    "\n",
    "print(\"=== Claude 응답 ===\")\n",
    "print(f\"Content: {claude_response.content[0].text}\")\n",
    "print(f\"\\n--- 메타데이터 ---\")\n",
    "print(f\"Model: {claude_response.model}\")\n",
    "print(f\"Input: {claude_response.usage.input_tokens}, Output: {claude_response.usage.output_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ollama 로컬 LLM\n",
    "\n",
    "[Ollama](https://ollama.ai)를 사용하면 로컬에서 오픈소스 LLM을 실행할 수 있습니다.\n",
    "\n",
    "| 장점 | 설명 |\n",
    "|------|------|\n",
    "| 무료 | API 비용 없음 |\n",
    "| 프라이버시 | 데이터가 로컬에서만 처리 |\n",
    "| 오프라인 | 인터넷 연결 불필요 |\n",
    "\n",
    "```bash\n",
    "# 설치 후 모델 다운로드\n",
    "ollama pull llama3.2\n",
    "ollama pull exaone3.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama 응답 ===\n",
      "Python is a high-level programming language known for its readability and versatility, widely used for web development, data analysis, artificial intelligence, and more.\n"
     ]
    }
   ],
   "source": [
    "# Ollama 클라이언트 (OpenAI 호환 API)\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"  # Ollama는 인증 불필요\n",
    ")\n",
    "\n",
    "def call_ollama(user_message: str, model: str = \"exaone3.5\"):\n",
    "    \"\"\"Ollama 로컬 LLM을 호출합니다.\"\"\"\n",
    "    try:\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_message}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"오류: {e} (Ollama가 실행 중인지 확인하세요)\"\n",
    "\n",
    "# Ollama 테스트 (Ollama 서버가 실행 중이어야 함)\n",
    "ollama_result = call_ollama(\"What is Python? Answer in one sentence.\")\n",
    "print(f\"=== Ollama 응답 ===\")\n",
    "print(ollama_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. OpenAI 호환 인터페이스\n",
    "\n",
    "OpenAI의 API 형식이 사실상 표준이 되어, 많은 제공업체가 호환 API를 제공합니다.\n",
    "`base_url`만 변경하면 동일한 코드로 다양한 LLM에 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY가 설정되지 않았습니다.\n"
     ]
    }
   ],
   "source": [
    "# 다양한 서비스 엔드포인트\n",
    "ENDPOINTS = {\n",
    "    \"gemini\": \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    \"groq\": \"https://api.groq.com/openai/v1\",\n",
    "    \"ollama\": \"http://localhost:11434/v1\",\n",
    "}\n",
    "\n",
    "# Gemini 클라이언트 예시\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if google_api_key:\n",
    "    gemini_client = OpenAI(\n",
    "        api_key=google_api_key,\n",
    "        base_url=ENDPOINTS[\"gemini\"]\n",
    "    )\n",
    "    \n",
    "    response = gemini_client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n",
    "    )\n",
    "    print(f\"Gemini 응답: {response.choices[0].message.content}\")\n",
    "else:\n",
    "    print(\"GOOGLE_API_KEY가 설정되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 비용 계산\n",
    "\n",
    "API 사용 시 토큰 수에 따라 비용이 발생합니다.\n",
    "\n",
    "### 주요 모델 가격 (2025년 기준, 1M 토큰당)\n",
    "\n",
    "| 모델 | Input | Output | 특징 |\n",
    "|------|-------|--------|------|\n",
    "| gpt-4o | $2.50 | $10.00 | 고성능 |\n",
    "| gpt-4o-mini | $0.15 | $0.60 | 가성비 |\n",
    "| claude-sonnet-4 | $3.00 | $15.00 | 균형 |\n",
    "\n",
    "> 가격은 변동될 수 있으니 공식 문서를 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 계산 함수\n",
    "PRICING = {\n",
    "    \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "    \"gpt-4.1\": {\"input\": 2.00, \"output\": 8.00},\n",
    "}\n",
    "\n",
    "def calculate_cost(usage, model: str = \"gpt-4o-mini\") -> dict:\n",
    "    \"\"\"토큰 사용량을 기반으로 비용을 계산합니다.\"\"\"\n",
    "    if model not in PRICING:\n",
    "        return {\"error\": f\"Unknown model: {model}\"}\n",
    "    \n",
    "    price = PRICING[model]\n",
    "    input_cost = (usage.prompt_tokens / 1_000_000) * price[\"input\"]\n",
    "    output_cost = (usage.completion_tokens / 1_000_000) * price[\"output\"]\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": usage.prompt_tokens,\n",
    "        \"output_tokens\": usage.completion_tokens,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": input_cost + output_cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 계산 테스트\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
    "        {\"role\": \"user\", \"content\": \"파이썬의 장점을 5가지 알려주세요.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "cost = calculate_cost(response.usage, \"gpt-4o-mini\")\n",
    "print(\"=== 비용 계산 결과 ===\")\n",
    "print(f\"입력 토큰: {cost['input_tokens']:,}\")\n",
    "print(f\"출력 토큰: {cost['output_tokens']:,}\")\n",
    "print(f\"입력 비용: ${cost['input_cost']:.6f}\")\n",
    "print(f\"출력 비용: ${cost['output_cost']:.6f}\")\n",
    "print(f\"총 비용: ${cost['total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 요약\n",
    "\n",
    "이번 노트북에서 학습한 내용:\n",
    "\n",
    "| 주제 | 핵심 내용 |\n",
    "|------|----------|\n",
    "| **활용 예시** | 코드 생성, 감정 분석 등 system prompt로 역할 지정 |\n",
    "| **파라미터** | temperature(창의성), max_tokens(길이) 등 제어 |\n",
    "| **스트리밍** | stream=True로 실시간 응답 수신 |\n",
    "| **에러 처리** | try-except와 지수 백오프로 안정성 확보 |\n",
    "| **다중 LLM** | OpenAI, Claude, Ollama 상황에 맞게 선택 |\n",
    "| **호환 API** | base_url 변경으로 다양한 LLM 접근 |\n",
    "| **비용 관리** | 토큰 사용량 추적으로 비용 최적화 |\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "**Part 3 (고급)** 에서 다룰 내용:\n",
    "- 대화 이력 관리 (멀티턴 대화)\n",
    "- 프롬프트 캐싱\n",
    "- LiteLLM 통합 인터페이스\n",
    "- 다중 에이전트 시스템\n",
    "- LangChain 프레임워크"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
