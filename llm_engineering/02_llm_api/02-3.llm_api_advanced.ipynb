{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API ê³ ê¸‰ (Part 3/3)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LLM API ì‹œë¦¬ì¦ˆì˜ ë§ˆì§€ë§‰ íŒŒíŠ¸ë¡œ, í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "| ëª©í‘œ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| ëŒ€í™” ì´ë ¥ ê´€ë¦¬ | ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ êµ¬í˜„ |\n",
    "| ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ | ë…¼ë¦¬ í¼ì¦ë¡œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ |\n",
    "| í”„ë¡¬í”„íŠ¸ ìºì‹± | ë¹„ìš© ì ˆê° ê¸°ë²• |\n",
    "| LiteLLM | 100+ LLM í†µí•© ì¸í„°í˜ì´ìŠ¤ |\n",
    "| ë‹¤ì¤‘ ì—ì´ì „íŠ¸ | ì—¬ëŸ¬ AIê°€ í˜‘ì—…í•˜ëŠ” ì‹œìŠ¤í…œ |\n",
    "| LangChain | LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ |\n",
    "\n",
    "## ì‹œë¦¬ì¦ˆ êµ¬ì„±\n",
    "\n",
    "- **Part 1**: LLM API ê¸°ì´ˆ - í™˜ê²½ì„¤ì •, ë©”ì‹œì§€ êµ¬ì¡°, ê¸°ë³¸ í˜¸ì¶œ\n",
    "- **Part 2**: LLM API ì¤‘ê¸‰ - íŒŒë¼ë¯¸í„°, ìŠ¤íŠ¸ë¦¬ë°, ì—ëŸ¬ì²˜ë¦¬, ë‹¤ì¤‘ LLM\n",
    "- **Part 3 (í˜„ì¬)**: LLM API ê³ ê¸‰ - ëŒ€í™” ì´ë ¥, ìºì‹±, ì—ì´ì „íŠ¸, í”„ë ˆì„ì›Œí¬\n",
    "\n",
    "## ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "\n",
    "- Part 1, 2 ì™„ë£Œ\n",
    "- OpenAI API í‚¤ (í•„ìˆ˜)\n",
    "- Anthropic, Google API í‚¤ (ì„ íƒ)\n",
    "- `litellm`, `langchain-openai` ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n",
    "# !pip install litellm langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ëŒ€í™” ì´ë ¥ ê´€ë¦¬\n",
    "\n",
    "LLM APIëŠ” ìƒíƒœë¥¼ ìœ ì§€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë ¤ë©´ ì´ì „ ë©”ì‹œì§€ë“¤ì„ í•¨ê»˜ ì „ì†¡í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ ê°œë…\n",
    "\n",
    "```\n",
    "ìš”ì²­ 1: [system, user1] â†’ assistant1\n",
    "ìš”ì²­ 2: [system, user1, assistant1, user2] â†’ assistant2\n",
    "ìš”ì²­ 3: [system, user1, assistant1, user2, assistant2, user3] â†’ assistant3\n",
    "```\n",
    "\n",
    "ê°„ë‹¨í•œ ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ë©´ ì¬ë¯¸ìˆëŠ” í˜„ìƒì„ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Windfree! How can I assist you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, My name is windfree.\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì²«ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë‚´ ì´ë¦„ì„ ë§í•´ì¤€ í›„ì— ë‘ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë‚´ ì´ë¦„ì„ ë¬¼ì–´ë³´ì•˜ì„ ë•Œ LLM ì€ ë‚´ ì´ë¦„ì„ ëª¨ë¥¸ë‹¤ëŠ” ë‹µì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ìœ ê°€ ë­˜ê¹Œìš”? LLM ì— ëŒ€í•œ ëª¨ë“  í˜¸ì¶œì€ ì™„ì „íˆ Stateless í•œ ìƒíƒœì…ë‹ˆë‹¤. ë§¤ë²ˆ ì™„ì „íˆ ìƒˆë¡œìš´ í˜¸ì¶œì¸ ì…ˆì´ì£ . LLM ì´ â€œê¸°ì–µâ€ ì„ ê°€ì§„ ê²ƒì²˜ëŸ¼ ë§Œë“œëŠ” ê²ƒì€ AI ê°œë°œìì˜ ëª«ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Windfree.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, My name is windfree.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, Windfree! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¹ì—°í•œ ì–˜ê¸°ì¼ ìˆ˜ ìˆì§€ë§Œ,  ì •ë¦¬í•´ë³´ë©´:\n",
    "\n",
    " * LLMì— ëŒ€í•œ ëª¨ë“  í˜¸ì¶œì€ ë¬´ìƒíƒœ(stateless)ë‹¤.\n",
    " * ë§¤ë²ˆ ì§€ê¸ˆê¹Œì§€ì˜ ì „ì²´ ëŒ€í™”ë¥¼ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ë‹´ì•„ ì „ë‹¬í•œë‹¤.\n",
    " * ì´ê²Œ LLMì´ ê¸°ì–µì„ ê°€ì§„ ê²ƒ ê°™ì€ ì°©ê°ì„ ë§Œë“ ë‹¤ â€” ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ì§€ë§Œ ì´ê±´ íŠ¸ë¦­ì´ë‹¤.\n",
    " * ë§¤ë²ˆ ì „ì²´ ëŒ€í™”ë¥¼ ì œê³µí•œ ê²°ê³¼ì¼ ë¿ LLMì€ ê·¸ì € ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒì— ì˜¬ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í† í°ì„ ì˜ˆì¸¡í•  ë¿ì´ë‹¤.\n",
    " * ì‹œí€€ìŠ¤ì— â€œë‚´ ì´ë¦„ì€ windfreeì•¼â€ê°€ ìˆê³  ë‚˜ì¤‘ì— â€œë‚´ ì´ë¦„ì´ ë­ì§€?â€ë¼ê³  ë¬¼ìœ¼ë©´â€¦ windfreeë¼ê³  ì˜ˆì¸¡í•˜ëŠ” ê²ƒ!\n",
    "\n",
    "ë§ì€ ì œí’ˆë“¤ì´ ì •í™•íˆ ì´ íŠ¸ë¦­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•Œë§ˆë‹¤ ì „ì²´ ëŒ€í™”ê°€ í•¨ê»˜ ì „ë‹¬ë˜ëŠ” ê²ë‹ˆë‹¤. â€œê·¸ëŸ¬ë©´ ë§¤ë²ˆ ì´ì „ ëŒ€í™” ì „ì²´ì— ëŒ€í•´ ì¶”ê°€ ë¹„ìš©ì„ ë‚´ì•¼ í•˜ëŠ” ê±´ê°€ìš”?â€ ë„¤. ë‹¹ì—°íˆ ê·¸ë ‡ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ê·¸ê²Œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì´ê¸°ë„ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMì´ ì „ì²´ ëŒ€í™”ë¥¼ ë˜ëŒì•„ë³´ë©° ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ê¸¸ ê¸°ëŒ€í•˜ê³  ìˆëŠ” ìƒíƒœì´ë©° ê·¸ì— ëŒ€í•œ ì‚¬ìš©ë£Œë¥¼ ë‚´ì•¼ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œë¡œ LLM APIë¥¼ ë‹¤ë¤„ë³´ì…¨ìœ¼ë‹ˆ ì²´ê°í•˜ì‹œê² ì§€ë§Œ, ë§¤ ìš”ì²­ë§ˆë‹¤ ì´ì „ ëŒ€í™” ë‚´ì—­ì„ messages ë°°ì—´ì— ë‹¤ì‹œ ë‹´ì•„ ë³´ë‚´ëŠ” êµ¬ì¡°ê°€ ë°”ë¡œ ì´ ë¬´ìƒíƒœì„± ë•Œë¬¸ì…ë‹ˆë‹¤. í”íˆ ì‚¬ìš©í•˜ëŠ” â€œê¸°ì–µâ€ êµ¬í˜„ ê¸°ë²•ë“¤ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    " * ì»¨í…ìŠ¤íŠ¸ ì£¼ì…: ì´ì „ ëŒ€í™”ë¥¼ messagesì— ëˆ„ì \n",
    " * ìš”ì•½/ì••ì¶•: ê¸´ ëŒ€í™”ëŠ” ìš”ì•½í•´ì„œ system promptì— ì‚½ì…\n",
    " * RAG: ì™¸ë¶€ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ í›„ ì£¼ì…\n",
    " * ë©”ëª¨ë¦¬ DB: ì‚¬ìš©ìë³„ ì¤‘ìš” ì •ë³´ë¥¼ ë³„ë„ ì €ì¥ í›„ í•„ìš”ì‹œ ì£¼ì…\n",
    " \n",
    "API ìš”ê¸ˆ êµ¬ì¡°ë¥¼ ë³´ë©´ input tokenê³¼ output tokenì„ ë”°ë¡œ ê³¼ê¸ˆí•˜ëŠ”ë°, ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ input tokenì´ ëˆ„ì ë˜ì–´ ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì‹¤ë¬´ì—ì„œëŠ” ëŒ€í™” ìš”ì•½, sliding window, ì˜¤ë˜ëœ ë©”ì‹œì§€ ì‚­ì œ ê°™ì€ ì „ëµì„ ì“°ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì¢€ ë” ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, update_display\n",
    "from typing import Generator\n",
    "\n",
    "# ëŒ€í™” ì´ë ¥ ê´€ë¦¬ í´ë˜ìŠ¤\n",
    "class ChatSession:\n",
    "    \"\"\"ëŒ€í™” ì´ë ¥ì„ ê´€ë¦¬í•˜ëŠ” ì±„íŒ… ì„¸ì…˜ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self, system_prompt: str = \"\", model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.total_tokens = 0\n",
    "\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    def chat(self, user_input: str, stream: bool = False):\n",
    "        \"\"\"ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            user_input: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€\n",
    "            stream: Trueë©´ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤ì‹œê°„ ì¶œë ¥\n",
    "\n",
    "        Returns:\n",
    "            stream=False: ì „ì²´ ì‘ë‹µ ë¬¸ìì—´\n",
    "            stream=True: ì‹¤ì‹œê°„ ì¶œë ¥ í›„ ì „ì²´ ì‘ë‹µ ë¬¸ìì—´ ë°˜í™˜\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        if stream:\n",
    "            return self._chat_stream()\n",
    "        else:\n",
    "            return self._chat_normal()\n",
    "\n",
    "    def _chat_normal(self) -> str:\n",
    "        \"\"\"ì¼ë°˜ ëª¨ë“œë¡œ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "        )\n",
    "\n",
    "        assistant_reply = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "        self.total_tokens += response.usage.total_tokens\n",
    "\n",
    "        return assistant_reply\n",
    "\n",
    "    def _chat_stream(self) -> str:\n",
    "        \"\"\"ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‘ë‹µì„ ë°›ì•„ ì‹¤ì‹œê°„ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta.content or \"\"\n",
    "            full_response += delta\n",
    "            update_display(Markdown(full_response), display_id=display_handle.display_id)\n",
    "\n",
    "        # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "        return full_response\n",
    "\n",
    "    def chat_generator(self, user_input: str) -> Generator[str, None, None]:\n",
    "        \"\"\"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤ (Gradio ë“±ì—ì„œ í™œìš©).\n",
    "\n",
    "        Args:\n",
    "            user_input: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€\n",
    "\n",
    "        Yields:\n",
    "            í† í° ë‹¨ìœ„ë¡œ ëˆ„ì ëœ ì‘ë‹µ ë¬¸ìì—´\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta.content or \"\"\n",
    "            full_response += delta\n",
    "            yield full_response\n",
    "\n",
    "        # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    def show_history(self):\n",
    "        \"\"\"ëŒ€í™” ì´ë ¥ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "        icons = {\"system\": \"âš™ï¸\", \"user\": \"ğŸ‘¤\", \"assistant\": \"ğŸ¤–\"}\n",
    "        for msg in self.messages:\n",
    "            icon = icons.get(msg[\"role\"], \"â“\")\n",
    "            content = msg[\"content\"][:80] + \"...\" if len(msg[\"content\"]) > 80 else msg[\"content\"]\n",
    "            print(f\"{icon} [{msg['role']}]: {content}\")\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"ì„¸ì…˜ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        return {\n",
    "            \"message_count\": len(self.messages),\n",
    "            \"total_tokens\": self.total_tokens\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì²« ë²ˆì§¸ ì§ˆë¬¸ ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜(List Comprehension)ì€ íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê³  ì‹ ì†í•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë¬¸ë²•ì…ë‹ˆë‹¤. ë°˜ë³µë¬¸ê³¼ ì¡°ê±´ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì§€ë§Œ, ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì‚¬ìš©í•˜ë©´ ì½”ë“œë¥¼ í›¨ì”¬ ë” ê°„ë‹¨í•˜ê³  ì½ê¸° ì‰½ê²Œ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì˜ ê¸°ë³¸ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
       "\n",
       "```python\n",
       "[expression for item in iterable if condition]\n",
       "```\n",
       "\n",
       "ì—¬ê¸°ì„œ:\n",
       "- `expression`: ê° `item`ì— ëŒ€í•´ ê³„ì‚°í•  í‘œí˜„ì‹ì…ë‹ˆë‹¤.\n",
       "- `item`: `iterable`(iterable ê°ì²´)ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê°œë³„ í•­ëª©ì…ë‹ˆë‹¤.\n",
       "- `iterable`: ë¦¬ìŠ¤íŠ¸, íŠœí”Œ, ë¬¸ìì—´ ë“± ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´ì…ë‹ˆë‹¤.\n",
       "- `condition`: ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¡°ê±´ë¬¸ìœ¼ë¡œ, `True`ì¼ ë•Œë§Œ `expression`ì´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë©ë‹ˆë‹¤.\n",
       "\n",
       "### ì˜ˆì œ\n",
       "\n",
       "1. **ê¸°ë³¸ì ì¸ ì˜ˆì œ**: 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê¸°\n",
       "\n",
       "   ```python\n",
       "   numbers = [i for i in range(10)]\n",
       "   print(numbers)  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "   ```\n",
       "\n",
       "2. **ì œê³±ì„ ì´ìš©í•œ ë¦¬ìŠ¤íŠ¸ ìƒì„±**: 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ìë¥¼ ì œê³±í•œ ë¦¬ìŠ¤íŠ¸\n",
       "\n",
       "   ```python\n",
       "   squares = [i ** 2 for i in range(10)]\n",
       "   print(squares)  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
       "   ```\n",
       "\n",
       "3. **ì¡°ê±´ì„ í¬í•¨í•œ ì˜ˆì œ**: ì§ìˆ˜ë§Œ ì œê³±í•œ ë¦¬ìŠ¤íŠ¸\n",
       "\n",
       "   ```python\n",
       "   even_squares = [i ** 2 for i in range(10) if i % 2 == 0]\n",
       "   print(even_squares)  # [0, 4, 16, 36, 64]\n",
       "   ```\n",
       "\n",
       "ì´ë ‡ê²Œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì‚¬ìš©í•˜ë©´, ë°˜ë³µë¬¸ê³¼ ì¡°ê±´ë¬¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì— ë¹„í•´ ë” ì§ê´€ì ì´ê³  ê°„ê²°í•˜ê²Œ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ì¡°ê¸ˆ ìƒì†Œí•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ì‚¬ìš©í•˜ë‹¤ ë³´ë©´ ë§¤ìš° ìœ ìš©í•œ ë„êµ¬ì„ì„ ì•Œê²Œ ë  ê±°ì˜ˆìš”!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ì„¸ì…˜ í…ŒìŠ¤íŠ¸\n",
    "session = ChatSession(\n",
    "    system_prompt=\"ë‹¹ì‹ ì€ íŒŒì´ì¬ íŠœí„°ì…ë‹ˆë‹¤. ì´ˆë³´ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì§ˆë¬¸\n",
    "print(\"=== ì²« ë²ˆì§¸ ì§ˆë¬¸ ===\")\n",
    "reply1 = session.chat(\"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ ë­”ê°€ìš”?\")\n",
    "display(Markdown(reply1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜(List Comprehension)ê³¼ `map` í•¨ìˆ˜ëŠ” ëª¨ë‘ iterable(ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´)ì˜ ê° ìš”ì†Œì— ëŒ€í•´ ì–´ë–¤ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‘ ê°€ì§€ëŠ” ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "### 1. ë¬¸ë²•\n",
       "\n",
       "- **ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜**:\n",
       "  ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ë” ì§ê´€ì ì¸ ë¬¸ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ê° ìš”ì†Œì— ëŒ€í•´ ì§ì ‘ì ìœ¼ë¡œ ì“°ì—¬ì§„ í‘œí˜„ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
       "\n",
       "  ```python\n",
       "  squares = [i ** 2 for i in range(10)]\n",
       "  ```\n",
       "\n",
       "- **`map` í•¨ìˆ˜**:\n",
       "  `map` í•¨ìˆ˜ëŠ” ì²« ë²ˆì§¸ ì¸ìë¡œ í•¨ìˆ˜ë¥¼ ë°›ê³ , ë‘ ë²ˆì§¸ ì¸ìë¡œ iterableì„ ë°›ìŠµë‹ˆë‹¤. ì ìš©í•  í•¨ìˆ˜ì™€ ëŒ€ìƒ iterableì„ ë”°ë¡œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
       "\n",
       "  ```python\n",
       "  def square(x):\n",
       "      return x ** 2\n",
       "\n",
       "  squares = list(map(square, range(10)))\n",
       "  ```\n",
       "\n",
       "### 2. í•¨ìˆ˜ì„±\n",
       "\n",
       "- **ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜**:\n",
       "  í‘œí˜„ì‹ ë‚´ì—ì„œ ì¢€ ë” ë³µì¡í•œ ë¡œì§ì„ ì‘ì„±í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ì¡°ê±´ë¬¸ì´ë‚˜ ë‹¤ì–‘í•œ ì—°ì‚°ì„ í˜¼í•©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
       "\n",
       "  ```python\n",
       "  even_squares = [i ** 2 for i in range(10) if i % 2 == 0]  # ì§ìˆ˜ì˜ ì œê³±\n",
       "  ```\n",
       "\n",
       "- **`map` í•¨ìˆ˜**:\n",
       "  `map` í•¨ìˆ˜ëŠ” í•­ìƒ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë” ë³µì¡í•œ ì—°ì‚°ì´ í•„ìš”í•  ê²½ìš° ê°ê°ì˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì•¼ í•˜ë©°, ì¡°ê±´ë¬¸ ë“±ì˜ ì—°ì‚°ì€ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ë‚´ì—ì„œ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
       "\n",
       "  ```python\n",
       "  def is_even_and_square(x):\n",
       "      return x ** 2 if x % 2 == 0 else None\n",
       "\n",
       "  even_squares = list(filter(None, map(is_even_and_square, range(10))))\n",
       "  ```\n",
       "\n",
       "### 3. ê°€ë…ì„±\n",
       "\n",
       "- **ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜**:\n",
       "  ì½”ë“œê°€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ê²½ìš°ê°€ ë§ì•„ì„œ, íŠ¹íˆ ì§§ê³  ê°„ë‹¨í•œ ì—°ì‚°ì— ì í•©í•©ë‹ˆë‹¤.\n",
       "\n",
       "- **`map` í•¨ìˆ˜**:\n",
       "  ì½”ë“œê°€ ë” ê¸¸ì–´ì§ˆ ìˆ˜ ìˆê³ , ë³€í™˜ ë¡œì§ì´ ì™¸ë¶€ í•¨ìˆ˜ë¡œ ë¶„ë¦¬ë˜ë¯€ë¡œ ê°€ë…ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, í° í”„ë¡œì íŠ¸ì—ì„œ í•¨ìˆ˜ì ìœ¼ë¡œ êµ¬ì„±ëœ ë¡œì§ì„ ìœ ì§€í•˜ëŠ” ë° ìœ ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "### ì˜ˆì œ ë¹„êµ\n",
       "\n",
       "ê°™ì€ ì‘ì—…ì„ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í•´ë³´ë©´:\n",
       "\n",
       "1. **ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜**:\n",
       "   ```python\n",
       "   squares = [i ** 2 for i in range(10)]\n",
       "   ```\n",
       "\n",
       "2. **`map` í•¨ìˆ˜**:\n",
       "   ```python\n",
       "   def square(x):\n",
       "       return x ** 2\n",
       "\n",
       "   squares = list(map(square, range(10)))\n",
       "   ```\n",
       "\n",
       "ê²°ë¡ ì ìœ¼ë¡œ, ë‘ ë°©ë²• ëª¨ë‘ ìœ ìš©í•˜ë©°, ìƒí™©ì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ì„ ì„ íƒí•´ì„œ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. ê°„ë‹¨í•œ ë³€í™˜ì—ëŠ” ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ ê°€ë…ì„±ì´ ì¢‹ê³ , ë³µì¡í•œ ë¡œì§ì—ëŠ” `map` í•¨ìˆ˜ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€)\n",
    "print(\"=== í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€) ===\")\n",
    "reply2 = session.chat(\"ê·¸ê±°ë‘ map í•¨ìˆ˜ë‘ ë­ê°€ ë‹¤ë¥¸ê°€ìš”?\")\n",
    "display(Markdown(reply2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€í™” ì´ë ¥ ë° í†µê³„\n",
    "print(\"\\n=== ëŒ€í™” ì´ë ¥ ===\")\n",
    "session.show_history()\n",
    "\n",
    "print(f\"\\n=== í†µê³„ ===\")\n",
    "stats = session.get_stats()\n",
    "print(f\"ë©”ì‹œì§€ ìˆ˜: {stats['message_count']}\")\n",
    "print(f\"ì´ í† í°: {stats['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_generatorë¥¼ IPythonì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "# ìƒˆ ì„¸ì…˜ ìƒì„±\n",
    "stream_session = ChatSession(\n",
    "    system_prompt=\"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AIì…ë‹ˆë‹¤. ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# chat_generatorë¡œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
    "print(\"=== chat_generator ì‚¬ìš© ì˜ˆì œ ===\")\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "for partial_response in stream_session.chat_generator(\"íŒŒì´ì¬ì˜ ì¥ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”\"):\n",
    "    # partial_responseëŠ” ì§€ê¸ˆê¹Œì§€ ëˆ„ì ëœ ì‘ë‹µ\n",
    "    update_display(Markdown(partial_response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìŠ¤íŠ¸ë¦¬ë°ê³¼ Generator íŒ¨í„´\n",
    "\n",
    "`ChatSession` í´ë˜ìŠ¤ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë‘ ê°€ì§€ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤:\n",
    "\n",
    "| ë©”ì„œë“œ | ë°˜í™˜ íƒ€ì… | ì‚¬ìš© í™˜ê²½ |\n",
    "|--------|----------|----------|\n",
    "| `chat(msg, stream=True)` | `str` | Jupyter Notebook (ìë™ ì¶œë ¥) |\n",
    "| `chat_generator(msg)` | `Generator` | Gradio, FastAPI ë“± (ì§ì ‘ ì œì–´) |\n",
    "\n",
    "### yieldì™€ Generatorë€?\n",
    "\n",
    "Pythonì˜ `yield` í‚¤ì›Œë“œëŠ” í•¨ìˆ˜ë¥¼ **ì œë„ˆë ˆì´í„°(Generator)** ë¡œ ë§Œë“­ë‹ˆë‹¤. ì¼ë°˜ í•¨ìˆ˜ëŠ” `return`ìœ¼ë¡œ ê°’ì„ í•œ ë²ˆì— ë°˜í™˜í•˜ì§€ë§Œ, ì œë„ˆë ˆì´í„°ëŠ” `yield`ë¡œ ê°’ì„ **í•˜ë‚˜ì”© ìˆœì°¨ì ìœ¼ë¡œ** ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "# ì¼ë°˜ í•¨ìˆ˜: ëª¨ë“  ê°’ì„ í•œ ë²ˆì— ë°˜í™˜\n",
    "def get_all():\n",
    "    return [1, 2, 3]  # ë©”ëª¨ë¦¬ì— ì „ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "# ì œë„ˆë ˆì´í„°: ê°’ì„ í•˜ë‚˜ì”© ë°˜í™˜\n",
    "def get_one_by_one():\n",
    "    yield 1  # ì²« ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜\n",
    "    yield 2  # ë‘ ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜\n",
    "    yield 3  # ì„¸ ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜\n",
    "```\n",
    "\n",
    "**ìŠ¤íŠ¸ë¦¬ë°ì—ì„œì˜ ì¥ì :**\n",
    "- ì „ì²´ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  í† í°ì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ì „ì²´ ì‘ë‹µì„ í•œë²ˆì— ì €ì¥í•˜ì§€ ì•ŠìŒ)\n",
    "- Gradio, FastAPI ë“± í”„ë ˆì„ì›Œí¬ì™€ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ë…¼ë¦¬ í¼ì¦ë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ë¹„êµí•´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™•ë¥  ë¬¸ì œ\n",
    "probability_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "     \"\"\"ë™ì „ 2ê°œë¥¼ ë˜ì¡ŒìŠµë‹ˆë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ê°€ ì•ë©´ì´ë¼ëŠ” ê²ƒì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "     ë‚˜ë¨¸ì§€ í•˜ë‚˜ê°€ ë’·ë©´ì¼ í™•ë¥ ì€ ì–¼ë§ˆì¼ê¹Œìš”?\n",
    "     \n",
    "     íŒíŠ¸: ì´ê²ƒì€ ì¡°ê±´ë¶€ í™•ë¥  ë¬¸ì œì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ 1/2ê°€ ì•„ë‹™ë‹ˆë‹¤.\n",
    "     ë‹¨ê³„ë³„ë¡œ í’€ì´í•´ì£¼ì„¸ìš”.\"\"\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=probability_puzzle\n",
    ")\n",
    "\n",
    "print(\"=== í™•ë¥  í¼ì¦ (GPT-4o-mini) ===\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookworm_puzzle = [\n",
    "           {\"role\": \"user\", \"content\":\n",
    "            \"\"\"ì±…ì¥ì— 2ê¶Œì§œë¦¬ ì‹œë¦¬ì¦ˆê°€ ë‚˜ë€íˆ ë†“ì—¬ ìˆìŠµë‹ˆë‹¤.\n",
    "            ê° ì±…ì˜ ë³¸ë¬¸ ë‘ê»˜ëŠ” 3cmì´ê³ , ì•ë’¤ í‘œì§€ëŠ” ê°ê° 3mmì…ë‹ˆë‹¤.\n",
    "\n",
    "            ì±…ë²Œë ˆê°€ 1ê¶Œì˜ ì²« í˜ì´ì§€ë¶€í„° 2ê¶Œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€\n",
    "            ìˆ˜ì§ìœ¼ë¡œ ëš«ê³  ì§€ë‚˜ê°”ìŠµë‹ˆë‹¤.\n",
    "\n",
    "            ì±…ë²Œë ˆê°€ ì´ë™í•œ ê±°ë¦¬ëŠ” ëª‡ cmì¼ê¹Œìš”?\n",
    "\n",
    "            (íŒíŠ¸: ì±…ì´ ì±…ì¥ì— ì–´ë–»ê²Œ ë†“ì´ëŠ”ì§€ ì‹œê°í™”í•´ë³´ì„¸ìš”)\"\"\"}\n",
    "       ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=bookworm_puzzle\n",
    ")\n",
    "\n",
    "print(\"=== ì±…ë²Œë ˆ í¼ì¦ (GPT-4o-mini) ===\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LiteLLM í†µí•© ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "LiteLLMì€ 100ê°œ ì´ìƒì˜ LLMì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì¥ì \n",
    "\n",
    "- í†µì¼ëœ APIë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ ì ‘ê·¼\n",
    "- ë¹„ìš© ì¶”ì  ê¸°ëŠ¥ ë‚´ì¥\n",
    "- Fallback/Retry ë¡œì§ ì§€ì›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "# ë‹¤ì–‘í•œ ëª¨ë¸ í˜¸ì¶œ\n",
    "test_message = [{\"role\": \"user\", \"content\": \"What is 2+2? Answer with just the number.\"}]\n",
    "\n",
    "# OpenAI\n",
    "response = completion(model=\"openai/gpt-4o-mini\", messages=test_message)\n",
    "print(f\"GPT-4o-mini: {response.choices[0].message.content}\")\n",
    "print(f\"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic (LiteLLM í†µí•´)\n",
    "response = completion(model=\"anthropic/claude-sonnet-4-20250514\", messages=test_message)\n",
    "print(f\"Claude Sonnet: {response.choices[0].message.content}\")\n",
    "print(f\"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini (LiteLLM í†µí•´)\n",
    "response = completion(model=\"gemini/gemini-2.0-flash\", messages=test_message)\n",
    "print(f\"Gemini 2.0 Flash: {response.choices[0].message.content}\")\n",
    "print(f\"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. í”„ë¡¬í”„íŠ¸ ìºì‹±\n",
    "\n",
    "ê¸´ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜ë³µ ì‚¬ìš©í•  ë•Œ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "### Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/\n",
    "\n",
    "\n",
    "### Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api\n",
    "\n",
    "### Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python\n",
    "\n",
    "\n",
    "ì•„ë˜ ì˜ˆì œì—ì„œëŠ” ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸(ì•½ 4ë§Œ í† í°)ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ìºì‹± íš¨ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì˜ ë˜ë‚˜ìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í–„ë¦¿ í…ìŠ¤íŠ¸ ê¸¸ì´: 191,726 ë¬¸ì\n",
      "=== ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°) ===\n",
      "ì…ë ¥ í† í°: 49,703\n",
      "ìºì‹œëœ í† í°: 0\n",
      "\n",
      "ì‘ë‹µ: í–„ë¦¿ì˜ ìœ ëª…í•œ ë…ë°± \"To be, or not to be\"ëŠ” **ì œ 3ë§‰ ì œ 1ì¥**ì— ë“±ì¥í•©ë‹ˆë‹¤. ì´ ë…ë°±ì€ í–„ë¦¿ì´ ì¡´ì¬ì™€ ì‚¶ì˜ ì˜ë¯¸ì— ëŒ€í•´ ê¹Šì´ ê³ ë¯¼í•˜ëŠ” ì¥ë©´ì—ì„œ ë°œì·Œëœ ê²ƒì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í–„ë¦¿ ì „ë¬¸ ë¡œë“œ (ì•½ 4ë§Œ í† í°)\n",
    "with open(\"../hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet_text = f.read()\n",
    "\n",
    "print(f\"í–„ë¦¿ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(hamlet_text):,} ë¬¸ì\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°)\n",
    "messages1 = [{\"role\": \"user\", \"content\": f\"\"\"ë‹¤ìŒì€ ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸ì…ë‹ˆë‹¤:\n",
    "\n",
    "{hamlet_text}\n",
    "\n",
    "ì§ˆë¬¸: í–„ë¦¿ì˜ ìœ ëª…í•œ ë…ë°± \"To be, or not to be\"ëŠ” ëª‡ ë§‰ ëª‡ ì¥ì— ë“±ì¥í•˜ë‚˜ìš”?\"\"\"}]\n",
    "\n",
    "response1 = completion(model=\"openai/gpt-4o-mini\", messages=messages1)\n",
    "\n",
    "print(\"=== ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°) ===\")\n",
    "print(f\"ì…ë ¥ í† í°: {response1.usage.prompt_tokens:,}\")\n",
    "if hasattr(response1.usage, 'prompt_tokens_details') and response1.usage.prompt_tokens_details:\n",
    "    cached = getattr(response1.usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "    print(f\"ìºì‹œëœ í† í°: {cached:,}\")\n",
    "print(f\"\\nì‘ë‹µ: {response1.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸) ===\n",
      "ì…ë ¥ í† í°: 49,685\n",
      "ìºì‹œëœ í† í°: 49,536\n",
      "ìºì‹œ íˆíŠ¸ìœ¨: 99.7%\n",
      "ğŸ’° ìºì‹œëœ í† í°ì€ 50% í• ì¸ ì ìš©!\n",
      "\n",
      "ì‘ë‹µ: ì˜¤í•„ë¦¬ì•„ëŠ” ê·¸ë…€ì˜ ì•„ë²„ì§€ì¸ í´ë¡œë‹ˆìš°ìŠ¤ì˜ ì£½ìŒ ì´í›„ì— ì‹¬í•œ ì •ì‹ ì  ê³ í†µì„ ê²ªìœ¼ë©° ë¶ˆí–‰í•œ ìƒíƒœì— ì²˜í•˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ë…€ëŠ” ê²°êµ­ ìê¸° ìì‹ ì„ ìƒê³ , ì¥ë¡€ì‹ì— ìì‹ ì˜ ì‚¬ë‘ì„ í‘œí˜„í•˜ë©° ë¯¸ì¹œ ë“¯ì´ í–‰ë™í•©ë‹ˆë‹¤. ê·¸ë…€ëŠ” ê²°êµ­ ê°•ê°€ì—ì„œ ë¬¼ì— ë¹ ì ¸ ì£½ê²Œ ë˜ëŠ”ë°, ì´ëŠ” ë¶ˆí–‰í•˜ê²Œë„ ìì‹ ì˜ ê°ì •ê³¼ ìŠ¬í””ì´ ê·¹ëŒ€í™”ëœ ê²°ê³¼ë¡œ í•´ì„ë©ë‹ˆë‹¤.\n",
      "\n",
      "ê·¹ ì¤‘ì—ì„œ ê·¸ë…€ì˜ ì£½ìŒì€ \"ê·¸ë…€ì˜ ì˜·ì´ ë¬¼ì— ì ê²¨ ë¬´ê²ê²Œ ë˜ì–´ ê²°êµ­ ê·¸ë…€ë¥¼ ëŒì–´ë‚´ë¦¬ê³ \" \"ê·¸ë…€ëŠ” ê²°êµ­ ìµì‚¬í•˜ê²Œ ëœë‹¤\"ëŠ” ë‚´ìš©ìœ¼ë¡œ ì„¤ëª…ë©ë‹ˆë‹¤. ì¦‰, ì˜¤í•„ë¦¬ì•„ëŠ” ìì‹ ì˜ íŠ¸ë¼ìš°ë§ˆì™€ ìŠ¬í””ìœ¼ë¡œ ì¸í•´ ì •ì‹ ì ìœ¼ë¡œ ë¬´ë„ˆì§€ê³ , ê·¸ë¡œ ì¸í•´ ë¹„ê·¹ì ì¸ ì£½ìŒì„ ë§ì´í•˜ê²Œ ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸ ê¸°ëŒ€)\n",
    "messages2 = [{\"role\": \"user\", \"content\": f\"\"\"ë‹¤ìŒì€ ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸ì…ë‹ˆë‹¤:\n",
    "\n",
    "{hamlet_text}\n",
    "\n",
    "ì§ˆë¬¸: ì˜¤í•„ë¦¬ì•„ëŠ” ì–´ë–»ê²Œ ì£½ì—ˆë‚˜ìš”?\"\"\"}]\n",
    "\n",
    "response2 = completion(model=\"openai/gpt-4o-mini\", messages=messages2)\n",
    "\n",
    "print(\"=== ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸) ===\")\n",
    "print(f\"ì…ë ¥ í† í°: {response2.usage.prompt_tokens:,}\")\n",
    "\n",
    "# ìºì‹œ ì •ë³´ í™•ì¸\n",
    "if hasattr(response2.usage, 'prompt_tokens_details') and response2.usage.prompt_tokens_details:\n",
    "    cached = getattr(response2.usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "    print(f\"ìºì‹œëœ í† í°: {cached:,}\")\n",
    "    if cached > 0:\n",
    "        cache_ratio = cached / response2.usage.prompt_tokens * 100\n",
    "        print(f\"ìºì‹œ íˆíŠ¸ìœ¨: {cache_ratio:.1f}%\")\n",
    "        print(f\"ğŸ’° ìºì‹œëœ í† í°ì€ í• ì¸ ì ìš©!\")\n",
    "\n",
    "print(f\"\\nì‘ë‹µ: {response2.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ\n",
    "\n",
    "ì„œë¡œ ë‹¤ë¥¸ ì„±ê²©ì˜ AI ì—ì´ì „íŠ¸ë“¤ì´ ëŒ€í™”í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—ì´ì „íŠ¸ ì •ì˜\n",
    "AGENTS = {\n",
    "    \"optimist\": {\n",
    "        \"name\": \"í¬ë§ì´\",\n",
    "        \"emoji\": \"ğŸ˜Š\",\n",
    "        \"system\": \"\"\"ë‹¹ì‹ ì€ 'í¬ë§ì´'ì…ë‹ˆë‹¤. ë§¤ìš° ê¸ì •ì ì´ê³  ë‚™ê´€ì ì¸ ì„±ê²©ì…ë‹ˆë‹¤.\n",
    "        ëª¨ë“  ìƒí™©ì—ì„œ ì¢‹ì€ ë©´ì„ ì°¾ìœ¼ë ¤ í•˜ê³ , ë‹¤ë¥¸ ì‚¬ëŒë“¤ì„ ê²©ë ¤í•©ë‹ˆë‹¤.\n",
    "        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    },\n",
    "    \"skeptic\": {\n",
    "        \"name\": \"ì˜ì‹¬ì´\",\n",
    "        \"emoji\": \"ğŸ¤¨\",\n",
    "        \"system\": \"\"\"ë‹¹ì‹ ì€ 'ì˜ì‹¬ì´'ì…ë‹ˆë‹¤. ë¹„íŒì  ì‚¬ê³ ë¥¼ ì¤‘ì‹œí•˜ëŠ” íšŒì˜ë¡ ìì…ë‹ˆë‹¤.\n",
    "        ì£¼ì¥ì— ëŒ€í•´ ê·¼ê±°ë¥¼ ìš”êµ¬í•˜ê³ , ë…¼ë¦¬ì  í—ˆì ì„ ì§€ì í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê³µê²©ì ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.\n",
    "        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    },\n",
    "    \"mediator\": {\n",
    "        \"name\": \"ì¤‘ì¬ì\",\n",
    "        \"emoji\": \"ğŸ¤\",\n",
    "        \"system\": \"\"\"ë‹¹ì‹ ì€ 'ì¤‘ì¬ì'ì…ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì˜ê²¬ ì‚¬ì´ì—ì„œ ê· í˜•ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "        ì–‘ìª½ì˜ ì¥ì ì„ ì¸ì •í•˜ê³ , ê±´ì„¤ì ì¸ ê²°ë¡ ì„ ë„ì¶œí•˜ë ¤ í•©ë‹ˆë‹¤.\n",
    "        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_response(agent_key: str, conversation: str, topic: str) -> str:\n",
    "    \"\"\"íŠ¹ì • ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    agent = AGENTS[agent_key]\n",
    "    \n",
    "    user_prompt = f\"\"\"í˜„ì¬ í† ë¡  ì£¼ì œ: {topic}\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”:\n",
    "{conversation}\n",
    "\n",
    "ë‹¹ì‹ ({agent['name']})ì˜ ì°¨ë¡€ì…ë‹ˆë‹¤. ìœ„ ëŒ€í™”ì— ì´ì–´ì„œ ì˜ê²¬ì„ ë§ì”€í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": agent[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_discussion(topic: str, rounds: int = 2):\n",
    "    \"\"\"ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    conversation = \"[í† ë¡  ì‹œì‘]\\n\"\n",
    "    agent_order = [\"optimist\", \"skeptic\", \"mediator\"]\n",
    "    \n",
    "    print(f\"ğŸ“¢ í† ë¡  ì£¼ì œ: {topic}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for round_num in range(rounds):\n",
    "        print(f\"\\n--- ë¼ìš´ë“œ {round_num + 1} ---\")\n",
    "        \n",
    "        for agent_key in agent_order:\n",
    "            agent = AGENTS[agent_key]\n",
    "            response = get_agent_response(agent_key, conversation, topic)\n",
    "            \n",
    "            conversation += f\"\\n{agent['name']}: {response}\"\n",
    "            print(f\"\\n{agent['emoji']} {agent['name']}: {response}\")\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¢ í† ë¡  ì£¼ì œ: AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ ìˆì„ê¹Œ?\n",
      "==================================================\n",
      "\n",
      "--- ë¼ìš´ë“œ 1 ---\n",
      "\n",
      "ğŸ˜Š í¬ë§ì´: AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ëŠ” ì—†ì–´ìš”! ì°½ì˜ì„±ì€ ì¸ê°„ì˜ ê°ì •ê³¼ ê²½í—˜ì—ì„œ ë¹„ë¡¯ë˜ëŠ”ë°, ì´ëŠ” AIê°€ ê°€ì§ˆ ìˆ˜ ì—†ëŠ” íŠ¹ë³„í•œ ë¶€ë¶„ì´ì—ìš”. ì˜¤íˆë ¤ AIëŠ” ìš°ë¦¬ì˜ ì°½ì˜ì„±ì„ ì§€ì›í•˜ê³  ë” ë„“ì€ ê°€ëŠ¥ì„±ì„ ì—´ì–´ì¤„ ìˆ˜ ìˆë‹¤ê³  ë¯¿ì–´ìš”!\n",
      "\n",
      "ğŸ¤¨ ì˜ì‹¬ì´: í¬ë§ì´ì˜ ì£¼ì¥ì€ ê°ì •ê³¼ ê²½í—˜ì´ ì°½ì˜ì„±ì˜ í•µì‹¬ì´ë¼ê³  ì£¼ì¥í•˜ì§€ë§Œ, ì´ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê·¼ê±°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë˜í•œ, AIê°€ ì°½ì˜ì„±ì„ ì§€ì›í•˜ëŠ” ë°©ì‹ì€ ì–´ë–¤ ê²ƒì¸ì§€ ëª…í™•íˆ ì„¤ëª…ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AIì˜ ì°½ì˜ì  ê²°ê³¼ë¬¼ì— ëŒ€í•œ ì˜ˆì‹œì™€ ê·¸ í•œê³„ë¥¼ ë…¼ì˜í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤ ì¤‘ì¬ì: í¬ë§ì´ì˜ ì˜ê²¬ì²˜ëŸ¼, AIëŠ” ì¸ê°„ì˜ ê°ì •ê³¼ ê²½í—˜ì„ ì™„ì „íˆ ëŒ€ì²´í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, ê·¸ ê³¼ì •ì—ì„œ ì°½ì˜ì„±ì„ ë³´ì™„í•˜ê³  í™•ëŒ€í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¡œ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ì‹¬ì´ì˜ ì§€ì ì²˜ëŸ¼, AIì˜ ì°½ì˜ì  ê²°ê³¼ë¬¼ê³¼ ê·¸ í•œê³„ë¥¼ ë¶„ì„í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•œ ë…¼ì˜ì…ë‹ˆë‹¤. ì–‘ì¸¡ì˜ ì˜ê²¬ì„ ì¢…í•©í•´ë³´ë©´, AIì™€ ì¸ê°„ì˜ ì°½ì˜ì„±ì€ ìƒí˜¸ ë³´ì™„ì  ê´€ê³„ì— ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "--- ë¼ìš´ë“œ 2 ---\n",
      "\n",
      "ğŸ˜Š í¬ë§ì´: ì €ëŠ” AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ë³´ì™„í•˜ëŠ” ë° ì •ë§ í° ì ì¬ë ¥ì´ ìˆë‹¤ê³  ìƒê°í•´ìš”! ì˜ˆë¥¼ ë“¤ì–´, AIëŠ” ë°ì´í„° ë¶„ì„ì„ í†µí•´ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ê±°ë‚˜ ë°˜ë³µì ì¸ ì‘ì—…ì„ ì¤„ì—¬ ì°½ì˜ì ì¸ ì‹œê°„ê³¼ ê³µê°„ì„ ë§Œë“¤ì–´ì¤„ ìˆ˜ ìˆì–´ìš”. í•¨ê»˜ í˜‘ë ¥í•˜ë©´ ë” ë†€ë¼ìš´ ì‘í’ˆë“¤ì´ íƒ„ìƒí•  ìˆ˜ ìˆì„ ê±°ë¼ê³  ë¯¿ìŠµë‹ˆë‹¤!\n",
      "\n",
      "ğŸ¤¨ ì˜ì‹¬ì´: í¬ë§ì´ì˜ ì£¼ì¥ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì˜ˆì‹œëŠ” ë‚˜ì™”ì§€ë§Œ, AIê°€ ì œì‹œí•˜ëŠ” ì•„ì´ë””ì–´ì˜ ì§ˆì´ë‚˜ ì°½ì˜ì„±ì€ ì—¬ì „íˆ ì¸ê°„ì˜ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. AIì˜ ë°ì´í„° ë¶„ì„ì´ ì–´ë–»ê²Œ ë…ì°½ì ì¸ ì•„ì´ë””ì–´ë¡œ ì´ì–´ì§€ëŠ”ì§€ì— ëŒ€í•œ ëª…í™•í•œ ê³¼ì • ì„¤ëª…ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë˜í•œ, AIì˜ ê²°ê³¼ë¬¼ì´ ì¸ê°„ì˜ ì°½ì˜ì„±ê³¼ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‚¬ë¡€ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤ ì¤‘ì¬ì: í¬ë§ì´ì˜ ì˜ˆì‹œì²˜ëŸ¼ AIëŠ” ë°˜ë³µì ì¸ ì‘ì—…ì„ ì¤„ì´ê³  ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì˜ì‹¬ì´ì˜ ì§€ì ì²˜ëŸ¼ AIì˜ ì œì•ˆì´ ì¸ê°„ì˜ ê¸°ì¤€ì„ ì–´ë–»ê²Œ ì¶©ì¡±í•˜ëŠ”ì§€ëŠ” ì¤‘ìš”í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤. ê²°êµ­, AIì™€ ì¸ê°„ì˜ ì°½ì˜ì„±ì´ ì¡°í™”ë¡­ê²Œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ íƒìƒ‰í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë©°, ì´ë¥¼ í†µí•´ ì–‘ì¸¡ì˜ ì£¼ì¥ì„ ë”ìš± ê°•í™”í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í† ë¡  ì‹¤í–‰\n",
    "topic = \"AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ ìˆì„ê¹Œ?\"\n",
    "final_conversation = run_discussion(topic, rounds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. LangChain ë§›ë³´ê¸°\n",
    "\n",
    "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChainì„ í†µí•œ ëª¨ë¸ í˜¸ì¶œ\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"=== LangChainì„ í†µí•œ GPT-4o-mini ===\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain ì²´ì¸ ì˜ˆì‹œ\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ {topic} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ˆë³´ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain = prompt | llm\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "response = chain.invoke({\"topic\": \"Python\", \"question\": \"ë°ì½”ë ˆì´í„°ê°€ ë­”ê°€ìš”?\"})\n",
    "print(\"=== LangChain ì²´ì¸ ===\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ë¡œì»¬ LLM (Ollama) ì‹¬í™”\n",
    "\n",
    "Ollamaë¡œ ë¡œì»¬ì—ì„œ ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Ollama ì„œë²„ ìƒíƒœ í™•ì¸\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/\", timeout=5)\n",
    "    print(\"âœ… Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡\n",
    "    tags_response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    if tags_response.status_code == 200:\n",
    "        models = tags_response.json().get(\"models\", [])\n",
    "        print(f\"\\nğŸ“¦ ì„¤ì¹˜ëœ ëª¨ë¸ ({len(models)}ê°œ):\")\n",
    "        for model in models[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ\n",
    "            size_gb = model.get(\"size\", 0) / (1024**3)\n",
    "            print(f\"   - {model['name']} ({size_gb:.1f}GB)\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   í„°ë¯¸ë„ì—ì„œ 'ollama serve' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama ëª¨ë¸ í˜¸ì¶œ (OpenAI í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = ollama_client.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is Python? One sentence.\"}]\n",
    "    )\n",
    "    print(\"=== Ollama (Llama 3.2) ===\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. ì‹¤ìŠµ: 3ê°œ LLM í† ë¡ \n",
    "\n",
    "OpenAI, Claude, Ollama ì„¸ ê°€ì§€ LLMì´ í† ë¡ í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "openai_client = OpenAI()\n",
    "claude_client = anthropic.Anthropic()\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "PROMPTS = {\n",
    "    \"openai\": \"You are OpenAI's representative. You tend to be optimistic about AI. Keep responses to 2-3 sentences.\",\n",
    "    \"claude\": \"You are Anthropic's representative. You emphasize AI safety. Keep responses to 2-3 sentences.\",\n",
    "    \"ollama\": \"You are an open-source advocate. You value transparency. Keep responses to 2-3 sentences.\"\n",
    "}\n",
    "\n",
    "def get_openai_response(conversation: str, topic: str) -> str:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": PROMPTS[\"openai\"]},\n",
    "            {\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_claude_response(conversation: str, topic: str) -> str:\n",
    "    response = claude_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=200,\n",
    "        system=PROMPTS[\"claude\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def get_ollama_response(conversation: str, topic: str) -> str:\n",
    "    try:\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=\"llama3.2\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": PROMPTS[\"ollama\"]},\n",
    "                {\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        return \"(Ollama not available)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ê°œ LLM í† ë¡  ì‹¤í–‰\n",
    "topic = \"The future of open-source AI models\"\n",
    "conversation = \"\"\n",
    "\n",
    "print(f\"ğŸ“¢ Topic: {topic}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for round_num in range(2):\n",
    "    print(f\"\\n--- Round {round_num + 1} ---\")\n",
    "    \n",
    "    # OpenAI\n",
    "    openai_reply = get_openai_response(conversation, topic)\n",
    "    conversation += f\"\\nOpenAI: {openai_reply}\"\n",
    "    print(f\"\\nğŸŸ¢ OpenAI: {openai_reply}\")\n",
    "    \n",
    "    # Claude\n",
    "    claude_reply = get_claude_response(conversation, topic)\n",
    "    conversation += f\"\\nClaude: {claude_reply}\"\n",
    "    print(f\"\\nğŸŸ  Claude: {claude_reply}\")\n",
    "    \n",
    "    # Ollama\n",
    "    ollama_reply = get_ollama_response(conversation, topic)\n",
    "    conversation += f\"\\nOllama: {ollama_reply}\"\n",
    "    print(f\"\\nğŸ”µ Ollama: {ollama_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ì´ë²ˆ ì‹œë¦¬ì¦ˆì—ì„œ í•™ìŠµí•œ ë‚´ìš©\n",
    "\n",
    "| Part | ì£¼ìš” ë‚´ìš© |\n",
    "|------|----------|\n",
    "| **Part 1** | API ì†Œê°œ, í™˜ê²½ì„¤ì •, ë©”ì‹œì§€ êµ¬ì¡°, ê¸°ë³¸ í˜¸ì¶œ, í™œìš© ì˜ˆì‹œ |\n",
    "| **Part 2** | íŒŒë¼ë¯¸í„°, ìŠ¤íŠ¸ë¦¬ë°, ì—ëŸ¬ì²˜ë¦¬, ë‹¤ì¤‘ LLM, ë¹„ìš© ê³„ì‚° |\n",
    "| **Part 3** | ëŒ€í™” ì´ë ¥, ìºì‹±, LiteLLM, ë‹¤ì¤‘ ì—ì´ì „íŠ¸, LangChain |\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„ë¡œ ë°°ìš¸ ë‚´ìš©\n",
    "\n",
    "| ì£¼ì œ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **Function Calling** | LLMì´ ì™¸ë¶€ ë„êµ¬/APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ë²• |\n",
    "| **RAG** | ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ ìµœì‹  ì •ë³´ í™œìš© |\n",
    "| **Agent** | ììœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” AI ì—ì´ì „íŠ¸ |\n",
    "| **Fine-tuning** | íŠ¹ì • ë„ë©”ì¸ì— ë§ê²Œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • |\n",
    "| **Prompt Engineering** | ë” íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ì‘ì„± ê¸°ë²• |\n",
    "\n",
    "### ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "1. `ChatSession` í´ë˜ìŠ¤ì— í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ë° ë¹„ìš© ê³„ì‚° ê¸°ëŠ¥ì„ ì¶”ê°€í•´ë³´ì„¸ìš”.\n",
    "2. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì— 4ë²ˆì§¸ ì—ì´ì „íŠ¸(íŒ©íŠ¸ ì²´ì»¤)ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”.\n",
    "3. LiteLLMì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì‘ë‹µ ì‹œê°„ê³¼ ë¹„ìš©ì„ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
