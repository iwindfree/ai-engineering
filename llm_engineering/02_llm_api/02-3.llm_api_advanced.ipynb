{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API 고급 (Part 3/3)\n",
    "\n",
    "이 노트북은 LLM API 시리즈의 마지막 파트로, 프로덕션 수준의 고급 기법들을 다룹니다.\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "| 목표 | 설명 |\n",
    "|------|------|\n",
    "| 대화 이력 관리 | 멀티턴 대화 시스템 구현 |\n",
    "| 추론 능력 테스트 | 논리 퍼즐로 모델 성능 비교 |\n",
    "| 프롬프트 캐싱 | 비용 절감 기법 |\n",
    "| LiteLLM | 100+ LLM 통합 인터페이스 |\n",
    "| 다중 에이전트 | 여러 AI가 협업하는 시스템 |\n",
    "| LangChain | LLM 애플리케이션 프레임워크 |\n",
    "\n",
    "## 시리즈 구성\n",
    "\n",
    "- **Part 1**: LLM API 기초 - 환경설정, 메시지 구조, 기본 호출\n",
    "- **Part 2**: LLM API 중급 - 파라미터, 스트리밍, 에러처리, 다중 LLM\n",
    "- **Part 3 (현재)**: LLM API 고급 - 대화 이력, 캐싱, 에이전트, 프레임워크\n",
    "\n",
    "## 사전 요구사항\n",
    "\n",
    "- Part 1, 2 완료\n",
    "- OpenAI API 키 (필수)\n",
    "- Anthropic, Google API 키 (선택)\n",
    "- `litellm`, `langchain-openai` 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1330222323.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install litellm langchain-openai\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치 (필요시 주석 해제)\n",
    "#pip install litellm langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 클라이언트 초기화\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 대화 이력 관리\n",
    "\n",
    "LLM API는 상태를 유지하지 않습니다. 대화의 맥락을 유지하려면 이전 메시지들을 함께 전송해야 합니다.\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "```\n",
    "요청 1: [system, user1] → assistant1\n",
    "요청 2: [system, user1, assistant1, user2] → assistant2\n",
    "요청 3: [system, user1, assistant1, user2, assistant2, user3] → assistant3\n",
    "```\n",
    "\n",
    "간단한 예를 들어 설명하겠습니다. 아래의 코드를 수행해보면 재미있는 현상을 발견할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Windfree! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, My name is windfree.\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I'm designed to respect user privacy and confidentiality.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 호출에서 내 이름을 말해준 후에 두번째 호출에서 내 이름을 물어보았을 때 LLM 은 내 이름을 모른다는 답을 하고 있습니다. 이유가 뭘까요? LLM 에 대한 모든 호출은 완전히 Stateless 한 상태입니다. 매번 완전히 새로운 호출인 셈이죠. LLM 이 “기억” 을 가진 것처럼 만드는 것은 AI 개발자의 몫입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Windfree.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, My name is windfree.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, Windfree! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당연한 얘기일 수 있지만,  정리해보면:\n",
    "\n",
    " * LLM에 대한 모든 호출은 무상태(stateless)다.\n",
    " * 매번 지금까지의 전체 대화를 입력 프롬프트에 담아 전달한다.\n",
    " * 이게 LLM이 기억을 가진 것 같은 착각을 만든다 — 대화 맥락을 유지하는 것처럼 보이게 하지만 이건 트릭이다.\n",
    " * 매번 전체 대화를 제공한 결과일 뿐 LLM은 그저 시퀀스에서 다음에 올 가장 가능성 높은 토큰을 예측할 뿐이다.\n",
    " * 시퀀스에 “내 이름은 windfree야”가 있고 나중에 “내 이름이 뭐지?”라고 물으면… windfree라고 예측하는 것!\n",
    "\n",
    "많은 제품들이 정확히 이 트릭을 사용합니다. 메시지를 보낼 때마다 전체 대화가 함께 전달되는 겁니다. “그러면 매번 이전 대화 전체에 대해 추가 비용을 내야 하는 건가요?” 네. 당연히 그렇습니다. 그리고 그게 우리가 원하는 것이기도 합니다. 우리는 LLM이 전체 대화를 되돌아보며 다음 토큰을 예측하길 기대하고 있는 상태이며 그에 대한 사용료를 내야 하는 것입니다.\n",
    "\n",
    "실제로 LLM API를 다뤄보셨으니 체감하시겠지만, 매 요청마다 이전 대화 내역을 messages 배열에 다시 담아 보내는 구조가 바로 이 무상태성 때문입니다. 흔히 사용하는 “기억” 구현 기법들은 아래와 같습니다.\n",
    "\n",
    " * 컨텍스트 주입: 이전 대화를 messages에 누적\n",
    " * 요약/압축: 긴 대화는 요약해서 system prompt에 삽입\n",
    " * RAG: 외부 저장소에서 관련 정보 검색 후 주입\n",
    " * 메모리 DB: 사용자별 중요 정보를 별도 저장 후 필요시 주입\n",
    " \n",
    "API 요금 구조를 보면 input token과 output token을 따로 과금하는데, 대화가 길어질수록 input token이 누적되어 비용이 기하급수적으로 늘어납니다. 그래서 실무에서는 대화 요약, sliding window, 오래된 메시지 삭제 같은 전략을 쓰게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 좀 더 실용적인 예제를 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, update_display\n",
    "from typing import Generator\n",
    "\n",
    "# 대화 이력 관리 클래스\n",
    "class ChatSession:\n",
    "    \"\"\"대화 이력을 관리하는 채팅 세션 클래스\"\"\"\n",
    "\n",
    "    def __init__(self, system_prompt: str = \"\", model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.total_tokens = 0\n",
    "\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    def chat(self, user_input: str, stream: bool = False):\n",
    "        \"\"\"사용자 입력을 받아 응답을 반환합니다.\n",
    "\n",
    "        Args:\n",
    "            user_input: 사용자 입력 메시지\n",
    "            stream: True면 스트리밍 모드로 실시간 출력\n",
    "\n",
    "        Returns:\n",
    "            stream=False: 전체 응답 문자열\n",
    "            stream=True: 실시간 출력 후 전체 응답 문자열 반환\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        if stream:\n",
    "            return self._chat_stream()\n",
    "        else:\n",
    "            return self._chat_normal()\n",
    "\n",
    "    def _chat_normal(self) -> str:\n",
    "        \"\"\"일반 모드로 응답을 받습니다.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "        )\n",
    "\n",
    "        assistant_reply = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "        self.total_tokens += response.usage.total_tokens\n",
    "\n",
    "        return assistant_reply\n",
    "\n",
    "    def _chat_stream(self) -> str:\n",
    "        \"\"\"스트리밍 모드로 응답을 받아 실시간 출력합니다.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta.content or \"\"\n",
    "            full_response += delta\n",
    "            update_display(Markdown(full_response), display_id=display_handle.display_id)\n",
    "\n",
    "        # 대화 이력에 추가\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "        return full_response\n",
    "\n",
    "    def chat_generator(self, user_input: str) -> Generator[str, None, None]:\n",
    "        \"\"\"스트리밍 응답을 제너레이터로 반환합니다 (Gradio 등에서 활용).\n",
    "\n",
    "        Args:\n",
    "            user_input: 사용자 입력 메시지\n",
    "\n",
    "        Yields:\n",
    "            토큰 단위로 누적된 응답 문자열\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta.content or \"\"\n",
    "            full_response += delta\n",
    "            yield full_response\n",
    "\n",
    "        # 대화 이력에 추가\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    def show_history(self):\n",
    "        \"\"\"대화 이력을 출력합니다.\"\"\"\n",
    "        icons = {\"system\": \"⚙️\", \"user\": \"👤\", \"assistant\": \"🤖\"}\n",
    "        for msg in self.messages:\n",
    "            icon = icons.get(msg[\"role\"], \"❓\")\n",
    "            content = msg[\"content\"][:80] + \"...\" if len(msg[\"content\"]) > 80 else msg[\"content\"]\n",
    "            print(f\"{icon} [{msg['role']}]: {content}\")\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"세션 통계를 반환합니다.\"\"\"\n",
    "        return {\n",
    "            \"message_count\": len(self.messages),\n",
    "            \"total_tokens\": self.total_tokens\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 첫 번째 질문 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "리스트 컴프리헨션(List Comprehension)은 파이썬에서 리스트를 간결하고 효율적으로 생성하는 방법입니다. 이 구문을 사용하면 기존의 리스트나 반복 가능한 객체를 기반으로 새로운 리스트를 쉽게 만들 수 있습니다.\n",
       "\n",
       "기본적인 문법은 다음과 같습니다:\n",
       "\n",
       "```python\n",
       "새로운_리스트 = [표현식 for 항목 in 반복가능한_객체 if 조건]\n",
       "```\n",
       "\n",
       "여기서 각 부분의 의미는 다음과 같습니다:\n",
       "- **표현식**: 리스트의 각 항목을 변형하거나 계산할 수 있는 식입니다.\n",
       "- **항목**: 반복 가능한 객체에서 가져오는 각 요소입니다.\n",
       "- **반복가능한 객체**: 리스트, 튜플, 문자열 등의 객체입니다.\n",
       "- **조건**: (선택적) 항목을 추가할지 말지를 결정하는 조건문입니다.\n",
       "\n",
       "리스트 컴프리헨션의 예를 들어보겠습니다.\n",
       "\n",
       "1. **기본 예제**:\n",
       "   1부터 10까지의 제곱 값을 리스트로 만들고 싶다면 기존의 방법으로는 다음과 같이 할 수 있습니다:\n",
       "\n",
       "   ```python\n",
       "   squares = []\n",
       "   for i in range(1, 11):\n",
       "       squares.append(i ** 2)\n",
       "   ```\n",
       "\n",
       "   하지만 리스트 컴프리헨션을 사용하면 더 간결하게 쓸 수 있습니다:\n",
       "\n",
       "   ```python\n",
       "   squares = [i ** 2 for i in range(1, 11)]\n",
       "   ```\n",
       "\n",
       "2. **조건문이 있는 예제**:\n",
       "   1부터 10까지의 숫자 중 짝수만 제곱하여 리스트로 만들고 싶다면, 아래와 같이 할 수 있습니다:\n",
       "\n",
       "   ```python\n",
       "   even_squares = [i ** 2 for i in range(1, 11) if i % 2 == 0]\n",
       "   ```\n",
       "\n",
       "   여기서는 각 `i`가 짝수인 경우에만 제곱값이 리스트에 추가됩니다.\n",
       "\n",
       "리스트 컴프리헨션은 코드의 가독성을 높이고, 작성하는 데 필요한 줄 수를 줄여주는 매우 유용한 기능입니다. 처음에는 익숙하지 않을 수 있으나, 자주 사용하다 보면 쉽게 익힐 수 있을 거예요!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 세션 테스트\n",
    "session = ChatSession(\n",
    "    system_prompt=\"당신은 파이썬 튜터입니다. 초보자에게 친절하게 설명해주세요.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# 첫 번째 질문\n",
    "print(\"=== 첫 번째 질문 ===\")\n",
    "reply1 = session.chat(\"파이썬에서 리스트 컴프리헨션이 뭔가요?\")\n",
    "display(Markdown(reply1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 후속 질문 (맥락 유지) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "`리스트 컴프리헨션`과 `map()` 함수는 모두 데이터를 변형하고 새로운 리스트를 생성하는 데 사용됩니다. 그러나 몇 가지 중요한 차이가 있습니다. 이 두 가지를 비교해 보겠습니다.\n",
       "\n",
       "### 1. 사용 방법\n",
       "\n",
       "- **리스트 컴프리헨션**:\n",
       "  리스트 컴프리헨션은 리스트를 생성하기 위한 구문으로, 직접적으로 식을 작성하고 반복을 제어할 수 있습니다. 예를 들어:\n",
       "\n",
       "  ```python\n",
       "  squares = [x ** 2 for x in range(10)]\n",
       "  ```\n",
       "\n",
       "- **map() 함수**:\n",
       "  `map()` 함수는 특정 함수를 각 요소에 적용하여 새로운 반복 가능한 객체(map 객체)를 생성합니다. `map()`을 사용할 때는 변형할 함수를 정의해야 합니다. 예를 들어:\n",
       "\n",
       "  ```python\n",
       "  def square(x):\n",
       "      return x ** 2\n",
       "      \n",
       "  squares = list(map(square, range(10)))\n",
       "  ```\n",
       "\n",
       "### 2. 가독성\n",
       "\n",
       "- **리스트 컴프리헨션**:\n",
       "  일반적으로 가독성이 더 좋습니다. 한 줄로 내용을 표현하기 때문에, 짧은 코드에서 리스트의 내용을 쉽게 이해할 수 있습니다.\n",
       "\n",
       "- **map() 함수**:\n",
       "  함수 대신 `lambda`를 사용할 수 있지만, 복잡한 연산이 필요할 경우 가독성이 떨어질 수 있습니다. 예를 들어:\n",
       "\n",
       "  ```python\n",
       "  squares = list(map(lambda x: x ** 2, range(10)))\n",
       "  ```\n",
       "\n",
       "### 3. 조건문\n",
       "\n",
       "리스트 컴프리헨션에서는 조건문을 쉽게 사용할 수 있습니다. 예를 들어 짝수의 제곱만 포함하려면:\n",
       "\n",
       "```python\n",
       "even_squares = [x ** 2 for x in range(10) if x % 2 == 0]\n",
       "```\n",
       "\n",
       "반면, `map()` 함수에서는 조건을 처리하기 위해 추가적인 함수나 `filter()`를 사용해야 합니다. 예를 들어, 아래와 같은 처리가 필요합니다:\n",
       "\n",
       "```python\n",
       "def filter_even(x):\n",
       "    return x % 2 == 0\n",
       "\n",
       "even_squares = list(map(lambda x: x ** 2, filter(filter_even, range(10))))\n",
       "```\n",
       "\n",
       "### 4. 반환 값\n",
       "\n",
       "- **리스트 컴프리헨션**은 항상 리스트를 반환합니다.\n",
       "- **map() 함수**는 결과를 담고 있는 `map` 객체를 반환하며, 이를 리스트로 변환하려면 `list()`로 감싸야 합니다.\n",
       "\n",
       "### 요약\n",
       "\n",
       "- 리스트 컴프리헨션은 더 간편하고 가독성이 좋으며, 조건문을 쉽게 사용할 수 있는 장점이 있습니다.\n",
       "- `map()` 함수는 특정 함수를 적용하는 데 유용하지만, 상황에 따라 사용법이 다소 복잡해질 수 있습니다.\n",
       "\n",
       "어떤 방식이 더 좋을지는 상황과 코드 스타일에 따라 다를 수 있습니다. 두 가지 방법 모두 익혀두면 다양한 상황에서 유용하게 사용할 수 있으니, 편한 방법을 선택해 사용하시면 됩니다!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 후속 질문 (맥락 유지)\n",
    "print(\"=== 후속 질문 (맥락 유지) ===\")\n",
    "reply2 = session.chat(\"그거랑 map 함수랑 뭐가 다른가요?\")\n",
    "display(Markdown(reply2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 이력 및 통계\n",
    "print(\"\\n=== 대화 이력 ===\")\n",
    "session.show_history()\n",
    "\n",
    "print(f\"\\n=== 통계 ===\")\n",
    "stats = session.get_stats()\n",
    "print(f\"메시지 수: {stats['message_count']}\")\n",
    "print(f\"총 토큰: {stats['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== chat_generator 사용 예제 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. **간결하고 읽기 쉬운 문법**: 파이썬은 직관적인 문법을 통해 코드 작성과 유지보수가 용이합니다.\n",
       "\n",
       "2. **다양한 라이브러리와 프레임워크**: 데이터 분석, 웹 개발, 머신러닝 등 다양한 분야에서 사용할 수 있는 풍부한 라이브러리가 있습니다.\n",
       "\n",
       "3. **활발한 커뮤니티**: 방대한 사용자 및 개발자 커뮤니티가 있어 도움을 받기 쉽고, 자료도 풍부하게 제공됩니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chat_generator를 IPython에서 사용하는 예제\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "# 새 세션 생성\n",
    "stream_session = ChatSession(\n",
    "    system_prompt=\"당신은 친절한 AI입니다. 간결하게 답변해주세요.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# chat_generator로 스트리밍 출력\n",
    "print(\"=== chat_generator 사용 예제 ===\")\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "for partial_response in stream_session.chat_generator(\"파이썬의 장점 3가지를 알려주세요\"):\n",
    "    # partial_response는 지금까지 누적된 응답\n",
    "    update_display(Markdown(partial_response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트리밍과 Generator 패턴\n",
    "\n",
    "`ChatSession` 클래스는 스트리밍 응답을 위한 두 가지 방식을 제공합니다:\n",
    "\n",
    "| 메서드 | 반환 타입 | 사용 환경 |\n",
    "|--------|----------|----------|\n",
    "| `chat(msg, stream=True)` | `str` | Jupyter Notebook (자동 출력) |\n",
    "| `chat_generator(msg)` | `Generator` | Gradio, FastAPI 등 (직접 제어) |\n",
    "\n",
    "### yield와 Generator란?\n",
    "\n",
    "Python의 `yield` 키워드는 함수를 **제너레이터(Generator)** 로 만듭니다. 일반 함수는 `return`으로 값을 한 번에 반환하지만, 제너레이터는 `yield`로 값을 **하나씩 순차적으로** 반환합니다.\n",
    "\n",
    "```python\n",
    "# 일반 함수: 모든 값을 한 번에 반환\n",
    "def get_all():\n",
    "    return [1, 2, 3]  # 메모리에 전체 리스트 생성\n",
    "\n",
    "# 제너레이터: 값을 하나씩 반환\n",
    "def get_one_by_one():\n",
    "    yield 1  # 첫 번째 호출에서 반환\n",
    "    yield 2  # 두 번째 호출에서 반환\n",
    "    yield 3  # 세 번째 호출에서 반환\n",
    "```\n",
    "\n",
    "**스트리밍에서의 장점:**\n",
    "- 전체 응답을 기다리지 않고 토큰이 생성되는 즉시 처리 가능\n",
    "- 메모리 효율적 (전체 응답을 한번에 저장하지 않음)\n",
    "- Gradio, FastAPI 등 프레임워크와 자연스럽게 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 추론 능력 테스트\n",
    "\n",
    "논리 퍼즐로 다양한 모델의 추론 능력을 비교해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 확률 퍼즐 (GPT-4o-mini) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "동전 2개를 던졌을 때, 모든 가능한 결과는 다음과 같습니다:\n",
       "\n",
       "1. HH (앞면, 앞면)\n",
       "2. HT (앞면, 뒷면)\n",
       "3. TH (뒷면, 앞면)\n",
       "4. TT (뒷면, 뒷면)\n",
       "\n",
       "이 중에서 \"하나가 앞면\"이라는 조건을 고려할 때, TT는 제외됩니다. 그러므로 가능한 결과는 다음과 같습니다:\n",
       "\n",
       "1. HH\n",
       "2. HT\n",
       "3. TH\n",
       "\n",
       "이제 조건부 확률을 구할 준비가 되었습니다. 우리가 알고 싶은 것은 \"하나가 앞면이고 나머지 하나가 뒷면일 확률\"입니다. 가능한 경우 중에서 이 조건을 만족하는 경우를 찾겠습니다:\n",
       "\n",
       "- HT (앞면, 뒷면)\n",
       "- TH (뒷면, 앞면)\n",
       "\n",
       "HT와 TH 모두에서 하나가 앞머밴 것과 나머지가 뒷면인 경우가 포함되어 있습니다. 하지만 HH는 제외해야 합니다. \n",
       "\n",
       "따라서, \"하나가 앞면이고 나머지 하나가 뒷면일 확률\"은 다음과 같이 계산됩니다:\n",
       "\n",
       "- 총 가능한 경우: HH, HT, TH (3 상황)\n",
       "- HT와 TH의 경우 중 뒷면이 나올 확률: 2 (HT, TH)\n",
       "\n",
       "이제 확률을 계산할 수 있습니다.\n",
       "\n",
       "조건부 확률은 다음과 같이 계산합니다:\n",
       "\n",
       "\\[\n",
       "P(\\text{나머지가 뒷면} \\mid \\text{하나가 앞면}) = \\frac{\\text{조건을 만족하는 경우의 수}}{\\text{조건이 주어진 총 경우의 수}} = \\frac{2}{3}\n",
       "\\]\n",
       "\n",
       "따라서, 나머지 하나가 뒷면일 확률은 \\(\\frac{2}{3}\\)입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 확률 문제\n",
    "probability_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "     \"\"\"동전 2개를 던졌습니다. 그 중 하나가 앞면이라는 것을 알게 되었습니다.\n",
    "     나머지 하나가 뒷면일 확률은 얼마일까요?\n",
    "     \n",
    "     힌트: 이것은 조건부 확률 문제입니다. 단순히 1/2가 아닙니다.\n",
    "     단계별로 풀이해주세요.\"\"\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=probability_puzzle\n",
    ")\n",
    "\n",
    "print(\"=== 확률 퍼즐 (GPT-4o-mini) ===\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 책벌레 퍼즐 (GPT-4o-mini) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "책의 각 권은 다음과 같은 구성입니다:\n",
       "\n",
       "- 1권 앞 표지: 3mm = 0.3cm\n",
       "- 1권 본문: 3cm\n",
       "- 1권 뒷 표지: 3mm = 0.3cm\n",
       "- 2권 앞 표지: 3mm = 0.3cm\n",
       "- 2권 본문: 3cm\n",
       "- 2권 뒷 표지: 3mm = 0.3cm\n",
       "\n",
       "책이 나란히 놓여 있을 때, 책벌레가 1권의 첫 페이지에서 2권의 마지막 페이지까지 이동한 경로는 다음과 같습니다.\n",
       "\n",
       "1. 1권의 앞 표지(0.3cm) 통과\n",
       "2. 1권의 본문 (3cm) 통과\n",
       "3. 1권의 뒷 표지 (0.3cm) 통과\n",
       "4. 2권의 앞 표지 (0.3cm) 통과\n",
       "5. 2권의 본문 (3cm) 통과\n",
       "6. 2권의 뒷 표지 (0.3cm) 통과하지 않습니다, 마지막 페이지까지 통과하는 것이므로.\n",
       "\n",
       "이동 거리를 계산해보면:\n",
       "\n",
       "- 1권의 앞 표지: 0.3cm\n",
       "- 1권의 본문: 3cm\n",
       "- 1권의 뒷 표지: 0.3cm\n",
       "- 2권의 앞 표지: 0.3cm\n",
       "- 2권의 본문: 3cm\n",
       "\n",
       "따라서 이동한 총 거리는:\n",
       "\n",
       "\\[ 0.3 + 3 + 0.3 + 0.3 + 3 = 7.2 \\, \\text{cm} \\]\n",
       "\n",
       "책벌레가 이동한 거리는 **7.2 cm**입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bookworm_puzzle = [\n",
    "           {\"role\": \"user\", \"content\":\n",
    "            \"\"\"책장에 2권짜리 시리즈가 나란히 놓여 있습니다.\n",
    "            각 책의 본문 두께는 3cm이고, 앞뒤 표지는 각각 3mm입니다.\n",
    "\n",
    "            책벌레가 1권의 첫 페이지부터 2권의 마지막 페이지까지\n",
    "            수직으로 뚫고 지나갔습니다.\n",
    "\n",
    "            책벌레가 이동한 거리는 몇 cm일까요?\n",
    "\n",
    "            (힌트: 책이 책장에 어떻게 놓이는지 시각화해보세요)\"\"\"}\n",
    "       ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=bookworm_puzzle\n",
    ")\n",
    "\n",
    "print(\"=== 책벌레 퍼즐 (GPT-4o-mini) ===\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LiteLLM 통합 인터페이스\n",
    "\n",
    "LiteLLM은 100개 이상의 LLM을 단일 인터페이스로 호출할 수 있게 해주는 라이브러리입니다.\n",
    "\n",
    "### 장점\n",
    "\n",
    "- 통일된 API로 다양한 모델 접근\n",
    "- 비용 추적 기능 내장\n",
    "- Fallback/Retry 로직 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-mini: 4\n",
      "  토큰: 21, 비용: $0.000004\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "# 다양한 모델 호출\n",
    "test_message = [{\"role\": \"user\", \"content\": \"What is 2+2? Answer with just the number.\"}]\n",
    "\n",
    "# OpenAI\n",
    "response = completion(model=\"openai/gpt-4o-mini\", messages=test_message)\n",
    "print(f\"GPT-4o-mini: {response.choices[0].message.content}\")\n",
    "print(f\"  토큰: {response.usage.total_tokens}, 비용: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip-system-certs\n",
      "  Downloading pip_system_certs-5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pip>=24.2 in /Users/windfree/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages (from pip-system-certs) (24.3.1)\n",
      "Downloading pip_system_certs-5.3-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: pip-system-certs\n",
      "Successfully installed pip-system-certs-5.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pip-system-certs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 예제에서 SSL 오류가 나는 경우  \n",
    "* pip install pip-system-certs 을 실행해 줍니다. \n",
    "* pip 모듈이 없다고 나오는 경우에는 터미널에서 .venv/bin/python -m ensurepip --upgrade 를 실행하고 커널을 재실행해주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude Sonnet: 4\n",
      "  토큰: 25, 비용: $0.000135\n"
     ]
    }
   ],
   "source": [
    "# Anthropic (LiteLLM 통해)\n",
    "response = completion(model=\"anthropic/claude-sonnet-4-20250514\", messages=test_message)\n",
    "print(f\"Claude Sonnet: {response.choices[0].message.content}\")\n",
    "print(f\"  토큰: {response.usage.total_tokens}, 비용: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: litellm.RateLimitError: geminiException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 33.766755354s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\",\n            \"quotaId\": \"GenerateContentInputTokensPerModelPerMinute-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"33s\"\n      }\n    ]\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2599\u001b[39m, in \u001b[36mVertexLLM.completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[39m\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2599\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2600\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:979\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    978\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:961\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    960\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBivm0nktmWR-3dJQeT58c2GdpkikN0-1E'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mVertexAIError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/main.py:3113\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   3112\u001b[39m     new_params = safe_deep_copy(optional_params \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m     response = \u001b[43mvertex_chat_completion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_ai_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3135\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mvertex_ai\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2603\u001b[39m, in \u001b[36mVertexLLM.completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[39m\n\u001b[32m   2602\u001b[39m     error_code = err.response.status_code\n\u001b[32m-> \u001b[39m\u001b[32m2603\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[32m   2604\u001b[39m         status_code=error_code,\n\u001b[32m   2605\u001b[39m         message=err.response.text,\n\u001b[32m   2606\u001b[39m         headers=err.response.headers,\n\u001b[32m   2607\u001b[39m     )\n\u001b[32m   2608\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException:\n",
      "\u001b[31mVertexAIError\u001b[39m: {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 33.766755354s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\",\n            \"quotaId\": \"GenerateContentInputTokensPerModelPerMinute-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"33s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Gemini (LiteLLM 통해)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini/gemini-2.0-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_message\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGemini 2.0 Flash: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  토큰: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.usage.total_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 비용: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse._hidden_params.get(\u001b[33m'\u001b[39m\u001b[33mresponse_cost\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/utils.py:1405\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1402\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1403\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1404\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/utils.py:1274\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1272\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1273\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1275\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1277\u001b[39m     kwargs=kwargs,\n\u001b[32m   1278\u001b[39m     call_type=call_type,\n\u001b[32m   1279\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/main.py:4080\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   4077\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   4078\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   4079\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4080\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4081\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4082\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4083\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4084\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4085\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4086\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2340\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2339\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2342\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1332\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1324\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m429 Quota exceeded\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1325\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mQuota exceeded for\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   (...)\u001b[39m\u001b[32m   1329\u001b[39m     \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1330\u001b[39m ):\n\u001b[32m   1331\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m   1333\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlitellm.RateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1334\u001b[39m         model=model,\n\u001b[32m   1335\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   1336\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   1337\u001b[39m         response=httpx.Response(\n\u001b[32m   1338\u001b[39m             status_code=\u001b[32m429\u001b[39m,\n\u001b[32m   1339\u001b[39m             request=httpx.Request(\n\u001b[32m   1340\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1341\u001b[39m                 url=\u001b[33m\"\u001b[39m\u001b[33m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1342\u001b[39m             ),\n\u001b[32m   1343\u001b[39m         ),\n\u001b[32m   1344\u001b[39m     )\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1346\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m500 Internal Server Error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1347\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe model is overloaded.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1348\u001b[39m ):\n\u001b[32m   1349\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: litellm.RateLimitError: geminiException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 33.766755354s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\",\n            \"quotaId\": \"GenerateContentInputTokensPerModelPerMinute-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"33s\"\n      }\n    ]\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "# Gemini (LiteLLM 통해)\n",
    "response = completion(model=\"gemini/gemini-2.0-flash\", messages=test_message)\n",
    "print(f\"Gemini 2.0 Flash: {response.choices[0].message.content}\")\n",
    "print(f\"  토큰: {response.usage.total_tokens}, 비용: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 프롬프트 캐싱\n",
    "\n",
    "긴 프롬프트를 반복 사용할 때 비용을 절감할 수 있는 기법입니다.\n",
    "\n",
    "### Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/\n",
    "\n",
    "\n",
    "### Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api\n",
    "\n",
    "### Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python\n",
    "\n",
    "\n",
    "아래 예제에서는 셰익스피어의 햄릿 전문(약 4만 토큰)을 사용하여 실제 캐싱 효과를 확인합니다. 잘 되나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "햄릿 텍스트 길이: 191,726 문자\n",
      "=== 첫 번째 호출 (캐시 프라이밍) ===\n",
      "입력 토큰: 49,703\n",
      "캐시된 토큰: 0\n",
      "\n",
      "응답: 햄릿의 유명한 독백 \"To be, or not to be\"는 3막 1장에 등장합니다.\n"
     ]
    }
   ],
   "source": [
    "# 햄릿 전문 로드 (약 4만 토큰)\n",
    "with open(\"../../hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet_text = f.read()\n",
    "\n",
    "print(f\"햄릿 텍스트 길이: {len(hamlet_text):,} 문자\")\n",
    "\n",
    "# 첫 번째 호출 (캐시 프라이밍)\n",
    "messages1 = [{\"role\": \"user\", \"content\": f\"\"\"다음은 셰익스피어의 햄릿 전문입니다:\n",
    "\n",
    "{hamlet_text}\n",
    "\n",
    "질문: 햄릿의 유명한 독백 \"To be, or not to be\"는 몇 막 몇 장에 등장하나요?\"\"\"}]\n",
    "\n",
    "response1 = completion(model=\"openai/gpt-4o-mini\", messages=messages1)\n",
    "\n",
    "print(\"=== 첫 번째 호출 (캐시 프라이밍) ===\")\n",
    "print(f\"입력 토큰: {response1.usage.prompt_tokens:,}\")\n",
    "if hasattr(response1.usage, 'prompt_tokens_details') and response1.usage.prompt_tokens_details:\n",
    "    cached = getattr(response1.usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "    print(f\"캐시된 토큰: {cached:,}\")\n",
    "print(f\"\\n응답: {response1.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 두 번째 호출 (캐시 히트) ===\n",
      "입력 토큰: 49,685\n",
      "캐시된 토큰: 49,536\n",
      "캐시 히트율: 99.7%\n",
      "💰 캐시된 토큰은 할인 적용!\n",
      "\n",
      "응답: 오필리아는 \"햄릿\"에서 물에 빠져 죽은 것으로 묘사됩니다. 그녀는 괴로움과 슬픔에 압도되어 감정적으로 불안정한 상태에 있었고, 이는 결국 그녀의 죽음으로 이어집니다. 그녀가 물에 빠진 장소는 '버드나무가 시냇물 위로 기울어지는 곳'이라고 묘사되며, 그녀는 물속에서 꽃다발을 만들고 노래를 부르다가 갑자기 빠지게 됩니다. 어머니인 여왕이 그녀의 죽음을 듣고 슬퍼하는 장면이 등장합니다. 오필리아는 자신의 아버지인 폴로니우스를 잃은 슬픔과 삶의 압박감에 시달린 결과로, 비극적인 죽음을 맞이하게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 두 번째 호출 (캐시 히트 기대)\n",
    "messages2 = [{\"role\": \"user\", \"content\": f\"\"\"다음은 셰익스피어의 햄릿 전문입니다:\n",
    "\n",
    "{hamlet_text}\n",
    "\n",
    "질문: 오필리아는 어떻게 죽었나요?\"\"\"}]\n",
    "\n",
    "response2 = completion(model=\"openai/gpt-4o-mini\", messages=messages2)\n",
    "\n",
    "print(\"=== 두 번째 호출 (캐시 히트) ===\")\n",
    "print(f\"입력 토큰: {response2.usage.prompt_tokens:,}\")\n",
    "\n",
    "# 캐시 정보 확인\n",
    "if hasattr(response2.usage, 'prompt_tokens_details') and response2.usage.prompt_tokens_details:\n",
    "    cached = getattr(response2.usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "    print(f\"캐시된 토큰: {cached:,}\")\n",
    "    if cached > 0:\n",
    "        cache_ratio = cached / response2.usage.prompt_tokens * 100\n",
    "        print(f\"캐시 히트율: {cache_ratio:.1f}%\")\n",
    "        print(f\"💰 캐시된 토큰은 할인 적용!\")\n",
    "\n",
    "print(f\"\\n응답: {response2.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 다중 에이전트 시스템\n",
    "\n",
    "서로 다른 성격의 AI 에이전트들이 대화하는 시스템을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트 정의\n",
    "AGENTS = {\n",
    "    \"optimist\": {\n",
    "        \"name\": \"희망이\",\n",
    "        \"emoji\": \"😊\",\n",
    "        \"system\": \"\"\"당신은 '희망이'입니다. 매우 긍정적이고 낙관적인 성격입니다.\n",
    "        모든 상황에서 좋은 면을 찾으려 하고, 다른 사람들을 격려합니다.\n",
    "        답변은 2-3문장으로 짧게 해주세요.\"\"\"\n",
    "    },\n",
    "    \"skeptic\": {\n",
    "        \"name\": \"의심이\",\n",
    "        \"emoji\": \"🤨\",\n",
    "        \"system\": \"\"\"당신은 '의심이'입니다. 비판적 사고를 중시하는 회의론자입니다.\n",
    "        주장에 대해 근거를 요구하고, 논리적 허점을 지적합니다. 하지만 공격적이지는 않습니다.\n",
    "        답변은 2-3문장으로 짧게 해주세요.\"\"\"\n",
    "    },\n",
    "    \"mediator\": {\n",
    "        \"name\": \"중재자\",\n",
    "        \"emoji\": \"🤝\",\n",
    "        \"system\": \"\"\"당신은 '중재자'입니다. 서로 다른 의견 사이에서 균형을 찾습니다.\n",
    "        양쪽의 장점을 인정하고, 건설적인 결론을 도출하려 합니다.\n",
    "        답변은 2-3문장으로 짧게 해주세요.\"\"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_response(agent_key: str, conversation: str, topic: str) -> str:\n",
    "    \"\"\"특정 에이전트의 응답을 생성합니다.\"\"\"\n",
    "    agent = AGENTS[agent_key]\n",
    "    \n",
    "    user_prompt = f\"\"\"현재 토론 주제: {topic}\n",
    "\n",
    "지금까지의 대화:\n",
    "{conversation}\n",
    "\n",
    "당신({agent['name']})의 차례입니다. 위 대화에 이어서 의견을 말씀해주세요.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": agent[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_discussion(topic: str, rounds: int = 4):\n",
    "    \"\"\"다중 에이전트 토론을 실행합니다.\"\"\"\n",
    "    conversation = \"[토론 시작]\\n\"\n",
    "    agent_order = [\"optimist\", \"skeptic\", \"mediator\"]\n",
    "    \n",
    "    print(f\"📢 토론 주제: {topic}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for round_num in range(rounds):\n",
    "        print(f\"\\n--- 라운드 {round_num + 1} ---\")\n",
    "        \n",
    "        for agent_key in agent_order:\n",
    "            agent = AGENTS[agent_key]\n",
    "            response = get_agent_response(agent_key, conversation, topic)\n",
    "            \n",
    "            conversation += f\"\\n{agent['name']}: {response}\"\n",
    "            print(f\"\\n{agent['emoji']} {agent['name']}: {response}\")\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📢 토론 주제: AI가 인간의 창의성을 대체할 수 있을까?\n",
      "==================================================\n",
      "\n",
      "--- 라운드 1 ---\n",
      "\n",
      "😊 희망이: AI는 정말 멋진 도구로, 인간의 창의성을 보완하고 영감을 줄 수 있어요! 우리는 AI와 함께 협력함으로써 더 놀라운 아이디어를 만들어낼 수 있습니다. 창의성의 본질은 인간의 독창성과 감정에서 나오니, 걱정할 필요 없어요!\n",
      "\n",
      "🤨 의심이: 희망이님의 주장에는 AI가 인간의 창의성을 \"보완\"할 수 있다는 점이 언급되었지만, AI가 어떻게 구체적으로 이 과정을 지원할 수 있는지에 대한 구체적인 예가 필요합니다. 또한, AI가 창의성을 대체할 가능성에 대한 우려를 간과하고 있는 것 같습니다. AI가 인간의 독창성과 감정을 이해할 수 있다는 보장이 있나요?\n",
      "\n",
      "🤝 중재자: 희망이님은 AI가 창의성을 보완할 수 있는 가능성을 잘 강조하셨고, 이는 인간과 AI의 협업을 통해 새로운 아이디어를 창출할 수 있음을 시사합니다. 반면, 의심님은 AI의 한계와 대체 가능성에 대한 우려를 제기하며 더욱 구체적인 논의가 필요하다는 점을 강조하셨습니다. 두 의견 모두 중요한 시각을 제공하므로, AI의 역할과 한계를 명확하게 이해하는 것이 창의성의 미래를 논의하는 데 필요합니다.\n",
      "\n",
      "--- 라운드 2 ---\n",
      "\n",
      "😊 희망이: 희망이: 맞아요, 구체적인 예가 필요해요! 예를 들어, AI는 디자인이나 음악 작곡에서 다양한 스타일을 제안해줘서 인간이 그 아이디어를 바탕으로 더 깊이 있는 창작을 할 수 있게 도와줄 수 있어요. 우리는 협력하여 서로의 강점을 살릴 수 있는 미래를 만들어갈 수 있습니다!\n",
      "\n",
      "🤨 의심이: 희망이님의 예시에서 AI가 디자인이나 음악 작곡에 기여할 수 있다고 언급하셨지만, AI가 제안하는 스타일이나 아이디어가 정말로 혁신적이고 독창적인지에 대한 의문이 남습니다. AI는 기존 데이터를 기반으로 작동하므로, 새로운 창의적 발상을 어떻게 만들어낼 수 있는지 구체적인 메커니즘이 필요합니다. 그리고 인간의 감정과 경험을 대체할 수 있는지에 대한 논의도 여전히 필요합니다.\n",
      "\n",
      "🤝 중재자: 희망이님은 AI가 인간의 창의성을 보완하는 구체적인 예를 제시하며 협업의 가능성을 강조하셨습니다. 의심님은 AI의 혁신성과 독창성에 대한 우려를 제기하며 더 깊이 있는 논의의 필요성을 언급하셨습니다. 두 의견을 종합하면, AI의 기여를 이해하기 위해서는 실제 사례와 더불어 AI의 한계와 이를 극복할 방안에 대한 심도 있는 논의가 필요합니다.\n",
      "\n",
      "--- 라운드 3 ---\n",
      "\n",
      "😊 희망이: 희망이: 의심이님, 좋은 지적이에요! AI가 기존 데이터를 기반으로 하지만, 그 데이터를 활용해 새로운 조합과 시너지를 창출할 수 있습니다. 또한, 인간의 감정과 경험은 AI가 완전히 이해하지 못하더라도, 우리는 서로의 다름을 통해 더욱 풍부한 창작을 할 수 있어요. 결국 협력의 힘이 정말 중요하답니다!\n",
      "\n",
      "🤨 의심이: 희망이님의 주장은 AI가 기존 데이터를 활용해 새로운 조합을 만든다고 하셨지만, 이러한 조합이 과연 '창의적'이라고 할 수 있는지에 대한 명확한 기준이 필요합니다. 또한, 협력의 힘이 중요하다는 점은 동의하지만, 인간의 감정과 경험을 완전히 이해하지 못하는 AI와의 협력이 실제로 얼마나 효과적일지에 대한 구체적인 논의가 부족합니다.\n",
      "\n",
      "🤝 중재자: 희망이님은 AI가 기존 데이터를 활용해 새로운 조합을 만들어낼 수 있는 가능성과 협력의 중요성을 강조하셨습니다. 의심님은 이러한 조합이 진정한 창의성으로 인정받을 수 있는 기준과 AI와의 협력의 효과성에 대한 논의가 필요하다고 지적하셨습니다. 두 의견을 종합하면, AI의 창의적 기여를 평가하기 위한 명확한 기준 설정과 인간의 감정을 이해하는 방법에 대한 깊이 있는 논의가 필요할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# 토론 실행\n",
    "topic = \"AI가 인간의 창의성을 대체할 수 있을까?\"\n",
    "final_conversation = run_discussion(topic, rounds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. LangChain 맛보기\n",
    "\n",
    "LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChain을 통한 모델 호출\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"머신러닝과 딥러닝의 차이를 한 문장으로 설명해주세요.\"}\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"=== LangChain을 통한 GPT-4o-mini ===\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m prompt = ChatPromptTemplate.from_messages([\n\u001b[32m      6\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m당신은 \u001b[39m\u001b[38;5;132;01m{topic}\u001b[39;00m\u001b[33m 전문가입니다. 초보자에게 친절하게 설명해주세요.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m ])\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 체인 구성\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m chain = prompt | \u001b[43mllm\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 체인 실행\u001b[39;00m\n\u001b[32m     14\u001b[39m response = chain.invoke({\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPython\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m데코레이터가 뭔가요?\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[31mNameError\u001b[39m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "# LangChain 체인 예시\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 프롬프트 템플릿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 {topic} 전문가입니다. 초보자에게 친절하게 설명해주세요.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 체인 구성\n",
    "chain = prompt | llm\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.invoke({\"topic\": \"Python\", \"question\": \"데코레이터가 뭔가요?\"})\n",
    "print(\"=== LangChain 체인 ===\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 로컬 LLM (Ollama) 심화\n",
    "\n",
    "Ollama로 로컬에서 다양한 오픈소스 모델을 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama 서버가 실행 중입니다.\n",
      "\n",
      "📦 설치된 모델 (3개):\n",
      "   - exaone3.5:latest (4.4GB)\n",
      "   - llama3.2:latest (1.9GB)\n",
      "   - gpt-oss:latest (12.8GB)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Ollama 서버 상태 확인\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/\", timeout=5)\n",
    "    print(\"✅ Ollama 서버가 실행 중입니다.\")\n",
    "    \n",
    "    # 설치된 모델 목록\n",
    "    tags_response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    if tags_response.status_code == 200:\n",
    "        models = tags_response.json().get(\"models\", [])\n",
    "        print(f\"\\n📦 설치된 모델 ({len(models)}개):\")\n",
    "        for model in models[:5]:  # 상위 5개만 표시\n",
    "            size_gb = model.get(\"size\", 0) / (1024**3)\n",
    "            print(f\"   - {model['name']} ({size_gb:.1f}GB)\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Ollama 서버가 실행되지 않았습니다.\")\n",
    "    print(\"   터미널에서 'ollama serve' 명령을 실행하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama (Llama 3.2) ===\n",
      "Python is a high-level programming language known for its readability and versatility, widely used for web development, data analysis, artificial intelligence, and more.\n"
     ]
    }
   ],
   "source": [
    "# Ollama 모델 호출 (OpenAI 호환 인터페이스)\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = ollama_client.chat.completions.create(\n",
    "        model=\"exaone3.5\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is Python? One sentence.\"}]\n",
    "    )\n",
    "    print(\"=== Ollama (Llama 3.2) ===\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 실습: 3개 LLM 토론\n",
    "\n",
    "OpenAI, Claude, Ollama 세 가지 LLM이 토론하는 시스템을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "# 클라이언트 초기화\n",
    "openai_client = OpenAI()\n",
    "claude_client = anthropic.Anthropic()\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# 시스템 프롬프트\n",
    "PROMPTS = {\n",
    "    \"openai\": \"You are OpenAI's representative. You tend to be optimistic about AI. Keep responses to 2-3 sentences.\",\n",
    "    \"claude\": \"You are Anthropic's representative. You emphasize AI safety. Keep responses to 2-3 sentences.\",\n",
    "    \"ollama\": \"You are an open-source advocate. You value transparency. Keep responses to 2-3 sentences.\"\n",
    "}\n",
    "\n",
    "def get_openai_response(conversation: str, topic: str) -> str:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": PROMPTS[\"openai\"]},\n",
    "            {\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_claude_response(conversation: str, topic: str) -> str:\n",
    "    response = claude_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=200,\n",
    "        system=PROMPTS[\"claude\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def get_ollama_response(conversation: str, topic: str) -> str:\n",
    "    try:\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=\"llama3.2\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": PROMPTS[\"ollama\"]},\n",
    "                {\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        return \"(Ollama not available)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개 LLM 토론 실행\n",
    "topic = \"The future of open-source AI models\"\n",
    "conversation = \"\"\n",
    "\n",
    "print(f\"📢 Topic: {topic}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for round_num in range(2):\n",
    "    print(f\"\\n--- Round {round_num + 1} ---\")\n",
    "    \n",
    "    # OpenAI\n",
    "    openai_reply = get_openai_response(conversation, topic)\n",
    "    conversation += f\"\\nOpenAI: {openai_reply}\"\n",
    "    print(f\"\\n🟢 OpenAI: {openai_reply}\")\n",
    "    \n",
    "    # Claude\n",
    "    claude_reply = get_claude_response(conversation, topic)\n",
    "    conversation += f\"\\nClaude: {claude_reply}\"\n",
    "    print(f\"\\n🟠 Claude: {claude_reply}\")\n",
    "    \n",
    "    # Ollama\n",
    "    ollama_reply = get_ollama_response(conversation, topic)\n",
    "    conversation += f\"\\nOllama: {ollama_reply}\"\n",
    "    print(f\"\\n🔵 Ollama: {ollama_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 요약 및 다음 단계\n",
    "\n",
    "### 이번 시리즈에서 학습한 내용\n",
    "\n",
    "| Part | 주요 내용 |\n",
    "|------|----------|\n",
    "| **Part 1** | API 소개, 환경설정, 메시지 구조, 기본 호출, 활용 예시 |\n",
    "| **Part 2** | 파라미터, 스트리밍, 에러처리, 다중 LLM, 비용 계산 |\n",
    "| **Part 3** | 대화 이력, 캐싱, LiteLLM, 다중 에이전트, LangChain |\n",
    "\n",
    "### 다음 단계로 배울 내용\n",
    "\n",
    "| 주제 | 설명 |\n",
    "|------|------|\n",
    "| **Function Calling** | LLM이 외부 도구/API를 호출하는 방법 |\n",
    "| **RAG** | 검색 증강 생성으로 최신 정보 활용 |\n",
    "| **Agent** | 자율적으로 작업을 수행하는 AI 에이전트 |\n",
    "| **Fine-tuning** | 특정 도메인에 맞게 모델 미세 조정 |\n",
    "| **Prompt Engineering** | 더 효과적인 프롬프트 작성 기법 |\n",
    "\n",
    "### 연습 문제\n",
    "\n",
    "1. `ChatSession` 클래스에 토큰 사용량 추적 및 비용 계산 기능을 추가해보세요.\n",
    "2. 다중 에이전트 토론에 4번째 에이전트(팩트 체커)를 추가해보세요.\n",
    "3. LiteLLM을 사용하여 여러 모델의 응답 시간과 비용을 비교하는 벤치마크를 작성해보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
