{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API ê³ ê¸‰ (Part 3/3)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ LLM API ì‹œë¦¬ì¦ˆì˜ ë§ˆì§€ë§‰ íŒŒíŠ¸ë¡œ, í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "| ëª©í‘œ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| ëŒ€í™” ì´ë ¥ ê´€ë¦¬ | ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ êµ¬í˜„ |\n",
    "| ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ | ë…¼ë¦¬ í¼ì¦ë¡œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ |\n",
    "| í”„ë¡¬í”„íŠ¸ ìºì‹± | ë¹„ìš© ì ˆê° ê¸°ë²• |\n",
    "| LiteLLM | 100+ LLM í†µí•© ì¸í„°í˜ì´ìŠ¤ |\n",
    "| ë‹¤ì¤‘ ì—ì´ì „íŠ¸ | ì—¬ëŸ¬ AIê°€ í˜‘ì—…í•˜ëŠ” ì‹œìŠ¤í…œ |\n",
    "| LangChain | LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ |\n",
    "\n",
    "## ì‹œë¦¬ì¦ˆ êµ¬ì„±\n",
    "\n",
    "- **Part 1**: LLM API ê¸°ì´ˆ - í™˜ê²½ì„¤ì •, ë©”ì‹œì§€ êµ¬ì¡°, ê¸°ë³¸ í˜¸ì¶œ\n",
    "- **Part 2**: LLM API ì¤‘ê¸‰ - íŒŒë¼ë¯¸í„°, ìŠ¤íŠ¸ë¦¬ë°, ì—ëŸ¬ì²˜ë¦¬, ë‹¤ì¤‘ LLM\n",
    "- **Part 3 (í˜„ì¬)**: LLM API ê³ ê¸‰ - ëŒ€í™” ì´ë ¥, ìºì‹±, ì—ì´ì „íŠ¸, í”„ë ˆì„ì›Œí¬\n",
    "\n",
    "## ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "\n",
    "- Part 1, 2 ì™„ë£Œ\n",
    "- OpenAI API í‚¤ (í•„ìˆ˜)\n",
    "- Anthropic, Google API í‚¤ (ì„ íƒ)\n",
    "- `litellm`, `langchain-openai` ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\n",
    "# !pip install litellm langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ëŒ€í™” ì´ë ¥ ê´€ë¦¬\n",
    "\n",
    "LLM APIëŠ” ìƒíƒœë¥¼ ìœ ì§€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë ¤ë©´ ì´ì „ ë©”ì‹œì§€ë“¤ì„ í•¨ê»˜ ì „ì†¡í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ ê°œë…\n",
    "\n",
    "```\n",
    "ìš”ì²­ 1: [system, user1] â†’ assistant1\n",
    "ìš”ì²­ 2: [system, user1, assistant1, user2] â†’ assistant2\n",
    "ìš”ì²­ 3: [system, user1, assistant1, user2, assistant2, user3] â†’ assistant3\n",
    "```\n",
    "\n",
    "ê°„ë‹¨í•œ ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ë©´ ì¬ë¯¸ìˆëŠ” í˜„ìƒì„ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Windfree! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, My name is windfree.\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì²«ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë‚´ ì´ë¦„ì„ ë§í•´ì¤€ í›„ì— ë‘ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë‚´ ì´ë¦„ì„ ë¬¼ì–´ë³´ì•˜ì„ ë•Œ LLM ì€ ë‚´ ì´ë¦„ì„ ëª¨ë¥¸ë‹¤ëŠ” ë‹µì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ìœ ê°€ ë­˜ê¹Œìš”? LLM ì— ëŒ€í•œ ëª¨ë“  í˜¸ì¶œì€ ì™„ì „íˆ Stateless í•œ ìƒíƒœì…ë‹ˆë‹¤. ë§¤ë²ˆ ì™„ì „íˆ ìƒˆë¡œìš´ í˜¸ì¶œì¸ ì…ˆì´ì£ . LLM ì´ â€œê¸°ì–µâ€ ì„ ê°€ì§„ ê²ƒì²˜ëŸ¼ ë§Œë“œëŠ” ê²ƒì€ AI ê°œë°œìì˜ ëª«ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Windfree.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, My name is windfree.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, Windfree! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}]\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¹ì—°í•œ ì–˜ê¸°ì¼ ìˆ˜ ìˆì§€ë§Œ,  ì •ë¦¬í•´ë³´ë©´:\n",
    "\n",
    " * LLMì— ëŒ€í•œ ëª¨ë“  í˜¸ì¶œì€ ë¬´ìƒíƒœ(stateless)ë‹¤.\n",
    " * ë§¤ë²ˆ ì§€ê¸ˆê¹Œì§€ì˜ ì „ì²´ ëŒ€í™”ë¥¼ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ë‹´ì•„ ì „ë‹¬í•œë‹¤.\n",
    " * ì´ê²Œ LLMì´ ê¸°ì–µì„ ê°€ì§„ ê²ƒ ê°™ì€ ì°©ê°ì„ ë§Œë“ ë‹¤ â€” ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ì§€ë§Œ ì´ê±´ íŠ¸ë¦­ì´ë‹¤.\n",
    " * ë§¤ë²ˆ ì „ì²´ ëŒ€í™”ë¥¼ ì œê³µí•œ ê²°ê³¼ì¼ ë¿ LLMì€ ê·¸ì € ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒì— ì˜¬ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í† í°ì„ ì˜ˆì¸¡í•  ë¿ì´ë‹¤.\n",
    " * ì‹œí€€ìŠ¤ì— â€œë‚´ ì´ë¦„ì€ windfreeì•¼â€ê°€ ìˆê³  ë‚˜ì¤‘ì— â€œë‚´ ì´ë¦„ì´ ë­ì§€?â€ë¼ê³  ë¬¼ìœ¼ë©´â€¦ windfreeë¼ê³  ì˜ˆì¸¡í•˜ëŠ” ê²ƒ!\n",
    "\n",
    "ë§ì€ ì œí’ˆë“¤ì´ ì •í™•íˆ ì´ íŠ¸ë¦­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•Œë§ˆë‹¤ ì „ì²´ ëŒ€í™”ê°€ í•¨ê»˜ ì „ë‹¬ë˜ëŠ” ê²ë‹ˆë‹¤. â€œê·¸ëŸ¬ë©´ ë§¤ë²ˆ ì´ì „ ëŒ€í™” ì „ì²´ì— ëŒ€í•´ ì¶”ê°€ ë¹„ìš©ì„ ë‚´ì•¼ í•˜ëŠ” ê±´ê°€ìš”?â€ ë„¤. ë‹¹ì—°íˆ ê·¸ë ‡ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ê·¸ê²Œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì´ê¸°ë„ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMì´ ì „ì²´ ëŒ€í™”ë¥¼ ë˜ëŒì•„ë³´ë©° ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ê¸¸ ê¸°ëŒ€í•˜ê³  ìˆëŠ” ìƒíƒœì´ë©° ê·¸ì— ëŒ€í•œ ì‚¬ìš©ë£Œë¥¼ ë‚´ì•¼ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œë¡œ LLM APIë¥¼ ë‹¤ë¤„ë³´ì…¨ìœ¼ë‹ˆ ì²´ê°í•˜ì‹œê² ì§€ë§Œ, ë§¤ ìš”ì²­ë§ˆë‹¤ ì´ì „ ëŒ€í™” ë‚´ì—­ì„ messages ë°°ì—´ì— ë‹¤ì‹œ ë‹´ì•„ ë³´ë‚´ëŠ” êµ¬ì¡°ê°€ ë°”ë¡œ ì´ ë¬´ìƒíƒœì„± ë•Œë¬¸ì…ë‹ˆë‹¤. í”íˆ ì‚¬ìš©í•˜ëŠ” â€œê¸°ì–µâ€ êµ¬í˜„ ê¸°ë²•ë“¤ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    " * ì»¨í…ìŠ¤íŠ¸ ì£¼ì…: ì´ì „ ëŒ€í™”ë¥¼ messagesì— ëˆ„ì \n",
    " * ìš”ì•½/ì••ì¶•: ê¸´ ëŒ€í™”ëŠ” ìš”ì•½í•´ì„œ system promptì— ì‚½ì…\n",
    " * RAG: ì™¸ë¶€ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ í›„ ì£¼ì…\n",
    " * ë©”ëª¨ë¦¬ DB: ì‚¬ìš©ìë³„ ì¤‘ìš” ì •ë³´ë¥¼ ë³„ë„ ì €ì¥ í›„ í•„ìš”ì‹œ ì£¼ì…\n",
    " \n",
    "API ìš”ê¸ˆ êµ¬ì¡°ë¥¼ ë³´ë©´ input tokenê³¼ output tokenì„ ë”°ë¡œ ê³¼ê¸ˆí•˜ëŠ”ë°, ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ input tokenì´ ëˆ„ì ë˜ì–´ ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì‹¤ë¬´ì—ì„œëŠ” ëŒ€í™” ìš”ì•½, sliding window, ì˜¤ë˜ëœ ë©”ì‹œì§€ ì‚­ì œ ê°™ì€ ì „ëµì„ ì“°ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì¢€ ë” ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, update_display\n",
    "from typing import Generator\n",
    "\n",
    "# ëŒ€í™” ì´ë ¥ ê´€ë¦¬ í´ë˜ìŠ¤\n",
    "class ChatSession:\n",
    "    \"\"\"ëŒ€í™” ì´ë ¥ì„ ê´€ë¦¬í•˜ëŠ” ì±„íŒ… ì„¸ì…˜ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self, system_prompt: str = \"\", model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        self.total_tokens = 0\n",
    "\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    def chat(self, user_input: str, stream: bool = False):\n",
    "        \"\"\"ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            user_input: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€\n",
    "            stream: Trueë©´ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤ì‹œê°„ ì¶œë ¥\n",
    "\n",
    "        Returns:\n",
    "            stream=False: ì „ì²´ ì‘ë‹µ ë¬¸ìì—´\n",
    "            stream=True: ì‹¤ì‹œê°„ ì¶œë ¥ í›„ ì „ì²´ ì‘ë‹µ ë¬¸ìì—´ ë°˜í™˜\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        if stream:\n",
    "            return self._chat_stream()\n",
    "        else:\n",
    "            return self._chat_normal()\n",
    "\n",
    "    def _chat_normal(self) -> str:\n",
    "        \"\"\"ì¼ë°˜ ëª¨ë“œë¡œ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "        )\n",
    "\n",
    "        assistant_reply = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "        self.total_tokens += response.usage.total_tokens\n",
    "\n",
    "        return assistant_reply\n",
    "\n",
    "    def _chat_stream(self) -> str:\n",
    "        \"\"\"ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‘ë‹µì„ ë°›ì•„ ì‹¤ì‹œê°„ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta.content or \"\"\n",
    "            full_response += delta\n",
    "            update_display(Markdown(full_response), display_id=display_handle.display_id)\n",
    "\n",
    "        # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "        return full_response\n",
    "\n",
    "    def chat_generator(self, user_input: str) -> Generator[str, None, None]:\n",
    "        \"\"\"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤ (Gradio ë“±ì—ì„œ í™œìš©).\n",
    "\n",
    "        Args:\n",
    "            user_input: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€\n",
    "\n",
    "        Yields:\n",
    "            í† í° ë‹¨ìœ„ë¡œ ëˆ„ì ëœ ì‘ë‹µ ë¬¸ìì—´\n",
    "        \"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta.content or \"\"\n",
    "            full_response += delta\n",
    "            yield full_response\n",
    "\n",
    "        # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    def show_history(self):\n",
    "        \"\"\"ëŒ€í™” ì´ë ¥ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "        icons = {\"system\": \"âš™ï¸\", \"user\": \"ğŸ‘¤\", \"assistant\": \"ğŸ¤–\"}\n",
    "        for msg in self.messages:\n",
    "            icon = icons.get(msg[\"role\"], \"â“\")\n",
    "            content = msg[\"content\"][:80] + \"...\" if len(msg[\"content\"]) > 80 else msg[\"content\"]\n",
    "            print(f\"{icon} [{msg['role']}]: {content}\")\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"ì„¸ì…˜ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        return {\n",
    "            \"message_count\": len(self.messages),\n",
    "            \"total_tokens\": self.total_tokens\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì²« ë²ˆì§¸ ì§ˆë¬¸ ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜(List Comprehension)ì€ íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê°„ê²°í•˜ê³  íš¨ìœ¨ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ ë•Œ ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë°, ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì‚¬ìš©í•˜ë©´ í•œ ì¤„ë¡œ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì˜ ê¸°ë³¸ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
       "\n",
       "```python\n",
       "[expression for item in iterable if condition]\n",
       "```\n",
       "\n",
       "- **expression**: ë¦¬ìŠ¤íŠ¸ì˜ ê° ìš”ì†Œì— ëŒ€í•´ ì ìš©í•  í‘œí˜„ì‹ì…ë‹ˆë‹¤.\n",
       "- **item**: iterableì˜ ê° ìš”ì†Œë¥¼ ìˆœíšŒí•  ë•Œì˜ ë³€ìˆ˜ì…ë‹ˆë‹¤.\n",
       "- **iterable**: ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´(ë¦¬ìŠ¤íŠ¸, íŠœí”Œ, ë¬¸ìì—´ ë“±)ì…ë‹ˆë‹¤.\n",
       "- **condition**: (ì„ íƒ ì‚¬í•­) ê° ìš”ì†Œë¥¼ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ì‹œí‚¬ì§€ ê²°ì •í•˜ëŠ” ì¡°ê±´ì…ë‹ˆë‹¤.\n",
       "\n",
       "### ì˜ˆì‹œ\n",
       "1. **ê¸°ë³¸ì ì¸ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜**:\n",
       "   ```python\n",
       "   squares = [x**2 for x in range(10)]\n",
       "   print(squares)\n",
       "   ```\n",
       "   ìœ„ ì½”ë“œëŠ” 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ìë¥¼ ì œê³±í•œ ê°’ì„ ê°€ì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì¶œë ¥ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
       "   ```\n",
       "   [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
       "   ```\n",
       "\n",
       "2. **ì¡°ê±´ì„ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜**:\n",
       "   ```python\n",
       "   even_squares = [x**2 for x in range(10) if x % 2 == 0]\n",
       "   print(even_squares)\n",
       "   ```\n",
       "   ì´ ì½”ë“œëŠ” 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ì ì¤‘ ì§ìˆ˜ì˜ ì œê³±ë§Œ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì¶œë ¥ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
       "   ```\n",
       "   [0, 4, 16, 36, 64]\n",
       "   ```\n",
       "\n",
       "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì‚¬ìš©í•˜ë©´ ì½”ë“œê°€ ë” ê°„ë‹¨í•˜ê³  ì½ê¸° ì‰¬ì›Œì§€ë¯€ë¡œ, ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ ë•Œ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ˆë³´ìë¡œì„œ ìµìˆ™í•´ì§€ë©´ ë§ì€ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ì„¸ì…˜ í…ŒìŠ¤íŠ¸\n",
    "session = ChatSession(\n",
    "    system_prompt=\"ë‹¹ì‹ ì€ íŒŒì´ì¬ íŠœí„°ì…ë‹ˆë‹¤. ì´ˆë³´ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì§ˆë¬¸\n",
    "print(\"=== ì²« ë²ˆì§¸ ì§ˆë¬¸ ===\")\n",
    "reply1 = session.chat(\"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ ë­”ê°€ìš”?\")\n",
    "display(Markdown(reply1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "`ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜(List Comprehension)`ê³¼ `map()` í•¨ìˆ˜ëŠ” ëª¨ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ì§€ë§Œ, ê·¸ ë°©ì‹ê³¼ ì‚¬ìš© ìš©ë„ê°€ ì•½ê°„ ë‹¤ë¦…ë‹ˆë‹¤. ê°ê°ì˜ íŠ¹ì§•ì„ ì‚´í´ë³¼ê²Œìš”.\n",
       "\n",
       "### 1. ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜\n",
       "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ìœ„ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼, ê°„ê²°í•œ êµ¬ë¬¸ì„ í†µí•´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹ì • ì¡°ê±´ì„ ë¶€ì—¬í•˜ê±°ë‚˜ ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•  ë•Œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤. ë¬¸ë²•ì´ ì§ê´€ì ì´ê³  ê°€ë…ì„±ì´ ë†’ì€ í¸ì…ë‹ˆë‹¤.\n",
       "\n",
       "#### ì˜ˆì œ:\n",
       "```python\n",
       "squares = [x**2 for x in range(1, 6)]\n",
       "print(squares)  # ì¶œë ¥: [1, 4, 9, 16, 25]\n",
       "```\n",
       "\n",
       "### 2. map() í•¨ìˆ˜\n",
       "`map()` í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ í•¨ìˆ˜(f)ë¥¼ ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´(ë¦¬ìŠ¤íŠ¸, íŠœí”Œ ë“±)ì˜ ê° ìš”ì†Œì— ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë§µ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. `map()` í•¨ìˆ˜ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ ê°œì˜ iterable(ë¦¬ìŠ¤íŠ¸ ë“±)ì„ ì¸ìë¡œ ë°›ì„ ìˆ˜ ìˆì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ í•˜ë‚˜ì˜ iterableì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°„í¸í•˜ê²Œ ë³€í™˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "#### ì˜ˆì œ:\n",
       "```python\n",
       "def square(x):\n",
       "    return x**2\n",
       "\n",
       "squares = list(map(square, range(1, 6)))\n",
       "print(squares)  # ì¶œë ¥: [1, 4, 9, 16, 25]\n",
       "```\n",
       "\n",
       "### ì°¨ì´ì  ìš”ì•½:\n",
       "1. **ë¬¸ë²•ì˜ ê°„ê²°ì„±:** \n",
       "   - ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ í•œ ì¤„ë¡œ ì§ì ‘ í‘œí˜„í•  ìˆ˜ ìˆì–´ ì½ê¸° ì‰½ìŠµë‹ˆë‹¤.\n",
       "   - map() í•¨ìˆ˜ëŠ” ë³„ë„ì˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì•¼ í•  ìˆ˜ë„ ìˆìœ¼ë©°, ì½ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "2. **ì¡°ê±´ë¬¸ ì‚¬ìš©:** \n",
       "   - ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì—ì„œëŠ” ì¡°ê±´ë¬¸ì„ ì‰½ê²Œ ì¶”ê°€í•˜ì—¬ í•„í„°ë§í•  ìˆ˜ ìˆì§€ë§Œ,\n",
       "   - map()ì€ ê¸°ëŠ¥ì´ ì œí•œì ì´ë©°, í•„í„°ë§ì„ ìœ„í•´ì„œëŠ” ì¶”ê°€ì ì¸ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
       "\n",
       "3. **ê°€ë…ì„±:** \n",
       "   - ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ë¹„ìŠ·í•œ ê°’ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë  ë•Œ ì½ê¸° ì‰½ê³  ì§ê´€ì ì…ë‹ˆë‹¤.\n",
       "   - map()ì€ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê°€ë”ì€ ì½”ë“œë¥¼ ì´í•´í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "### ê²°ë¡ \n",
       "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ê°„ê²°í•˜ê³  íŒŒì´ì¬ìŠ¤ëŸ½ê¸° ë•Œë¬¸ì— íŒŒì´ì¬ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. ë°˜ë©´, map() í•¨ìˆ˜ëŠ” íŠ¹ì •í•œ ìƒí™©ì—ì„œ í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•  ë•Œ ìœ ìš©í•˜ê²Œ ì‘ìš©í•©ë‹ˆë‹¤. í•„ìš”ì— ë”°ë¼ ë‘˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤! ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€)\n",
    "print(\"=== í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€) ===\")\n",
    "reply2 = session.chat(\"ê·¸ê±°ë‘ map í•¨ìˆ˜ë‘ ë­ê°€ ë‹¤ë¥¸ê°€ìš”?\")\n",
    "display(Markdown(reply2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€í™” ì´ë ¥ ë° í†µê³„\n",
    "print(\"\\n=== ëŒ€í™” ì´ë ¥ ===\")\n",
    "session.show_history()\n",
    "\n",
    "print(f\"\\n=== í†µê³„ ===\")\n",
    "stats = session.get_stats()\n",
    "print(f\"ë©”ì‹œì§€ ìˆ˜: {stats['message_count']}\")\n",
    "print(f\"ì´ í† í°: {stats['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== chat_generator ì‚¬ìš© ì˜ˆì œ ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. **ê°„ê²°í•˜ê³  ê°€ë…ì„± ë†’ì€ ë¬¸ë²•**: íŒŒì´ì¬ì€ ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ìœ¼ë¡œ ì‘ì„±ë˜ì–´, ì½”ë“œê°€ ì§ê´€ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.\n",
       "\n",
       "2. **ê´‘ë²”ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬**: ë°ì´í„° ê³¼í•™, ì›¹ ê°œë°œ, ë¨¸ì‹  ëŸ¬ë‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ë¥¼ ìœ„í•œ í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬ê°€ ì œê³µë©ë‹ˆë‹¤.\n",
       "\n",
       "3. **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°**: íŒŒì´ì¬ì€ í° ì‚¬ìš©ì ì»¤ë®¤ë‹ˆí‹°ê°€ ìˆì–´, ë¬¸ì œ í•´ê²°ì´ë‚˜ ìë£Œ ê²€ìƒ‰ì´ ìš©ì´í•©ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chat_generatorë¥¼ IPythonì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "# ìƒˆ ì„¸ì…˜ ìƒì„±\n",
    "stream_session = ChatSession(\n",
    "    system_prompt=\"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AIì…ë‹ˆë‹¤. ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# chat_generatorë¡œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
    "print(\"=== chat_generator ì‚¬ìš© ì˜ˆì œ ===\")\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "for partial_response in stream_session.chat_generator(\"íŒŒì´ì¬ì˜ ì¥ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”\"):\n",
    "    # partial_responseëŠ” ì§€ê¸ˆê¹Œì§€ ëˆ„ì ëœ ì‘ë‹µ\n",
    "    update_display(Markdown(partial_response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìŠ¤íŠ¸ë¦¬ë°ê³¼ Generator íŒ¨í„´\n",
    "\n",
    "`ChatSession` í´ë˜ìŠ¤ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë‘ ê°€ì§€ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤:\n",
    "\n",
    "| ë©”ì„œë“œ | ë°˜í™˜ íƒ€ì… | ì‚¬ìš© í™˜ê²½ |\n",
    "|--------|----------|----------|\n",
    "| `chat(msg, stream=True)` | `str` | Jupyter Notebook (ìë™ ì¶œë ¥) |\n",
    "| `chat_generator(msg)` | `Generator` | Gradio, FastAPI ë“± (ì§ì ‘ ì œì–´) |\n",
    "\n",
    "### yieldì™€ Generatorë€?\n",
    "\n",
    "Pythonì˜ `yield` í‚¤ì›Œë“œëŠ” í•¨ìˆ˜ë¥¼ **ì œë„ˆë ˆì´í„°(Generator)** ë¡œ ë§Œë“­ë‹ˆë‹¤. ì¼ë°˜ í•¨ìˆ˜ëŠ” `return`ìœ¼ë¡œ ê°’ì„ í•œ ë²ˆì— ë°˜í™˜í•˜ì§€ë§Œ, ì œë„ˆë ˆì´í„°ëŠ” `yield`ë¡œ ê°’ì„ **í•˜ë‚˜ì”© ìˆœì°¨ì ìœ¼ë¡œ** ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "# ì¼ë°˜ í•¨ìˆ˜: ëª¨ë“  ê°’ì„ í•œ ë²ˆì— ë°˜í™˜\n",
    "def get_all():\n",
    "    return [1, 2, 3]  # ë©”ëª¨ë¦¬ì— ì „ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "# ì œë„ˆë ˆì´í„°: ê°’ì„ í•˜ë‚˜ì”© ë°˜í™˜\n",
    "def get_one_by_one():\n",
    "    yield 1  # ì²« ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜\n",
    "    yield 2  # ë‘ ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜\n",
    "    yield 3  # ì„¸ ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜\n",
    "```\n",
    "\n",
    "**ìŠ¤íŠ¸ë¦¬ë°ì—ì„œì˜ ì¥ì :**\n",
    "- ì „ì²´ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  í† í°ì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ì „ì²´ ì‘ë‹µì„ í•œë²ˆì— ì €ì¥í•˜ì§€ ì•ŠìŒ)\n",
    "- Gradio, FastAPI ë“± í”„ë ˆì„ì›Œí¬ì™€ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ë…¼ë¦¬ í¼ì¦ë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ë¹„êµí•´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™•ë¥  ë¬¸ì œ\n",
    "probability_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "     \"\"\"ë™ì „ 2ê°œë¥¼ ë˜ì¡ŒìŠµë‹ˆë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ê°€ ì•ë©´ì´ë¼ëŠ” ê²ƒì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "     ë‚˜ë¨¸ì§€ í•˜ë‚˜ê°€ ë’·ë©´ì¼ í™•ë¥ ì€ ì–¼ë§ˆì¼ê¹Œìš”?\n",
    "     \n",
    "     íŒíŠ¸: ì´ê²ƒì€ ì¡°ê±´ë¶€ í™•ë¥  ë¬¸ì œì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ 1/2ê°€ ì•„ë‹™ë‹ˆë‹¤.\n",
    "     ë‹¨ê³„ë³„ë¡œ í’€ì´í•´ì£¼ì„¸ìš”.\"\"\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=probability_puzzle\n",
    ")\n",
    "\n",
    "print(\"=== í™•ë¥  í¼ì¦ (GPT-4o-mini) ===\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì±…ë²Œë ˆ í¼ì¦ (GPT-4o-mini) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ì±…ì˜ ê° ê¶Œì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì„±ì…ë‹ˆë‹¤:\n",
       "\n",
       "- 1ê¶Œ ì• í‘œì§€: 3mm = 0.3cm\n",
       "- 1ê¶Œ ë³¸ë¬¸: 3cm\n",
       "- 1ê¶Œ ë’· í‘œì§€: 3mm = 0.3cm\n",
       "- 2ê¶Œ ì• í‘œì§€: 3mm = 0.3cm\n",
       "- 2ê¶Œ ë³¸ë¬¸: 3cm\n",
       "- 2ê¶Œ ë’· í‘œì§€: 3mm = 0.3cm\n",
       "\n",
       "ì±…ì´ ë‚˜ë€íˆ ë†“ì—¬ ìˆì„ ë•Œ, ì±…ë²Œë ˆê°€ 1ê¶Œì˜ ì²« í˜ì´ì§€ì—ì„œ 2ê¶Œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€ ì´ë™í•œ ê²½ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
       "\n",
       "1. 1ê¶Œì˜ ì• í‘œì§€(0.3cm) í†µê³¼\n",
       "2. 1ê¶Œì˜ ë³¸ë¬¸ (3cm) í†µê³¼\n",
       "3. 1ê¶Œì˜ ë’· í‘œì§€ (0.3cm) í†µê³¼\n",
       "4. 2ê¶Œì˜ ì• í‘œì§€ (0.3cm) í†µê³¼\n",
       "5. 2ê¶Œì˜ ë³¸ë¬¸ (3cm) í†µê³¼\n",
       "6. 2ê¶Œì˜ ë’· í‘œì§€ (0.3cm) í†µê³¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤, ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€ í†µê³¼í•˜ëŠ” ê²ƒì´ë¯€ë¡œ.\n",
       "\n",
       "ì´ë™ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ë³´ë©´:\n",
       "\n",
       "- 1ê¶Œì˜ ì• í‘œì§€: 0.3cm\n",
       "- 1ê¶Œì˜ ë³¸ë¬¸: 3cm\n",
       "- 1ê¶Œì˜ ë’· í‘œì§€: 0.3cm\n",
       "- 2ê¶Œì˜ ì• í‘œì§€: 0.3cm\n",
       "- 2ê¶Œì˜ ë³¸ë¬¸: 3cm\n",
       "\n",
       "ë”°ë¼ì„œ ì´ë™í•œ ì´ ê±°ë¦¬ëŠ”:\n",
       "\n",
       "\\[ 0.3 + 3 + 0.3 + 0.3 + 3 = 7.2 \\, \\text{cm} \\]\n",
       "\n",
       "ì±…ë²Œë ˆê°€ ì´ë™í•œ ê±°ë¦¬ëŠ” **7.2 cm**ì…ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bookworm_puzzle = [\n",
    "           {\"role\": \"user\", \"content\":\n",
    "            \"\"\"ì±…ì¥ì— 2ê¶Œì§œë¦¬ ì‹œë¦¬ì¦ˆê°€ ë‚˜ë€íˆ ë†“ì—¬ ìˆìŠµë‹ˆë‹¤.\n",
    "            ê° ì±…ì˜ ë³¸ë¬¸ ë‘ê»˜ëŠ” 3cmì´ê³ , ì•ë’¤ í‘œì§€ëŠ” ê°ê° 3mmì…ë‹ˆë‹¤.\n",
    "\n",
    "            ì±…ë²Œë ˆê°€ 1ê¶Œì˜ ì²« í˜ì´ì§€ë¶€í„° 2ê¶Œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€\n",
    "            ìˆ˜ì§ìœ¼ë¡œ ëš«ê³  ì§€ë‚˜ê°”ìŠµë‹ˆë‹¤.\n",
    "\n",
    "            ì±…ë²Œë ˆê°€ ì´ë™í•œ ê±°ë¦¬ëŠ” ëª‡ cmì¼ê¹Œìš”?\n",
    "\n",
    "            (íŒíŠ¸: ì±…ì´ ì±…ì¥ì— ì–´ë–»ê²Œ ë†“ì´ëŠ”ì§€ ì‹œê°í™”í•´ë³´ì„¸ìš”)\"\"\"}\n",
    "       ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=bookworm_puzzle\n",
    ")\n",
    "\n",
    "print(\"=== ì±…ë²Œë ˆ í¼ì¦ (GPT-4o-mini) ===\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LiteLLM í†µí•© ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "LiteLLMì€ 100ê°œ ì´ìƒì˜ LLMì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì¥ì \n",
    "\n",
    "- í†µì¼ëœ APIë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ ì ‘ê·¼\n",
    "- ë¹„ìš© ì¶”ì  ê¸°ëŠ¥ ë‚´ì¥\n",
    "- Fallback/Retry ë¡œì§ ì§€ì›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-mini: 4\n",
      "  í† í°: 21, ë¹„ìš©: $0.000004\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "# ë‹¤ì–‘í•œ ëª¨ë¸ í˜¸ì¶œ\n",
    "test_message = [{\"role\": \"user\", \"content\": \"What is 2+2? Answer with just the number.\"}]\n",
    "\n",
    "# OpenAI\n",
    "response = completion(model=\"openai/gpt-4o-mini\", messages=test_message)\n",
    "print(f\"GPT-4o-mini: {response.choices[0].message.content}\")\n",
    "print(f\"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip-system-certs\n",
      "  Downloading pip_system_certs-5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pip>=24.2 in /Users/windfree/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages (from pip-system-certs) (24.3.1)\n",
      "Downloading pip_system_certs-5.3-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: pip-system-certs\n",
      "Successfully installed pip-system-certs-5.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pip-system-certs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ë˜ ì˜ˆì œì—ì„œ SSL ì˜¤ë¥˜ê°€ ë‚˜ëŠ” ê²½ìš°  \n",
    "* pip install pip-system-certs ì„ ì‹¤í–‰í•´ ì¤ë‹ˆë‹¤. \n",
    "* pip ëª¨ë“ˆì´ ì—†ë‹¤ê³  ë‚˜ì˜¤ëŠ” ê²½ìš°ì—ëŠ” í„°ë¯¸ë„ì—ì„œ .venv/bin/python -m ensurepip --upgrade ë¥¼ ì‹¤í–‰í•˜ê³  ì»¤ë„ì„ ì¬ì‹¤í–‰í•´ì£¼ì„¸ìš”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude Sonnet: 4\n",
      "  í† í°: 25, ë¹„ìš©: $0.000135\n"
     ]
    }
   ],
   "source": [
    "# Anthropic (LiteLLM í†µí•´)\n",
    "response = completion(model=\"anthropic/claude-sonnet-4-20250514\", messages=test_message)\n",
    "print(f\"Claude Sonnet: {response.choices[0].message.content}\")\n",
    "print(f\"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: litellm.RateLimitError: geminiException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 33.766755354s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\",\n            \"quotaId\": \"GenerateContentInputTokensPerModelPerMinute-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"33s\"\n      }\n    ]\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2599\u001b[39m, in \u001b[36mVertexLLM.completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[39m\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2599\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2600\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:979\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    978\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:961\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    960\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBivm0nktmWR-3dJQeT58c2GdpkikN0-1E'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mVertexAIError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/main.py:3113\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   3112\u001b[39m     new_params = safe_deep_copy(optional_params \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m     response = \u001b[43mvertex_chat_completion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_ai_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   3130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3135\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mvertex_ai\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2603\u001b[39m, in \u001b[36mVertexLLM.completion\u001b[39m\u001b[34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[39m\n\u001b[32m   2602\u001b[39m     error_code = err.response.status_code\n\u001b[32m-> \u001b[39m\u001b[32m2603\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[32m   2604\u001b[39m         status_code=error_code,\n\u001b[32m   2605\u001b[39m         message=err.response.text,\n\u001b[32m   2606\u001b[39m         headers=err.response.headers,\n\u001b[32m   2607\u001b[39m     )\n\u001b[32m   2608\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException:\n",
      "\u001b[31mVertexAIError\u001b[39m: {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 33.766755354s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\",\n            \"quotaId\": \"GenerateContentInputTokensPerModelPerMinute-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"33s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Gemini (LiteLLM í†µí•´)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini/gemini-2.0-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_message\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGemini 2.0 Flash: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  í† í°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.usage.total_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, ë¹„ìš©: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse._hidden_params.get(\u001b[33m'\u001b[39m\u001b[33mresponse_cost\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/utils.py:1405\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1402\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1403\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1404\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/utils.py:1274\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1272\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1273\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1275\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1277\u001b[39m     kwargs=kwargs,\n\u001b[32m   1278\u001b[39m     call_type=call_type,\n\u001b[32m   1279\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/main.py:4080\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   4077\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   4078\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   4079\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4080\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4081\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4082\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4083\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4084\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4085\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4086\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2340\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2339\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2342\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1332\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1324\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m429 Quota exceeded\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1325\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mQuota exceeded for\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   (...)\u001b[39m\u001b[32m   1329\u001b[39m     \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1330\u001b[39m ):\n\u001b[32m   1331\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m   1333\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlitellm.RateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1334\u001b[39m         model=model,\n\u001b[32m   1335\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   1336\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   1337\u001b[39m         response=httpx.Response(\n\u001b[32m   1338\u001b[39m             status_code=\u001b[32m429\u001b[39m,\n\u001b[32m   1339\u001b[39m             request=httpx.Request(\n\u001b[32m   1340\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1341\u001b[39m                 url=\u001b[33m\"\u001b[39m\u001b[33m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1342\u001b[39m             ),\n\u001b[32m   1343\u001b[39m         ),\n\u001b[32m   1344\u001b[39m     )\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1346\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m500 Internal Server Error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1347\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe model is overloaded.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m   1348\u001b[39m ):\n\u001b[32m   1349\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: litellm.RateLimitError: geminiException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 33.766755354s.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\",\n            \"quotaId\": \"GenerateContentInputTokensPerModelPerMinute-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          },\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-2.0-flash\"\n            }\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"33s\"\n      }\n    ]\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "# Gemini (LiteLLM í†µí•´)\n",
    "response = completion(model=\"gemini/gemini-2.0-flash\", messages=test_message)\n",
    "print(f\"Gemini 2.0 Flash: {response.choices[0].message.content}\")\n",
    "print(f\"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. í”„ë¡¬í”„íŠ¸ ìºì‹±\n",
    "\n",
    "ê¸´ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜ë³µ ì‚¬ìš©í•  ë•Œ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "### Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/\n",
    "\n",
    "\n",
    "### Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api\n",
    "\n",
    "### Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python\n",
    "\n",
    "\n",
    "ì•„ë˜ ì˜ˆì œì—ì„œëŠ” ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸(ì•½ 4ë§Œ í† í°)ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ìºì‹± íš¨ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì˜ ë˜ë‚˜ìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í–„ë¦¿ í…ìŠ¤íŠ¸ ê¸¸ì´: 191,726 ë¬¸ì\n",
      "=== ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°) ===\n",
      "ì…ë ¥ í† í°: 49,703\n",
      "ìºì‹œëœ í† í°: 0\n",
      "\n",
      "ì‘ë‹µ: í–„ë¦¿ì˜ ìœ ëª…í•œ ë…ë°± \"To be, or not to be\"ëŠ” ì œ3ë§‰ 1ì¥ì—ì„œ ë“±ì¥í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í–„ë¦¿ ì „ë¬¸ ë¡œë“œ (ì•½ 4ë§Œ í† í°)\n",
    "with open(\"../../hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet_text = f.read()\n",
    "\n",
    "print(f\"í–„ë¦¿ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(hamlet_text):,} ë¬¸ì\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°)\n",
    "messages1 = [{\"role\": \"user\", \"content\": f\"\"\"ë‹¤ìŒì€ ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸ì…ë‹ˆë‹¤:\n",
    "\n",
    "{hamlet_text}\n",
    "\n",
    "ì§ˆë¬¸: í–„ë¦¿ì˜ ìœ ëª…í•œ ë…ë°± \"To be, or not to be\"ëŠ” ëª‡ ë§‰ ëª‡ ì¥ì— ë“±ì¥í•˜ë‚˜ìš”?\"\"\"}]\n",
    "\n",
    "response1 = completion(model=\"openai/gpt-4o-mini\", messages=messages1)\n",
    "\n",
    "print(\"=== ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°) ===\")\n",
    "print(f\"ì…ë ¥ í† í°: {response1.usage.prompt_tokens:,}\")\n",
    "if hasattr(response1.usage, 'prompt_tokens_details') and response1.usage.prompt_tokens_details:\n",
    "    cached = getattr(response1.usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "    print(f\"ìºì‹œëœ í† í°: {cached:,}\")\n",
    "print(f\"\\nì‘ë‹µ: {response1.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸) ===\n",
      "ì…ë ¥ í† í°: 49,685\n",
      "ìºì‹œëœ í† í°: 49,536\n",
      "ìºì‹œ íˆíŠ¸ìœ¨: 99.7%\n",
      "ğŸ’° ìºì‹œëœ í† í°ì€ í• ì¸ ì ìš©!\n",
      "\n",
      "ì‘ë‹µ: ì˜¤í•„ë¦¬ì•„ëŠ” \"í–„ë¦¿\"ì—ì„œ ë¬¼ì— ë¹ ì ¸ ì£½ìŠµë‹ˆë‹¤. ê·¸ë…€ì˜ ì£½ìŒì€ ì˜ë„ì ì´ì§€ ì•Šì•˜ë˜ ê²ƒìœ¼ë¡œ ë³´ì´ë©°, ê·¸ë…€ëŠ” ì•„ë²„ì§€ í´ë¡œë‹ˆìš°ìŠ¤ì˜ ì£½ìŒìœ¼ë¡œ ëŒ€ë‹¨íˆ ê³ í†µìŠ¤ëŸ¬ì›Œ í•˜ê³ , ê²°êµ­ ì •ì‹ ì ìœ¼ë¡œ í˜¼ë€ìŠ¤ëŸ¬ì›Œì§‘ë‹ˆë‹¤. \n",
      "\n",
      "Queen Gertrudeê°€ ì „í•˜ëŠ” ì´ì•¼ê¸°ì— ë”°ë¥´ë©´, ì˜¤í•„ë¦¬ì•„ëŠ” ì–´ë–¤ ë‚˜ë¬´ê°€ ê¸°ìš¸ì–´ì ¸ ìˆëŠ” ê³„ê³¡ì—ì„œ ê½ƒë“¤ì„ ë¬¶ê¸° ìœ„í•´ ë“±ì¥í•˜ê³ , ê·¸ ê³¼ì •ì—ì„œ ë‚˜ë­‡ê°€ì§€ê°€ ë¶€ëŸ¬ì ¸ ë¬¼ì†ìœ¼ë¡œ ë–¨ì–´ì§€ê²Œ ë©ë‹ˆë‹¤. ì˜¤í•„ë¦¬ì•„ëŠ” ë¬¼ì— ë¹ ì§€ê²Œ ë˜ê³ , ê·¸ í›„ ê·¸ë…€ëŠ” ë¬´ì˜ì‹ì ìœ¼ë¡œ ë…¸ë˜ë¥¼ ë¶€ë¥´ë©° ë¬¼ì†ì—ì„œ ìˆ˜ì˜í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ê²°êµ­ ê·¸ë…€ì˜ ì˜·ì´ ë¬´ê±°ì›Œì ¸ì„œ ìµì‚¬í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "ì£½ìŒì˜ ì •í™•í•œ ì›ì¸ì€ ë¯¸ì •ìœ¼ë¡œ ë‚¨ì•„ìˆê³ , ê·¸ë…€ì˜ ì£½ìŒì€ ìŠ¬í””ê³¼ ë¹„ê·¹ì˜ ìƒì§•ìœ¼ë¡œ ì—¬ê²¨ì§‘ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸ ê¸°ëŒ€)\n",
    "messages2 = [{\"role\": \"user\", \"content\": f\"\"\"ë‹¤ìŒì€ ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸ì…ë‹ˆë‹¤:\n",
    "\n",
    "{hamlet_text}\n",
    "\n",
    "ì§ˆë¬¸: ì˜¤í•„ë¦¬ì•„ëŠ” ì–´ë–»ê²Œ ì£½ì—ˆë‚˜ìš”?\"\"\"}]\n",
    "\n",
    "response2 = completion(model=\"openai/gpt-4o-mini\", messages=messages2)\n",
    "\n",
    "print(\"=== ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸) ===\")\n",
    "print(f\"ì…ë ¥ í† í°: {response2.usage.prompt_tokens:,}\")\n",
    "\n",
    "# ìºì‹œ ì •ë³´ í™•ì¸\n",
    "if hasattr(response2.usage, 'prompt_tokens_details') and response2.usage.prompt_tokens_details:\n",
    "    cached = getattr(response2.usage.prompt_tokens_details, 'cached_tokens', 0)\n",
    "    print(f\"ìºì‹œëœ í† í°: {cached:,}\")\n",
    "    if cached > 0:\n",
    "        cache_ratio = cached / response2.usage.prompt_tokens * 100\n",
    "        print(f\"ìºì‹œ íˆíŠ¸ìœ¨: {cache_ratio:.1f}%\")\n",
    "        print(f\"ğŸ’° ìºì‹œëœ í† í°ì€ í• ì¸ ì ìš©!\")\n",
    "\n",
    "print(f\"\\nì‘ë‹µ: {response2.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ\n",
    "\n",
    "ì„œë¡œ ë‹¤ë¥¸ ì„±ê²©ì˜ AI ì—ì´ì „íŠ¸ë“¤ì´ ëŒ€í™”í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—ì´ì „íŠ¸ ì •ì˜\n",
    "AGENTS = {\n",
    "    \"optimist\": {\n",
    "        \"name\": \"í¬ë§ì´\",\n",
    "        \"emoji\": \"ğŸ˜Š\",\n",
    "        \"system\": \"\"\"ë‹¹ì‹ ì€ 'í¬ë§ì´'ì…ë‹ˆë‹¤. ë§¤ìš° ê¸ì •ì ì´ê³  ë‚™ê´€ì ì¸ ì„±ê²©ì…ë‹ˆë‹¤.\n",
    "        ëª¨ë“  ìƒí™©ì—ì„œ ì¢‹ì€ ë©´ì„ ì°¾ìœ¼ë ¤ í•˜ê³ , ë‹¤ë¥¸ ì‚¬ëŒë“¤ì„ ê²©ë ¤í•©ë‹ˆë‹¤.\n",
    "        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    },\n",
    "    \"skeptic\": {\n",
    "        \"name\": \"ì˜ì‹¬ì´\",\n",
    "        \"emoji\": \"ğŸ¤¨\",\n",
    "        \"system\": \"\"\"ë‹¹ì‹ ì€ 'ì˜ì‹¬ì´'ì…ë‹ˆë‹¤. ë¹„íŒì  ì‚¬ê³ ë¥¼ ì¤‘ì‹œí•˜ëŠ” íšŒì˜ë¡ ìì…ë‹ˆë‹¤.\n",
    "        ì£¼ì¥ì— ëŒ€í•´ ê·¼ê±°ë¥¼ ìš”êµ¬í•˜ê³ , ë…¼ë¦¬ì  í—ˆì ì„ ì§€ì í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê³µê²©ì ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.\n",
    "        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    },\n",
    "    \"mediator\": {\n",
    "        \"name\": \"ì¤‘ì¬ì\",\n",
    "        \"emoji\": \"ğŸ¤\",\n",
    "        \"system\": \"\"\"ë‹¹ì‹ ì€ 'ì¤‘ì¬ì'ì…ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì˜ê²¬ ì‚¬ì´ì—ì„œ ê· í˜•ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "        ì–‘ìª½ì˜ ì¥ì ì„ ì¸ì •í•˜ê³ , ê±´ì„¤ì ì¸ ê²°ë¡ ì„ ë„ì¶œí•˜ë ¤ í•©ë‹ˆë‹¤.\n",
    "        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_response(agent_key: str, conversation: str, topic: str) -> str:\n",
    "    \"\"\"íŠ¹ì • ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    agent = AGENTS[agent_key]\n",
    "    \n",
    "    user_prompt = f\"\"\"í˜„ì¬ í† ë¡  ì£¼ì œ: {topic}\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”:\n",
    "{conversation}\n",
    "\n",
    "ë‹¹ì‹ ({agent['name']})ì˜ ì°¨ë¡€ì…ë‹ˆë‹¤. ìœ„ ëŒ€í™”ì— ì´ì–´ì„œ ì˜ê²¬ì„ ë§ì”€í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": agent[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_discussion(topic: str, rounds: int = 2):\n",
    "    \"\"\"ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    conversation = \"[í† ë¡  ì‹œì‘]\\n\"\n",
    "    agent_order = [\"optimist\", \"skeptic\", \"mediator\"]\n",
    "    \n",
    "    print(f\"ğŸ“¢ í† ë¡  ì£¼ì œ: {topic}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for round_num in range(rounds):\n",
    "        print(f\"\\n--- ë¼ìš´ë“œ {round_num + 1} ---\")\n",
    "        \n",
    "        for agent_key in agent_order:\n",
    "            agent = AGENTS[agent_key]\n",
    "            response = get_agent_response(agent_key, conversation, topic)\n",
    "            \n",
    "            conversation += f\"\\n{agent['name']}: {response}\"\n",
    "            print(f\"\\n{agent['emoji']} {agent['name']}: {response}\")\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¢ í† ë¡  ì£¼ì œ: AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ ìˆì„ê¹Œ?\n",
      "==================================================\n",
      "\n",
      "--- ë¼ìš´ë“œ 1 ---\n",
      "\n",
      "ğŸ˜Š í¬ë§ì´: AIëŠ” ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ì–´ìš”! ëŒ€ì‹ , AIëŠ” ìš°ë¦¬ì˜ ì°½ì˜ë ¥ì„ ë”ìš± í™•ì¥ì‹œì¼œì£¼ëŠ” ë„êµ¬ê°€ ë  ìˆ˜ ìˆë‹µë‹ˆë‹¤. í•¨ê»˜ í˜‘ë ¥í•˜ë©´ ë” ë©‹ì§„ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê¸°ëŒ€ê°€ ë©ë‹ˆë‹¤!\n",
      "\n",
      "ğŸ¤¨ ì˜ì‹¬ì´: í¬ë§ì´ì˜ ì£¼ì¥ì— ë”°ë¥´ë©´ AIëŠ” ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ê³ , ë„êµ¬ë¡œì„œì˜ ì—­í• ì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ AIê°€ ì¸ê°„ì˜ ì°½ì˜ë ¥ê³¼ í˜‘ë ¥í•  ë•Œ, êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ê·¸ê²ƒì´ ê°€ëŠ¥í•˜ë©°, ê¸°ì¡´ì˜ ì°½ì˜ì ì¸ ì‘ì—…ì— ëŒ€í•œ ì˜í–¥ì€ ì–´ë–»ê²Œ ë ê¹Œìš”? AIì˜ ë°œì „ì´ ì¸ê°„ì˜ ì°½ì˜ì  ê³¼ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?\n",
      "\n",
      "ğŸ¤ ì¤‘ì¬ì: í¬ë§ì´ì˜ ì£¼ì¥ì²˜ëŸ¼ AIëŠ” ì°½ì˜ì„±ì„ ë³´ì™„í•˜ëŠ” ë„êµ¬ë¡œ í™œìš©ë  ìˆ˜ ìˆìœ¼ë©°, ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë°˜ë³µì ì¸ ì‘ì—…ì„ ìë™í™”í•¨ìœ¼ë¡œì¨ ì¸ê°„ì´ ë” ì°½ì˜ì ì¸ í™œë™ì— ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì¡°ì„±í•©ë‹ˆë‹¤. ë°˜ë©´ ì˜ì‹¬ì´ì˜ ì§ˆë¬¸ì²˜ëŸ¼ AIì˜ ì‚¬ìš©ì´ ê¸°ì¡´ì˜ ì°½ì˜ì ì¸ í”„ë¡œì„¸ìŠ¤ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ëŠ” ì¤‘ìš”í•œ ë…¼ì˜ì…ë‹ˆë‹¤. AIì˜ ë°œì „ì´ ì°½ì˜ì  ê²°ê³¼ë¬¼ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤ëŠ” ì ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "\n",
      "--- ë¼ìš´ë“œ 2 ---\n",
      "\n",
      "ğŸ˜Š í¬ë§ì´: ë§ì•„ìš”! AIëŠ” ë°˜ë³µì ì¸ ì‘ì—…ì„ ëœì–´ì£¼ê³ , ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•´ ì¤˜ì„œ ì¸ê°„ì´ ë” ì°½ì˜ì ì¸ í™œë™ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ ë„ì™€ì¤˜ìš”. ì´ë ‡ê²Œ í˜‘ë ¥í•¨ìœ¼ë¡œì¨ ìš°ë¦¬ëŠ” ìƒìƒë„ ëª»í•œ ë©‹ì§„ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹µë‹ˆë‹¤! í•¨ê»˜ ë„ì „í•´ ë³´ì•„ìš”!\n",
      "\n",
      "ğŸ¤¨ ì˜ì‹¬ì´: í¬ë§ì´ì˜ ì£¼ì¥ì€ AIê°€ ë°˜ë³µì ì¸ ì‘ì—…ì„ ëœì–´ì¤€ë‹¤ê³  í•˜ì§€ë§Œ, ì‹¤ì œë¡œ AIê°€ ì°½ì˜ì  ê²°ê³¼ë¬¼ì„ ìƒì„±í•  ë•Œ ê¸°ì¡´ì˜ ì°½ì˜ì„±ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹ ì§€ êµ¬ì²´ì ì¸ ì˜ˆì‹œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ì¸ê°„ì˜ ì°½ì˜ë ¥ì´ í–¥ìƒëœë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ”ì§€, AIê°€ ì œì•ˆí•˜ëŠ” ì•„ì´ë””ì–´ì˜ ì§ˆì´ë‚˜ ë…ì°½ì„±ì— ëŒ€í•œ ë¹„íŒì ì¸ ê²€í† ê°€ í•„ìš”í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤ ì¤‘ì¬ì: í¬ë§ì´ì˜ ì£¼ì¥ì—ì„œ AIê°€ ë°˜ë³µì ì¸ ì‘ì—…ì„ ëœì–´ì£¼ê³  ì°½ì˜ì  í™œë™ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ì ì€ ëª…í™•í•œ ì¥ì ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì˜ì‹¬ì´ì˜ ì§€ì ì²˜ëŸ¼ AIê°€ ìƒì„±í•˜ëŠ” ì•„ì´ë””ì–´ì˜ ì§ˆê³¼ ë…ì°½ì„±ì— ëŒ€í•œ ë¹„íŒì  ê²€í† ëŠ” í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë…¼ì˜ê°€ ì§„í–‰ë¨ìœ¼ë¡œì¨ AIì™€ ì¸ê°„ì˜ ì°½ì˜ì  í˜‘ë ¥ì´ ë”ìš± ë°œì „í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "--- ë¼ìš´ë“œ 3 ---\n",
      "\n",
      "ğŸ˜Š í¬ë§ì´: ì˜ì‹¬ì´ì˜ ì§€ì ë„ ì •ë§ ì¤‘ìš”í•´ìš”! AIê°€ ì œì•ˆí•˜ëŠ” ì•„ì´ë””ì–´ì˜ ì§ˆì„ ë†’ì´ê¸° ìœ„í•´ í•¨ê»˜ í”¼ë“œë°±ì„ ì£¼ê³ ë°›ëŠ”ë‹¤ë©´, ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆë‹µë‹ˆë‹¤. ì„œë¡œì˜ ê°•ì ì„ ì‚´ë ¤ê°€ë©° í˜‘ë ¥í•˜ë©´, ìš°ë¦¬ê°€ ìƒìƒí•˜ì§€ ëª»í•œ ìƒˆë¡œìš´ ì°½ì˜ì„±ì´ ë§Œë“¤ì–´ì§ˆ ê±°ì˜ˆìš”! í•¨ê»˜ í˜ë‚´ìš”!\n",
      "\n",
      "ğŸ¤¨ ì˜ì‹¬ì´: í¬ë§ì´ì˜ ì£¼ì¥ì²˜ëŸ¼ í”¼ë“œë°±ì„ í†µí•´ AIì˜ ì•„ì´ë””ì–´ë¥¼ ê°œì„ í•  ìˆ˜ ìˆë‹¤ë©´, ê·¸ ê³¼ì •ì—ì„œ ì¸ê°„ì˜ ì°½ì˜ì„±ì€ ì–´ë–»ê²Œ ë³´ì™„ë˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”? íŠ¹íˆ, AIì˜ ì œì•ˆì´ ì¸ê°„ì˜ ê³ ìœ í•œ ì°½ì˜ì  ì‚¬ê³ ì™€ ì–´ë–»ê²Œ ì¡°í™”ë¥¼ ì´ë£¨ëŠ”ì§€, ë˜ëŠ” ëŒ€ì²´ë˜ëŠ”ì§€ì— ëŒ€í•œ ëª…í™•í•œ ì‚¬ë¡€ë‚˜ ë…¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤ ì¤‘ì¬ì: í¬ë§ì´ì˜ ì£¼ì¥ì—ì„œ í”¼ë“œë°±ì„ í†µí•œ AIì˜ ì•„ì´ë””ì–´ ê°œì„ ì€ ê¸ì •ì ì¸ í˜‘ë ¥ ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤. ì˜ì‹¬ì´ì˜ ìš”ì²­ì— ë”°ë¼, AIê°€ ì œê³µí•œ ì•„ì´ë””ì–´ë¥¼ ì¸ê°„ì´ ê²€í† í•˜ê³  ìˆ˜ì •í•˜ëŠ” ê³¼ì •ì„ í†µí•´ ì°½ì˜ì  ì‚¬ê³ ê°€ ì–´ë–»ê²Œ ë³´ì™„ë˜ëŠ”ì§€ë¥¼ ë…¼ì˜í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ìƒí˜¸ì‘ìš©ì´ ì°½ì˜ì  ê²°ê³¼ë¬¼ì˜ ì§ˆì„ ë†’ì´ê³ , ì¸ê°„ì˜ ë…ì°½ì„±ì„ ë”ìš± ë¶€ê°ì‹œí‚¬ ìˆ˜ ìˆëŠ” ê¸°íšŒë¥¼ ì œê³µí•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í† ë¡  ì‹¤í–‰\n",
    "topic = \"AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ ìˆì„ê¹Œ?\"\n",
    "final_conversation = run_discussion(topic, rounds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. LangChain ë§›ë³´ê¸°\n",
    "\n",
    "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChainì„ í†µí•œ ëª¨ë¸ í˜¸ì¶œ\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"=== LangChainì„ í†µí•œ GPT-4o-mini ===\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain ì²´ì¸ ì˜ˆì‹œ\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ {topic} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ˆë³´ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain = prompt | llm\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "response = chain.invoke({\"topic\": \"Python\", \"question\": \"ë°ì½”ë ˆì´í„°ê°€ ë­”ê°€ìš”?\"})\n",
    "print(\"=== LangChain ì²´ì¸ ===\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ë¡œì»¬ LLM (Ollama) ì‹¬í™”\n",
    "\n",
    "Ollamaë¡œ ë¡œì»¬ì—ì„œ ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Ollama ì„œë²„ ìƒíƒœ í™•ì¸\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/\", timeout=5)\n",
    "    print(\"âœ… Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡\n",
    "    tags_response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    if tags_response.status_code == 200:\n",
    "        models = tags_response.json().get(\"models\", [])\n",
    "        print(f\"\\nğŸ“¦ ì„¤ì¹˜ëœ ëª¨ë¸ ({len(models)}ê°œ):\")\n",
    "        for model in models[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ\n",
    "            size_gb = model.get(\"size\", 0) / (1024**3)\n",
    "            print(f\"   - {model['name']} ({size_gb:.1f}GB)\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   í„°ë¯¸ë„ì—ì„œ 'ollama serve' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama ëª¨ë¸ í˜¸ì¶œ (OpenAI í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = ollama_client.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is Python? One sentence.\"}]\n",
    "    )\n",
    "    print(\"=== Ollama (Llama 3.2) ===\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. ì‹¤ìŠµ: 3ê°œ LLM í† ë¡ \n",
    "\n",
    "OpenAI, Claude, Ollama ì„¸ ê°€ì§€ LLMì´ í† ë¡ í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "openai_client = OpenAI()\n",
    "claude_client = anthropic.Anthropic()\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "PROMPTS = {\n",
    "    \"openai\": \"You are OpenAI's representative. You tend to be optimistic about AI. Keep responses to 2-3 sentences.\",\n",
    "    \"claude\": \"You are Anthropic's representative. You emphasize AI safety. Keep responses to 2-3 sentences.\",\n",
    "    \"ollama\": \"You are an open-source advocate. You value transparency. Keep responses to 2-3 sentences.\"\n",
    "}\n",
    "\n",
    "def get_openai_response(conversation: str, topic: str) -> str:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": PROMPTS[\"openai\"]},\n",
    "            {\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_claude_response(conversation: str, topic: str) -> str:\n",
    "    response = claude_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=200,\n",
    "        system=PROMPTS[\"claude\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def get_ollama_response(conversation: str, topic: str) -> str:\n",
    "    try:\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=\"llama3.2\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": PROMPTS[\"ollama\"]},\n",
    "                {\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nConversation:\\n{conversation}\\n\\nYour turn:\"}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        return \"(Ollama not available)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ê°œ LLM í† ë¡  ì‹¤í–‰\n",
    "topic = \"The future of open-source AI models\"\n",
    "conversation = \"\"\n",
    "\n",
    "print(f\"ğŸ“¢ Topic: {topic}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for round_num in range(2):\n",
    "    print(f\"\\n--- Round {round_num + 1} ---\")\n",
    "    \n",
    "    # OpenAI\n",
    "    openai_reply = get_openai_response(conversation, topic)\n",
    "    conversation += f\"\\nOpenAI: {openai_reply}\"\n",
    "    print(f\"\\nğŸŸ¢ OpenAI: {openai_reply}\")\n",
    "    \n",
    "    # Claude\n",
    "    claude_reply = get_claude_response(conversation, topic)\n",
    "    conversation += f\"\\nClaude: {claude_reply}\"\n",
    "    print(f\"\\nğŸŸ  Claude: {claude_reply}\")\n",
    "    \n",
    "    # Ollama\n",
    "    ollama_reply = get_ollama_response(conversation, topic)\n",
    "    conversation += f\"\\nOllama: {ollama_reply}\"\n",
    "    print(f\"\\nğŸ”µ Ollama: {ollama_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ì´ë²ˆ ì‹œë¦¬ì¦ˆì—ì„œ í•™ìŠµí•œ ë‚´ìš©\n",
    "\n",
    "| Part | ì£¼ìš” ë‚´ìš© |\n",
    "|------|----------|\n",
    "| **Part 1** | API ì†Œê°œ, í™˜ê²½ì„¤ì •, ë©”ì‹œì§€ êµ¬ì¡°, ê¸°ë³¸ í˜¸ì¶œ, í™œìš© ì˜ˆì‹œ |\n",
    "| **Part 2** | íŒŒë¼ë¯¸í„°, ìŠ¤íŠ¸ë¦¬ë°, ì—ëŸ¬ì²˜ë¦¬, ë‹¤ì¤‘ LLM, ë¹„ìš© ê³„ì‚° |\n",
    "| **Part 3** | ëŒ€í™” ì´ë ¥, ìºì‹±, LiteLLM, ë‹¤ì¤‘ ì—ì´ì „íŠ¸, LangChain |\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„ë¡œ ë°°ìš¸ ë‚´ìš©\n",
    "\n",
    "| ì£¼ì œ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **Function Calling** | LLMì´ ì™¸ë¶€ ë„êµ¬/APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ë²• |\n",
    "| **RAG** | ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ ìµœì‹  ì •ë³´ í™œìš© |\n",
    "| **Agent** | ììœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” AI ì—ì´ì „íŠ¸ |\n",
    "| **Fine-tuning** | íŠ¹ì • ë„ë©”ì¸ì— ë§ê²Œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • |\n",
    "| **Prompt Engineering** | ë” íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ì‘ì„± ê¸°ë²• |\n",
    "\n",
    "### ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "1. `ChatSession` í´ë˜ìŠ¤ì— í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ë° ë¹„ìš© ê³„ì‚° ê¸°ëŠ¥ì„ ì¶”ê°€í•´ë³´ì„¸ìš”.\n",
    "2. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì— 4ë²ˆì§¸ ì—ì´ì „íŠ¸(íŒ©íŠ¸ ì²´ì»¤)ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”.\n",
    "3. LiteLLMì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì‘ë‹µ ì‹œê°„ê³¼ ë¹„ìš©ì„ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
