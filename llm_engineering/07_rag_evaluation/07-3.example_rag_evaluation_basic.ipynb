{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# RAG 시스템 평가 (RAG Evaluation)\n",
    "\n",
    "이번 노트북에서는 **RAG 시스템을 평가하는 방법**을 학습합니다.\n",
    "\n",
    "## 학습 목표\n",
    "RAG 시스템의 성능을 정량적으로 측정하고 개선점을 찾는 방법을 익힙니다.\n",
    "\n",
    "## RAG 평가가 중요한 이유\n",
    "\n",
    "RAG 시스템은 두 가지 핵심 컴포넌트로 구성됩니다:\n",
    "1. **Retrieval (검색)**: 관련 문서를 얼마나 잘 찾는가?\n",
    "2. **Generation (생성)**: 찾은 문서를 바탕으로 얼마나 좋은 답변을 생성하는가?\n",
    "\n",
    "각 컴포넌트를 개별적으로 평가해야 문제점을 정확히 파악할 수 있습니다.\n",
    "\n",
    "## 오늘 배울 내용\n",
    "\n",
    "### 1. 검색 평가 지표 (Retrieval Metrics)\n",
    "- **MRR (Mean Reciprocal Rank)**: 정답이 몇 번째에 등장하는가?\n",
    "- **nDCG (Normalized Discounted Cumulative Gain)**: 관련 문서들의 순위 품질\n",
    "- **Keyword Coverage**: 필수 키워드가 검색 결과에 포함되는가?\n",
    "\n",
    "### 2. 답변 평가 방법 (Answer Evaluation)\n",
    "- **LLM-as-a-Judge**: LLM을 활용한 답변 품질 평가\n",
    "- 평가 기준: Accuracy, Completeness, Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "\n",
    "필요한 라이브러리를 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion\n",
    "\n",
    "# LangChain 관련\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 설정\n",
    "MODEL = \"gpt-4.1-nano\"\n",
    "DB_NAME = \"vector_db\"\n",
    "KNOWLEDGE_BASE = \"example/knowledge-base\"\n",
    "TEST_FILE = \"example/rag_evaluation_basic/tests.jsonl\"\n",
    "RETRIEVAL_K = 10\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-model-header",
   "metadata": {},
   "source": [
    "## 1. 테스트 데이터 구조 정의\n",
    "\n",
    "RAG 평가를 위해서는 **테스트 데이터셋**이 필요합니다.\n",
    "\n",
    "각 테스트 케이스는 다음 정보를 포함합니다:\n",
    "- `question`: RAG 시스템에 던질 질문\n",
    "- `keywords`: 검색 결과에 반드시 포함되어야 하는 키워드들\n",
    "- `reference_answer`: 정답 (Ground Truth)\n",
    "- `category`: 질문 유형 (direct_fact, temporal, spanning 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-question-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestQuestion(BaseModel):\n",
    "    \"\"\"RAG 평가용 테스트 질문 모델\"\"\"\n",
    "    \n",
    "    question: str = Field(description=\"RAG 시스템에 던질 질문\")\n",
    "    keywords: list[str] = Field(description=\"검색 결과에 포함되어야 하는 키워드들\")\n",
    "    reference_answer: str = Field(description=\"정답 (Ground Truth)\")\n",
    "    category: str = Field(description=\"질문 카테고리 (direct_fact, temporal, spanning 등)\")\n",
    "\n",
    "\n",
    "def load_tests() -> list[TestQuestion]:\n",
    "    \"\"\"JSONL 파일에서 테스트 케이스 로드\"\"\"\n",
    "    tests = []\n",
    "    with open(TEST_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            tests.append(TestQuestion(**data))\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드 및 확인\n",
    "tests = load_tests()\n",
    "print(f\"총 {len(tests)}개의 테스트 케이스 로드됨\")\n",
    "\n",
    "# 첫 번째 테스트 케이스 확인\n",
    "test = tests[0]\n",
    "print(f\"\\n[테스트 예시]\")\n",
    "print(f\"Question: {test.question}\")\n",
    "print(f\"Keywords: {test.keywords}\")\n",
    "print(f\"Reference Answer: {test.reference_answer}\")\n",
    "print(f\"Category: {test.category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-categories",
   "metadata": {},
   "source": [
    "### 테스트 카테고리 설명\n",
    "\n",
    "| 카테고리 | 설명 | 예시 |\n",
    "|---------|------|------|\n",
    "| `direct_fact` | 단일 문서에서 직접 찾을 수 있는 사실 | \"CEO의 연봉은?\" |\n",
    "| `temporal` | 시간/날짜 관련 질문 | \"언제 설립되었나?\" |\n",
    "| `spanning` | 여러 문서를 종합해야 하는 질문 | \"IIOTY 수상자가 담당하는 제품은?\" |\n",
    "| `numerical` | 숫자/통계 관련 질문 | \"직원 수는?\" |\n",
    "| `comparative` | 비교/분석 질문 | \"가장 비싼 요금제는?\" |\n",
    "| `holistic` | 전체적인 이해가 필요한 질문 | \"총 계약 건수는?\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "category-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리별 테스트 분포 확인\n",
    "from collections import Counter\n",
    "\n",
    "category_counts = Counter(test.category for test in tests)\n",
    "print(\"카테고리별 테스트 분포:\")\n",
    "for category, count in sorted(category_counts.items()):\n",
    "    print(f\"  {category}: {count}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-setup-header",
   "metadata": {},
   "source": [
    "## 2. 기본 RAG 시스템 구축\n",
    "\n",
    "평가 대상이 될 RAG 시스템을 LangChain으로 구축합니다.\n",
    "\n",
    "### 2.1 데이터 수집 (Ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ingest-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents():\n",
    "    \"\"\"Knowledge Base에서 문서 로드\"\"\"\n",
    "    folders = glob.glob(str(Path(KNOWLEDGE_BASE) / \"*\"))\n",
    "    documents = []\n",
    "    \n",
    "    for folder in folders:\n",
    "        doc_type = os.path.basename(folder)\n",
    "        loader = DirectoryLoader(\n",
    "            folder, \n",
    "            glob=\"**/*.md\", \n",
    "            loader_cls=TextLoader, \n",
    "            loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "        )\n",
    "        folder_docs = loader.load()\n",
    "        \n",
    "        for doc in folder_docs:\n",
    "            doc.metadata[\"doc_type\"] = doc_type\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def create_chunks(documents):\n",
    "    \"\"\"문서를 청크로 분할\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,      # 청크 크기\n",
    "        chunk_overlap=200    # 오버랩 크기\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vectorstore(chunks):\n",
    "    \"\"\"벡터 스토어 생성\"\"\"\n",
    "    if os.path.exists(DB_NAME):\n",
    "        Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=DB_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"벡터스토어 생성 완료: {vectorstore._collection.count()}개 문서\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-ingest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집 실행 (이미 생성되어 있다면 스킵 가능)\n",
    "# documents = fetch_documents()\n",
    "# print(f\"로드된 문서: {len(documents)}개\")\n",
    "\n",
    "# chunks = create_chunks(documents)\n",
    "# print(f\"생성된 청크: {len(chunks)}개\")\n",
    "\n",
    "# vectorstore = create_vectorstore(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answer-header",
   "metadata": {},
   "source": [
    "### 2.2 질문 응답 (Answer)\n",
    "\n",
    "벡터 스토어에서 관련 문서를 검색하고 LLM으로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answer-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 벡터스토어 로드\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"example/rag_evaluation_basic/vector_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": RETRIEVAL_K})\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
    "You are chatting with a user about Insurellm.\n",
    "If relevant, use the given context to answer any question.\n",
    "If you don't know the answer, say so.\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fetch_context(question: str) -> list[Document]:\n",
    "    \"\"\"질문과 관련된 문서 검색\"\"\"\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\n",
    "def answer_question(question: str, history: list[dict] = []) -> tuple[str, list[Document]]:\n",
    "    \"\"\"\n",
    "    RAG 방식으로 질문에 답변\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (답변 문자열, 검색된 문서 리스트)\n",
    "    \"\"\"\n",
    "    # 1. 관련 문서 검색\n",
    "    context_docs = fetch_context(question)\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in context_docs)\n",
    "    \n",
    "    # 2. 프롬프트 구성\n",
    "    system_message = SystemMessage(content=SYSTEM_PROMPT.format(context=context_text))\n",
    "    messages = [system_message, HumanMessage(content=question)]\n",
    "    \n",
    "    # 3. LLM 호출\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return response.content, context_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 시스템 테스트\n",
    "test_question = \"Who won the IIOTY award in 2023?\"\n",
    "answer, docs = answer_question(test_question)\n",
    "\n",
    "print(f\"질문: {test_question}\")\n",
    "print(f\"\\n답변: {answer}\")\n",
    "print(f\"\\n검색된 문서 수: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-eval-header",
   "metadata": {},
   "source": [
    "## 3. 검색 평가 (Retrieval Evaluation)\n",
    "\n",
    "검색 품질을 측정하는 핵심 지표들을 구현합니다.\n",
    "\n",
    "### 3.1 MRR (Mean Reciprocal Rank)\n",
    "\n",
    "**정의**: 정답 문서가 검색 결과에서 몇 번째에 등장하는지 측정\n",
    "\n",
    "**계산 방법**:\n",
    "- 정답이 1번째: 1/1 = 1.0\n",
    "- 정답이 2번째: 1/2 = 0.5\n",
    "- 정답이 3번째: 1/3 = 0.33\n",
    "- 정답이 없음: 0\n",
    "\n",
    "**해석**:\n",
    "- MRR = 1.0: 항상 첫 번째에 정답 (완벽)\n",
    "- MRR ≥ 0.9: 우수\n",
    "- MRR ≥ 0.75: 양호\n",
    "- MRR < 0.75: 개선 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mrr-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(keyword: str, retrieved_docs: list) -> float:\n",
    "    \"\"\"\n",
    "    단일 키워드에 대한 MRR 계산\n",
    "    \n",
    "    Args:\n",
    "        keyword: 검색 결과에서 찾아야 하는 키워드\n",
    "        retrieved_docs: 검색된 문서 리스트\n",
    "        \n",
    "    Returns:\n",
    "        Reciprocal Rank (정답 순위의 역수)\n",
    "    \"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    \n",
    "    for rank, doc in enumerate(retrieved_docs, start=1):\n",
    "        if keyword_lower in doc.page_content.lower():\n",
    "            return 1.0 / rank  # 첫 번째 발견 위치의 역수\n",
    "    \n",
    "    return 0.0  # 키워드를 찾지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mrr-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR 예시\n",
    "test = tests[0]  # \"Who won the IIOTY award in 2023?\"\n",
    "docs = fetch_context(test.question)\n",
    "\n",
    "print(f\"질문: {test.question}\")\n",
    "print(f\"찾아야 할 키워드: {test.keywords}\")\n",
    "print()\n",
    "\n",
    "for keyword in test.keywords:\n",
    "    mrr = calculate_mrr(keyword, docs)\n",
    "    print(f\"  '{keyword}' MRR: {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ndcg-header",
   "metadata": {},
   "source": [
    "### 3.2 nDCG (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "**정의**: 검색 결과의 전체적인 순위 품질을 측정\n",
    "\n",
    "**MRR과의 차이**:\n",
    "- MRR: 첫 번째 정답 위치만 고려\n",
    "- nDCG: 모든 관련 문서의 위치를 고려\n",
    "\n",
    "**계산 방법**:\n",
    "1. DCG = Σ(relevance_i / log2(rank_i + 1))\n",
    "2. IDCG = 이상적인 순서일 때의 DCG\n",
    "3. nDCG = DCG / IDCG\n",
    "\n",
    "**해석**:\n",
    "- nDCG = 1.0: 완벽한 순위 (관련 문서가 모두 상위에 위치)\n",
    "- nDCG ≥ 0.9: 우수\n",
    "- nDCG < 0.75: 개선 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ndcg-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dcg(relevances: list[int], k: int) -> float:\n",
    "    \"\"\"Discounted Cumulative Gain 계산\"\"\"\n",
    "    dcg = 0.0\n",
    "    for i in range(min(k, len(relevances))):\n",
    "        # 순위가 낮을수록 (뒤에 있을수록) 가치가 할인됨\n",
    "        dcg += relevances[i] / math.log2(i + 2)  # i+2: rank 1부터 시작\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def calculate_ndcg(keyword: str, retrieved_docs: list, k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    단일 키워드에 대한 nDCG 계산 (Binary Relevance)\n",
    "    \n",
    "    Args:\n",
    "        keyword: 찾아야 하는 키워드\n",
    "        retrieved_docs: 검색된 문서 리스트\n",
    "        k: 상위 k개 문서만 고려\n",
    "        \n",
    "    Returns:\n",
    "        Normalized DCG 값 (0~1)\n",
    "    \"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    \n",
    "    # Binary relevance: 키워드 포함 시 1, 아니면 0\n",
    "    relevances = [\n",
    "        1 if keyword_lower in doc.page_content.lower() else 0\n",
    "        for doc in retrieved_docs[:k]\n",
    "    ]\n",
    "    \n",
    "    # 실제 DCG\n",
    "    dcg = calculate_dcg(relevances, k)\n",
    "    \n",
    "    # 이상적인 DCG (관련 문서가 모두 앞에 있을 때)\n",
    "    ideal_relevances = sorted(relevances, reverse=True)\n",
    "    idcg = calculate_dcg(ideal_relevances, k)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ndcg-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nDCG 예시\n",
    "test = tests[0]\n",
    "docs = fetch_context(test.question)\n",
    "\n",
    "print(f\"질문: {test.question}\")\n",
    "print()\n",
    "\n",
    "for keyword in test.keywords:\n",
    "    ndcg = calculate_ndcg(keyword, docs)\n",
    "    print(f\"  '{keyword}' nDCG: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-eval-complete",
   "metadata": {},
   "source": [
    "### 3.3 검색 평가 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval-eval-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEval(BaseModel):\n",
    "    \"\"\"검색 평가 결과 모델\"\"\"\n",
    "    \n",
    "    mrr: float = Field(description=\"Mean Reciprocal Rank (평균)\")\n",
    "    ndcg: float = Field(description=\"Normalized DCG (평균)\")\n",
    "    keywords_found: int = Field(description=\"찾은 키워드 수\")\n",
    "    total_keywords: int = Field(description=\"전체 키워드 수\")\n",
    "    keyword_coverage: float = Field(description=\"키워드 커버리지 (%)\")\n",
    "\n",
    "\n",
    "def evaluate_retrieval(test: TestQuestion, k: int = 10) -> RetrievalEval:\n",
    "    \"\"\"\n",
    "    단일 테스트에 대한 검색 평가 수행\n",
    "    \n",
    "    Args:\n",
    "        test: 테스트 케이스\n",
    "        k: 상위 k개 문서 검색\n",
    "        \n",
    "    Returns:\n",
    "        RetrievalEval 결과 객체\n",
    "    \"\"\"\n",
    "    # 문서 검색\n",
    "    retrieved_docs = fetch_context(test.question)\n",
    "    \n",
    "    # 각 키워드에 대해 MRR, nDCG 계산\n",
    "    mrr_scores = [calculate_mrr(kw, retrieved_docs) for kw in test.keywords]\n",
    "    ndcg_scores = [calculate_ndcg(kw, retrieved_docs, k) for kw in test.keywords]\n",
    "    \n",
    "    # 평균 계산\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0.0\n",
    "    \n",
    "    # 키워드 커버리지\n",
    "    keywords_found = sum(1 for score in mrr_scores if score > 0)\n",
    "    total_keywords = len(test.keywords)\n",
    "    keyword_coverage = (keywords_found / total_keywords * 100) if total_keywords > 0 else 0.0\n",
    "    \n",
    "    return RetrievalEval(\n",
    "        mrr=avg_mrr,\n",
    "        ndcg=avg_ndcg,\n",
    "        keywords_found=keywords_found,\n",
    "        total_keywords=total_keywords,\n",
    "        keyword_coverage=keyword_coverage\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval-eval-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 평가 실행 예시\n",
    "test = tests[0]\n",
    "result = evaluate_retrieval(test)\n",
    "\n",
    "print(f\"질문: {test.question}\")\n",
    "print(f\"\\n[검색 평가 결과]\")\n",
    "print(f\"  MRR: {result.mrr:.4f}\")\n",
    "print(f\"  nDCG: {result.ndcg:.4f}\")\n",
    "print(f\"  Keywords Found: {result.keywords_found}/{result.total_keywords}\")\n",
    "print(f\"  Keyword Coverage: {result.keyword_coverage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answer-eval-header",
   "metadata": {},
   "source": [
    "## 4. 답변 평가 (Answer Evaluation)\n",
    "\n",
    "### LLM-as-a-Judge\n",
    "\n",
    "LLM을 활용하여 생성된 답변의 품질을 평가하는 방법입니다.\n",
    "\n",
    "**평가 기준:**\n",
    "\n",
    "| 기준 | 설명 | 점수 기준 |\n",
    "|------|------|----------|\n",
    "| **Accuracy** | 사실적 정확성 | 1점(오답) ~ 5점(완벽) |\n",
    "| **Completeness** | 답변의 완전성 | 1점(불완전) ~ 5점(완전) |\n",
    "| **Relevance** | 질문과의 관련성 | 1점(무관) ~ 5점(직접 관련) |\n",
    "\n",
    "**왜 LLM-as-a-Judge인가?**\n",
    "- 자연어 답변은 정확히 일치하지 않아도 정답일 수 있음\n",
    "- 의미적 동등성을 사람처럼 판단 가능\n",
    "- 대규모 평가를 자동화할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answer-eval-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEval(BaseModel):\n",
    "    \"\"\"LLM-as-a-Judge 답변 평가 결과\"\"\"\n",
    "    \n",
    "    feedback: str = Field(\n",
    "        description=\"답변 품질에 대한 피드백\"\n",
    "    )\n",
    "    accuracy: float = Field(\n",
    "        description=\"사실적 정확도 (1~5점). 오답은 반드시 1점\"\n",
    "    )\n",
    "    completeness: float = Field(\n",
    "        description=\"답변의 완전성 (1~5점). 모든 정보 포함 시 5점\"\n",
    "    )\n",
    "    relevance: float = Field(\n",
    "        description=\"질문과의 관련성 (1~5점). 직접 답변 시 5점\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answer-eval-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(test: TestQuestion) -> tuple[AnswerEval, str, list]:\n",
    "    \"\"\"\n",
    "    LLM-as-a-Judge 방식으로 답변 품질 평가\n",
    "    \n",
    "    Args:\n",
    "        test: 테스트 케이스 (질문 + 정답)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (평가 결과, 생성된 답변, 검색된 문서들)\n",
    "    \"\"\"\n",
    "    # RAG로 답변 생성\n",
    "    generated_answer, retrieved_docs = answer_question(test.question)\n",
    "    \n",
    "    # LLM Judge 프롬프트\n",
    "    judge_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an expert evaluator assessing the quality of answers.\n",
    "Evaluate the generated answer by comparing it to the reference answer.\n",
    "Only give 5/5 scores for perfect answers.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Question:\n",
    "{test.question}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "Reference Answer:\n",
    "{test.reference_answer}\n",
    "\n",
    "Please evaluate the generated answer on three dimensions:\n",
    "1. Accuracy: How factually correct is it compared to the reference answer?\n",
    "2. Completeness: How thoroughly does it address all aspects of the question?\n",
    "3. Relevance: How well does it directly answer the specific question asked?\n",
    "\n",
    "Provide detailed feedback and scores from 1 (very poor) to 5 (ideal) for each dimension.\n",
    "If the answer is wrong, then the accuracy score must be 1.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # LLM Judge 호출 (Structured Output)\n",
    "    judge_response = completion(\n",
    "        model=MODEL, \n",
    "        messages=judge_messages, \n",
    "        response_format=AnswerEval\n",
    "    )\n",
    "    \n",
    "    answer_eval = AnswerEval.model_validate_json(\n",
    "        judge_response.choices[0].message.content\n",
    "    )\n",
    "    \n",
    "    return answer_eval, generated_answer, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answer-eval-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 평가 실행 예시\n",
    "test = tests[0]\n",
    "eval_result, generated, docs = evaluate_answer(test)\n",
    "\n",
    "print(f\"질문: {test.question}\")\n",
    "print(f\"\\n정답: {test.reference_answer}\")\n",
    "print(f\"\\n생성된 답변: {generated}\")\n",
    "print(f\"\\n[답변 평가 결과]\")\n",
    "print(f\"  Feedback: {eval_result.feedback}\")\n",
    "print(f\"  Accuracy: {eval_result.accuracy}/5\")\n",
    "print(f\"  Completeness: {eval_result.completeness}/5\")\n",
    "print(f\"  Relevance: {eval_result.relevance}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-eval-header",
   "metadata": {},
   "source": [
    "## 5. 전체 평가 실행\n",
    "\n",
    "모든 테스트 케이스에 대해 평가를 실행하고 결과를 집계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-retrieval-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_all_retrieval(tests: list[TestQuestion]) -> dict:\n",
    "    \"\"\"\n",
    "    전체 테스트에 대한 검색 평가 실행\n",
    "    \n",
    "    Returns:\n",
    "        집계된 평가 결과 딕셔너리\n",
    "    \"\"\"\n",
    "    total_mrr = 0.0\n",
    "    total_ndcg = 0.0\n",
    "    total_coverage = 0.0\n",
    "    category_results = {}\n",
    "    \n",
    "    for test in tqdm(tests, desc=\"검색 평가 중\"):\n",
    "        result = evaluate_retrieval(test)\n",
    "        \n",
    "        total_mrr += result.mrr\n",
    "        total_ndcg += result.ndcg\n",
    "        total_coverage += result.keyword_coverage\n",
    "        \n",
    "        # 카테고리별 집계\n",
    "        if test.category not in category_results:\n",
    "            category_results[test.category] = []\n",
    "        category_results[test.category].append(result.mrr)\n",
    "    \n",
    "    n = len(tests)\n",
    "    return {\n",
    "        \"avg_mrr\": total_mrr / n,\n",
    "        \"avg_ndcg\": total_ndcg / n,\n",
    "        \"avg_coverage\": total_coverage / n,\n",
    "        \"category_mrr\": {\n",
    "            cat: sum(scores) / len(scores) \n",
    "            for cat, scores in category_results.items()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-batch-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 검색 평가 실행 (시간이 걸릴 수 있음)\n",
    "# 샘플로 처음 10개만 평가\n",
    "sample_tests = tests[:10]\n",
    "retrieval_results = evaluate_all_retrieval(sample_tests)\n",
    "\n",
    "print(\"\\n[전체 검색 평가 결과]\")\n",
    "print(f\"  평균 MRR: {retrieval_results['avg_mrr']:.4f}\")\n",
    "print(f\"  평균 nDCG: {retrieval_results['avg_ndcg']:.4f}\")\n",
    "print(f\"  평균 Keyword Coverage: {retrieval_results['avg_coverage']:.1f}%\")\n",
    "\n",
    "print(\"\\n[카테고리별 MRR]\")\n",
    "for cat, mrr in retrieval_results['category_mrr'].items():\n",
    "    print(f\"  {cat}: {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpret-header",
   "metadata": {},
   "source": [
    "## 6. 결과 해석 및 개선 방향\n",
    "\n",
    "### 평가 결과 해석 가이드\n",
    "\n",
    "| 지표 | 우수 | 양호 | 개선 필요 |\n",
    "|------|------|------|----------|\n",
    "| MRR | ≥ 0.9 | ≥ 0.75 | < 0.75 |\n",
    "| nDCG | ≥ 0.9 | ≥ 0.75 | < 0.75 |\n",
    "| Coverage | ≥ 90% | ≥ 75% | < 75% |\n",
    "| Accuracy | ≥ 4.5 | ≥ 4.0 | < 4.0 |\n",
    "| Completeness | ≥ 4.5 | ≥ 4.0 | < 4.0 |\n",
    "| Relevance | ≥ 4.5 | ≥ 4.0 | < 4.0 |\n",
    "\n",
    "### 개선 방향\n",
    "\n",
    "**검색 품질이 낮을 때:**\n",
    "- 청크 크기 조정\n",
    "- 임베딩 모델 변경 (예: text-embedding-3-large)\n",
    "- Reranking 적용\n",
    "- Query Rewriting 적용\n",
    "\n",
    "**답변 품질이 낮을 때:**\n",
    "- 시스템 프롬프트 개선\n",
    "- 더 많은 컨텍스트 제공 (K 값 증가)\n",
    "- LLM 모델 업그레이드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 노트북에서 학습한 내용:\n",
    "\n",
    "### 1. RAG 평가의 필요성\n",
    "- 검색(Retrieval)과 생성(Generation)을 개별 평가\n",
    "- 문제점을 정확히 파악하여 개선 방향 도출\n",
    "\n",
    "### 2. 검색 평가 지표\n",
    "- **MRR**: 첫 번째 정답 위치의 역수\n",
    "- **nDCG**: 전체 순위 품질\n",
    "- **Keyword Coverage**: 필수 키워드 포함률\n",
    "\n",
    "### 3. 답변 평가 방법\n",
    "- **LLM-as-a-Judge**: LLM을 활용한 자동 평가\n",
    "- 평가 기준: Accuracy, Completeness, Relevance\n",
    "\n",
    "### 4. 다음 단계\n",
    "- 고급 RAG 기법 적용 (Reranking, Query Rewriting)\n",
    "- 평가 결과 비교를 통한 최적 설정 탐색\n",
    "- Gradio UI를 통한 평가 대시보드 구축"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
