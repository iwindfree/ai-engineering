{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c26c848",
   "metadata": {},
   "source": "# 고급 RAG 기법 (Advanced RAG Techniques)\n\n이번 노트북은 **07-1.rag_evaluation.ipynb**에서 이어지는 내용으로, **고급 RAG 기법**을 학습하고 동일한 테스트 데이터로 평가합니다.\n\n## 학습 목표\n기본 RAG(07-1)를 넘어서 실무에서 활용할 수 있는 고급 기법들을 익히고, **동일한 평가 프레임워크**로 성능을 비교합니다.\n\n## 오늘 배울 내용\n\n### Ingest (데이터 수집 및 전처리) 고급 기법\n1. **LangChain 없이 순수 Python으로 구현** - 최대한의 유연성을 위해 프레임워크 의존성 제거\n2. **LLM을 활용한 지능형 청킹** - 단순 문자 수 기반이 아닌, LLM이 문맥을 이해하여 의미 있는 단위로 분할\n3. **문서 전처리 (Document Pre-processing)** - LLM이 검색에 유리하도록 청크를 재작성\n\n### Retrieval (검색) 고급 기법\n4. **Reranking** - 검색 결과를 LLM이 재정렬하여 정확도 향상\n5. **Query Rewriting** - 사용자 질문을 검색에 최적화된 형태로 변환\n\n### Evaluation (평가)\n6. **07-1과 동일한 지표로 평가** - MRR, nDCG, Precision@K, Recall@K, LLM-as-a-Judge\n\n## 07-1 기본 RAG vs 07-4 고급 RAG\n\n| 구분 | 07-1 기본 RAG | 07-4 고급 RAG |\n|------|-------------|-------------|\n| 프레임워크 | LangChain | 순수 Python |\n| 청킹 | RecursiveCharacterTextSplitter (고정 크기) | LLM 기반 지능형 청킹 |\n| 검색 | 단순 벡터 유사도 | 벡터 유사도 + Reranking |\n| 쿼리 | 원본 질문 그대로 | Query Rewriting 적용 |\n| Knowledge Base | 동일 (`00_test_data/knowledge_base`) | 동일 |\n| 테스트 데이터 | 동일 (`tests.jsonl`) | 동일 |\n| 평가 지표 | MRR, nDCG, Precision, Recall, LLM Judge | 동일 |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f5f1d",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport math\nfrom pathlib import Path\nfrom collections import Counter\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom chromadb import PersistentClient\nfrom tqdm import tqdm\nfrom litellm import completion\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport plotly.graph_objects as go\n\n\nload_dotenv(override=True)\n\nMODEL = \"gpt-4.1-nano\"\n\nDB_NAME = \"advanced_vector_db\"\ncollection_name = \"docs\"\nembedding_model = \"text-embedding-3-large\"\nKNOWLEDGE_BASE_PATH = Path(\"../00_test_data/knowledge_base\")\nTEST_FILE = str(Path(\"../00_test_data/test_dataset/tests.jsonl\"))\nAVERAGE_CHUNK_SIZE = 500\nRETRIEVAL_K = 10\n\n# matplotlib 한글 설정\nplt.rcParams['font.family'] = 'AppleGothic'\nplt.rcParams['axes.unicode_minus'] = False\n\nopenai = OpenAI()\n\nprint(f\"모델: {MODEL}\")\nprint(f\"Knowledge Base: {KNOWLEDGE_BASE_PATH}\")\nprint(f\"벡터DB: {DB_NAME}\")\nprint(f\"테스트 파일: {TEST_FILE}\")"
  },
  {
   "cell_type": "markdown",
   "id": "35lnbxfdcc",
   "source": "## 테스트 데이터 로드\n\n07-1에서 사용한 것과 **동일한 테스트 데이터**(tests.jsonl)를 사용합니다.\n고급 RAG 기법 적용 후 동일한 평가 지표로 성능을 비교할 수 있습니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "66tl48lsw0m",
   "source": "class TestQuestion(BaseModel):\n    \"\"\"RAG 평가용 테스트 질문 모델 (07-1과 동일)\"\"\"\n    question: str = Field(description=\"RAG 시스템에 던질 질문\")\n    keywords: list[str] = Field(description=\"검색 결과에 포함되어야 하는 키워드들\")\n    reference_answer: str = Field(description=\"정답 (Ground Truth)\")\n    category: str = Field(description=\"질문 카테고리\")\n    source_docs: list[str] = Field(default_factory=list, description=\"정답이 포함된 원본 문서 경로들\")\n\n\ndef normalize_source(source_path: str) -> str:\n    \"\"\"청크 metadata의 source 경로를 정규화하여 knowledge_base/ 이하 상대 경로로 변환\"\"\"\n    if \"knowledge_base/\" in source_path:\n        return source_path.split(\"knowledge_base/\")[-1]\n    return source_path\n\n\ndef load_tests(filepath: str = TEST_FILE) -> list[TestQuestion]:\n    \"\"\"JSONL 파일에서 테스트 케이스 로드\"\"\"\n    tests = []\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            data = json.loads(line)\n            tests.append(TestQuestion(**data))\n    return tests\n\n\ntests = load_tests()\nprint(f\"총 {len(tests)}개의 테스트 케이스 로드됨\\n\")\n\n# 카테고리별 분포\ncategory_counts = Counter(t.category for t in tests)\nprint(\"카테고리별 분포:\")\nfor cat, count in sorted(category_counts.items()):\n    print(f\"  {cat}: {count}개\")\n\n# 첫 번째 테스트 확인\nt = tests[0]\nprint(f\"\\n[테스트 예시]\")\nprint(f\"  Question    : {t.question}\")\nprint(f\"  Keywords    : {t.keywords}\")\nprint(f\"  Answer      : {t.reference_answer}\")\nprint(f\"  Source Docs : {t.source_docs}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfac66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain의 Document 클래스에서 영감을 받아 유사한 구조 정의\n",
    "\n",
    "class Result(BaseModel):\n",
    "    page_content: str\n",
    "    metadata: dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54aq6urulh",
   "source": "### Structured Output이란?\n\nLLM의 응답을 **정해진 JSON 스키마**에 맞춰 받는 기능입니다.\n\n**일반 LLM 응답 (자유 텍스트):**\n```\n이 문서의 첫 번째 청크는 회사 개요이고, 두 번째 청크는...\n```\n→ 파싱이 어렵고, 형식이 매번 달라질 수 있음\n\n**Structured Output (JSON 스키마 강제):**\n```json\n{\n  \"chunks\": [\n    {\n      \"headline\": \"회사 개요\",\n      \"summary\": \"하늘여행사는 2008년에 설립된...\",\n      \"original_text\": \"# 회사 개요\\n## 주식회사 하늘여행사...\"\n    }\n  ]\n}\n```\n→ Pydantic 모델로 자동 파싱 가능, 안정적인 출력\n\n**사용 방법:**\n\n1. **Pydantic 모델 정의** — 원하는 출력 구조를 Python 클래스로 선언\n2. **`response_format=모델`** — LLM 호출 시 모델을 지정하면 해당 스키마의 JSON으로 응답\n3. **`model_validate_json()`** — 응답 JSON을 Pydantic 객체로 변환\n\n```python\n# 1. 스키마 정의\nclass Chunk(BaseModel):\n    headline: str = Field(description=\"제목\")\n    summary: str = Field(description=\"요약\")\n\nclass Chunks(BaseModel):\n    chunks: list[Chunk]\n\n# 2. LLM 호출 시 response_format 지정\nresponse = completion(model=MODEL, messages=messages, response_format=Chunks)\n\n# 3. JSON → Pydantic 객체로 파싱\nresult = Chunks.model_validate_json(response.choices[0].message.content)\n```\n\n**핵심 포인트:**\n- `Field(description=...)`의 설명이 LLM에게 각 필드를 어떻게 채울지 알려주는 역할\n- description이 상세할수록 LLM의 출력 품질이 높아짐\n- OpenAI, Anthropic 등 주요 LLM 프로바이더가 지원",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk를 표현하는 Pydantic 모델\n",
    "# LLM의 Structured Output을 위해 Field description을 상세히 작성\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    headline: str = Field(description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\")\n",
    "    summary: str = Field(description=\"A few sentences summarizing the content of this chunk to answer common questions\")\n",
    "    original_text: str = Field(description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\")\n",
    "\n",
    "    def as_result(self, document):\n",
    "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
    "        return Result(page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text,metadata=metadata)\n",
    "\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    chunks: list[Chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b64c1",
   "metadata": {},
   "source": [
    "## 데이터 수집 파이프라인 3단계\n",
    "\n",
    "RAG 시스템의 데이터 수집(Ingest) 과정은 다음 3단계로 구성됩니다:\n",
    "\n",
    "1. **문서 로드 (Fetch)** - Knowledge Base에서 문서들을 가져옵니다 (LangChain의 DirectoryLoader와 유사)\n",
    "2. **LLM 기반 청킹** - LLM을 호출하여 문서를 의미 있는 Chunk로 분할합니다\n",
    "3. **벡터 저장** - 생성된 Chunk들을 ChromaDB에 저장합니다\n",
    "\n",
    "이것이 전부입니다! 간단하죠?\n",
    "\n",
    "### Step 1: 문서 로드하기\n",
    "\n",
    "아래 함수는 LangChain의 `DirectoryLoader`를 순수 Python으로 구현한 것입니다.\n",
    "폴더 구조에서 문서 타입을 추출하고, 모든 마크다운 파일을 읽어옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5abdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents():\n",
    "    \"\"\"LangChain DirectoryLoader의 순수 Python 구현\"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for folder in KNOWLEDGE_BASE_PATH.iterdir():\n",
    "        doc_type = folder.name\n",
    "        for file in folder.rglob(\"*.md\"):\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                documents.append({\"type\": doc_type, \"source\": file.as_posix(), \"text\": f.read()})\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = fetch_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "za5bg094zl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 문서의 구조 확인\n",
    "print(f\"문서 개수: {len(documents)}\")\n",
    "print(f\"\\n첫 번째 문서 구조:\")\n",
    "print(f\"  - type: {documents[0]['type']}\")\n",
    "print(f\"  - source: {documents[0]['source']}\")\n",
    "print(f\"  - text 길이: {len(documents[0]['text'])} 글자\")\n",
    "print(f\"  - text 미리보기: {documents[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "du1g1z0d6fk",
   "metadata": {},
   "source": "### 문서(Document) 구조 이해하기\n\n`fetch_documents()` 함수가 반환하는 `documents`는 딕셔너리 리스트입니다.\n\n**각 문서의 구조:**\n\n```python\n{\n    \"type\": str,    # 문서 타입 (폴더 이름에서 추출)\n    \"source\": str,  # 파일 경로 (출처 추적용)\n    \"text\": str     # 파일 전체 내용\n}\n```\n\n**Knowledge Base 폴더 구조 (07-1과 동일):**\n\n```\n00_test_data/knowledge_base/\n├── company/            # type = \"company\"\n│   ├── overview.md\n│   └── history.md\n├── employees/          # type = \"employees\"\n│   ├── kim_haneul.md\n│   ├── park_minsoo.md\n│   ├── song_minji.md\n│   ├── kang_jihoon.md\n│   └── kwon_nara.md\n├── products/           # type = \"products\"\n│   ├── jeju_healing.md\n│   ├── japan_osaka.md\n│   ├── vietnam_danang.md\n│   └── maldives_honeymoon.md\n├── contracts/          # type = \"contracts\"\n│   ├── samsung_incentive.md\n│   └── hyundai_mice.md\n├── policies/           # type = \"policies\"\n│   ├── booking.md\n│   ├── cancellation.md\n│   └── insurance.md\n└── faq/                # type = \"faq\"\n    ├── general.md\n    └── product.md\n```\n\n**왜 이 구조인가?**\n\n| 필드 | 용도 |\n|-----|------|\n| `type` | 문서 분류, 필터링 검색, 시각화 색상 구분에 활용 |\n| `source` | 답변 시 출처 표시, 디버깅, 추적에 활용 |\n| `text` | LLM 청킹의 입력 데이터 |\n\n이 메타데이터는 나중에 ChromaDB에 저장되어 검색 결과와 함께 반환됩니다."
  },
  {
   "cell_type": "markdown",
   "id": "dffa1c68",
   "metadata": {},
   "source": [
    "### Step 2: LLM 기반 지능형 청킹\n",
    "\n",
    "이제 핵심인 **LLM 기반 청킹**을 구현합니다.\n",
    "\n",
    "기존 방식 vs LLM 기반 방식:\n",
    "\n",
    "| 기존 방식 | LLM 기반 방식 |\n",
    "|----------|--------------|\n",
    "| 고정 문자 수로 분할 | 의미 단위로 분할 |\n",
    "| 문맥 무시 | 문맥 이해 |\n",
    "| 단순 오버랩 | 지능적 오버랩 |\n",
    "| 원본 텍스트만 저장 | headline + summary + 원본 저장 |\n",
    "\n",
    "LLM에게 다음을 요청합니다:\n",
    "- **headline**: 검색에 유리한 짧은 제목\n",
    "- **summary**: 핵심 내용 요약\n",
    "- **original_text**: 원본 텍스트 (변경 없이)\n",
    "\n",
    "이렇게 하면 검색 시 headline과 summary가 매칭 확률을 높여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e4170",
   "metadata": {},
   "outputs": [],
   "source": "def make_prompt(document):\n    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n    return f\"\"\"\nYou take a document and you split the document into overlapping chunks for a KnowledgeBase.\n\nThe document is from the shared drive of a travel company called 하늘여행사 (Sky Travel).\nThe document is of type: {document[\"type\"]}\nThe document has been retrieved from: {document[\"source\"]}\n\nA chatbot will use these chunks to answer questions about the company.\nYou should divide up the document as you see fit, being sure that the entire document is returned in the chunks - don't leave anything out.\nThis document should probably be split into {how_many} chunks, but you can have more or less as appropriate.\nThere should be overlap between the chunks as appropriate; typically about 25% overlap or about 50 words, so you have the same text in multiple chunks for best retrieval results.\n\nFor each chunk, you should provide a headline, a summary, and the original text of the chunk.\nTogether your chunks should represent the entire document with overlap.\n\nHere is the document:\n\n{document[\"text\"]}\n\nRespond with the chunks.\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38103b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_prompt(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f58850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_messages(document):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab04779",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_messages(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9qxtvuyh5z8",
   "metadata": {},
   "source": [
    "### process_document 함수 상세 설명\n",
    "\n",
    "`process_document`는 이 노트북의 **핵심 함수**입니다. 단일 문서를 LLM에게 전달하여 지능형 청킹을 수행합니다.\n",
    "\n",
    "**함수 동작 흐름:**\n",
    "\n",
    "```\n",
    "문서 (Document)\n",
    "    ↓\n",
    "make_messages() - 프롬프트 생성\n",
    "    ↓\n",
    "LLM API 호출 (litellm.completion)\n",
    "    ↓\n",
    "Structured Output (JSON) 파싱\n",
    "    ↓\n",
    "Chunk 객체 리스트로 변환\n",
    "    ↓\n",
    "Result 객체 리스트 반환\n",
    "```\n",
    "\n",
    "**핵심 포인트:**\n",
    "\n",
    "1. **Structured Output 활용**\n",
    "   - `response_format=Chunks`를 지정하여 LLM이 정해진 JSON 스키마로 응답\n",
    "   - Pydantic 모델로 자동 검증 및 파싱\n",
    "\n",
    "2. **LLM이 결정하는 것들**\n",
    "   - 청크 분할 위치 (의미 단위로)\n",
    "   - 각 청크의 headline (검색용 제목)\n",
    "   - 각 청크의 summary (핵심 요약)\n",
    "   - 오버랩 범위 (문맥 보존을 위해)\n",
    "\n",
    "3. **왜 이 방식이 효과적인가?**\n",
    "   - 단순 문자 수 분할은 문장 중간에서 끊길 수 있음\n",
    "   - LLM은 문단, 주제, 논리적 구분을 이해함\n",
    "   - headline과 summary가 검색 매칭률을 높임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document):\n",
    "    \"\"\"\n",
    "    단일 문서를 LLM 기반 지능형 청킹으로 처리하는 핵심 함수\n",
    "    \n",
    "    Args:\n",
    "        document: dict - {\"type\": str, \"source\": str, \"text\": str} 형태의 문서\n",
    "        \n",
    "    Returns:\n",
    "        list[Result] - headline, summary, original_text가 결합된 Result 객체 리스트\n",
    "        \n",
    "    처리 과정:\n",
    "        1. make_messages()로 LLM 프롬프트 생성\n",
    "        2. litellm.completion()으로 LLM 호출 (Structured Output 사용)\n",
    "        3. Pydantic으로 JSON 응답을 Chunks 모델로 파싱\n",
    "        4. 각 Chunk를 Result 객체로 변환하여 반환\n",
    "    \"\"\"\n",
    "    # 1. 문서를 LLM 메시지 형식으로 변환\n",
    "    messages = make_messages(document)\n",
    "    \n",
    "    # 2. LLM 호출 - response_format으로 Pydantic 모델을 지정하면\n",
    "    #    LLM이 해당 스키마에 맞는 JSON으로 응답함 (Structured Output)\n",
    "    response = completion(model=MODEL, messages=messages, response_format=Chunks)\n",
    "    \n",
    "    # 3. LLM 응답에서 JSON 문자열 추출\n",
    "    reply = response.choices[0].message.content\n",
    "    \n",
    "    # 4. JSON을 Pydantic 모델로 파싱하여 Chunk 리스트 획득\n",
    "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "    \n",
    "    # 5. 각 Chunk를 Result 형태로 변환 (headline + summary + original_text 결합)\n",
    "    return [chunk.as_result(document) for chunk in doc_as_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480494d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_document(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccab1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm: 진행률 표시 라이브러리\n",
    "# - 반복문을 tqdm()으로 감싸면 자동으로 프로그레스 바 표시\n",
    "# - LLM API 호출은 시간이 오래 걸리므로 진행 상황 확인에 유용\n",
    "# - 예상 남은 시간(ETA), 처리 속도(it/s) 등도 함께 표시됨\n",
    "\n",
    "def create_chunks(documents):\n",
    "    \"\"\"모든 문서를 순회하며 LLM 기반 청킹 수행\"\"\"\n",
    "    chunks = []\n",
    "    for doc in tqdm(documents):  # tqdm(documents): 진행률 바와 함께 반복\n",
    "        chunks.extend(process_document(doc))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93115f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = create_chunks(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uip8hs5wc7e",
   "metadata": {},
   "source": [
    "**tqdm 출력 예시:**\n",
    "```\n",
    "100%|██████████| 25/25 [01:30<00:00,  3.62s/it]\n",
    "```\n",
    "\n",
    "| 표시 | 의미 |\n",
    "|-----|------|\n",
    "| `100%` | 진행률 |\n",
    "| `██████████` | 프로그레스 바 |\n",
    "| `25/25` | 완료/전체 개수 |\n",
    "| `01:30` | 경과 시간 |\n",
    "| `<00:00` | 예상 남은 시간 |\n",
    "| `3.62s/it` | 항목당 처리 시간 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750104c",
   "metadata": {},
   "source": [
    "### Step 2 완료! \n",
    "\n",
    "LLM 기반 청킹이 완료되었습니다. 다소 시간이 걸리지만, 결과의 품질은 훨씬 좋습니다.\n",
    "\n",
    "**성능 최적화 팁:**\n",
    "- 실제 프로덕션에서는 `multiprocessing.Pool`을 사용해 병렬 처리할 수 있습니다\n",
    "- Rate Limit 오류가 발생하면 병렬 처리를 끄거나 지연 시간을 추가하세요\n",
    "- 배치 처리로 API 호출 횟수를 줄일 수도 있습니다\n",
    "\n",
    "### Step 3: 임베딩 생성 및 벡터 DB 저장\n",
    "\n",
    "이제 생성된 청크들을 벡터로 변환하여 ChromaDB에 저장합니다.\n",
    "\n",
    "**이 단계에서 일어나는 일:**\n",
    "1. 기존 컬렉션이 있으면 삭제 (재생성)\n",
    "2. 모든 청크 텍스트를 임베딩 벡터로 변환\n",
    "3. ChromaDB 컬렉션에 벡터, 문서, 메타데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks):\n",
    "    chroma = PersistentClient(path=DB_NAME)\n",
    "    if collection_name in [c.name for c in chroma.list_collections()]:\n",
    "        chroma.delete_collection(collection_name)\n",
    "\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    emb = openai.embeddings.create(model=embedding_model, input=texts).data\n",
    "    vectors = [e.embedding for e in emb]\n",
    "\n",
    "    collection = chroma.get_or_create_collection(collection_name)\n",
    "\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    metas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metas)\n",
    "    print(f\"Vectorstore created with {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f52038",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf738d0",
   "metadata": {},
   "source": "# 벡터 시각화\n\n벡터 데이터베이스에 저장된 임베딩들을 시각화해봅시다.\n\n## t-SNE를 이용한 차원 축소\n\n고차원 임베딩 벡터(3072차원)를 2D/3D로 축소하여 시각화합니다.\n\n**t-SNE (t-distributed Stochastic Neighbor Embedding)란?**\n- 고차원 데이터의 구조를 저차원에서 보존하는 기법\n- 비슷한 데이터 포인트는 가깝게, 다른 데이터 포인트는 멀리 배치\n- 클러스터링 패턴을 시각적으로 확인할 수 있음\n\n색상 코드:\n- 파란색: products (여행 상품)\n- 녹색: employees (직원)\n- 빨간색: contracts (계약)\n- 주황색: company (회사 정보)\n- 보라색: policies (정책)\n- 갈색: faq (자주 묻는 질문)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318a46f",
   "metadata": {},
   "outputs": [],
   "source": "chroma = PersistentClient(path=DB_NAME)\ncollection = chroma.get_or_create_collection(collection_name)\nresult = collection.get(include=['embeddings', 'documents', 'metadatas'])\nvectors = np.array(result['embeddings'])\ndocuments = result['documents']\nmetadatas = result['metadatas']\ndoc_types = [metadata['type'] for metadata in metadatas]\n\n# 6가지 문서 타입에 대한 색상 매핑\ntype_color_map = {\n    'products': 'blue',\n    'employees': 'green',\n    'contracts': 'red',\n    'company': 'orange',\n    'policies': 'purple',\n    'faq': 'brown',\n}\ncolors = [type_color_map.get(t, 'gray') for t in doc_types]\n\nprint(f\"총 {len(vectors)}개의 벡터 로드됨\")\nfor t in sorted(set(doc_types)):\n    count = doc_types.count(t)\n    print(f\"  {t}: {count}개 ({type_color_map.get(t, 'gray')})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4683c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=10, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72b54a",
   "metadata": {},
   "source": [
    "## 고급 RAG 시스템 구축\n",
    "\n",
    "이제 본격적으로 **고급 RAG**를 구축합니다!\n",
    "\n",
    "### 적용할 고급 기법들:\n",
    "\n",
    "#### 1. Reranking (재순위화)\n",
    "- 벡터 유사도로 검색한 결과를 LLM이 다시 정렬\n",
    "- 단순 유사도보다 **의미적 관련성**을 더 정확하게 판단\n",
    "- 상위 K개 결과의 품질을 크게 향상시킴\n",
    "\n",
    "**Reranking이 필요한 이유:**\n",
    "- 임베딩 유사도는 \"비슷함\"을 측정하지만, \"관련성\"과는 다를 수 있음\n",
    "- 예: \"사과\"와 \"배\"는 유사하지만, \"사과 가격\"을 물으면 \"배 가격\" 문서는 관련 없음\n",
    "- LLM은 질문의 **의도**를 이해하여 더 정확한 순위 부여 가능\n",
    "\n",
    "#### 2. Query Rewriting (쿼리 재작성)\n",
    "- 사용자 질문을 검색에 최적화된 형태로 변환\n",
    "- 대화 맥락을 고려한 독립적인 쿼리 생성\n",
    "- 검색 정확도 향상\n",
    "\n",
    "**Query Rewriting 예시:**\n",
    "- 원본: \"그 사람 어디 대학 나왔어?\" \n",
    "- 재작성: \"직원 학력 대학교 출신\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankOrder(BaseModel):\n",
    "    order: list[int] = Field(\n",
    "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8446c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(question, chunks):\n",
    "    system_prompt = \"\"\"\n",
    "You are a document re-ranker.\n",
    "You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
    "The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance, but you may be able to improve on that.\n",
    "You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
    "Reply only with the list of ranked chunk ids, nothing else. Include all the chunk ids you are provided with, reranked.\n",
    "\"\"\"\n",
    "    user_prompt = f\"The user has asked the following question:\\n\\n{question}\\n\\nOrder all the chunks of text by relevance to the question, from most relevant to least relevant. Include all the chunk ids you are provided with, reranked.\\n\\n\"\n",
    "    user_prompt += \"Here are the chunks:\\n\\n\"\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        user_prompt += f\"# CHUNK ID: {index + 1}:\\n\\n{chunk.page_content}\\n\\n\"\n",
    "    user_prompt += \"Reply only with the list of ranked chunk ids, nothing else.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    response = completion(model=MODEL, messages=messages, response_format=RankOrder)\n",
    "    reply = response.choices[0].message.content\n",
    "    order = RankOrder.model_validate_json(reply).order\n",
    "    print(order)\n",
    "    return [chunks[i - 1] for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78048d",
   "metadata": {},
   "outputs": [],
   "source": "def fetch_context_unranked(question):\n    \"\"\"\n    질문을 벡터로 변환하여 ChromaDB에서 유사한 문서 검색\n    \n    openai.embeddings.create() 반환 구조:\n        CreateEmbeddingResponse(\n            data=[Embedding(embedding=[0.0023, -0.009, ...], index=0)],\n            model='text-embedding-3-large',\n            usage=Usage(prompt_tokens=5, total_tokens=5)\n        )\n    \n    .data[0].embedding → 질문의 의미를 표현하는 3072차원 벡터\n    \"\"\"\n    # 질문을 벡터로 변환 (질문의 의미를 숫자로 표현)\n    query = openai.embeddings.create(model=embedding_model, input=[question]).data[0].embedding\n    \n    # ChromaDB에서 질문 벡터와 유사한 문서 벡터 검색\n    results = collection.query(query_embeddings=[query], n_results=RETRIEVAL_K)\n    \n    chunks = []\n    for result in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n        chunks.append(Result(page_content=result[0], metadata=result[1]))\n    return chunks"
  },
  {
   "cell_type": "markdown",
   "id": "3b53f6de",
   "metadata": {},
   "source": [
    "### Reranking 테스트\n",
    "\n",
    "이제 Reranking이 실제로 검색 품질을 개선하는지 테스트해봅시다.\n",
    "\n",
    "**테스트 방법:**\n",
    "1. 질문으로 벡터 검색 수행 (unranked)\n",
    "2. 결과 순서 확인\n",
    "3. Reranking 적용\n",
    "4. 개선된 순서 확인\n",
    "\n",
    "**기대 결과:**\n",
    "- 질문과 직접 관련된 청크가 상위로 이동\n",
    "- 단순 키워드 매칭보다 의미적 관련성이 높은 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ed5be",
   "metadata": {},
   "outputs": [],
   "source": "question = \"하늘여행사는 어떤 상을 수상했나요?\"\nchunks = fetch_context_unranked(question)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23594f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in reranked:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405de4d0",
   "metadata": {},
   "outputs": [],
   "source": "question = \"몰디브 허니문 패키지의 가격은 얼마인가요?\"\nchunks = fetch_context_unranked(question)\nfor index, c in enumerate(chunks):\n    if \"몰디브\" in c.page_content:\n        print(f\"  {index}: 몰디브 관련 청크\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22948df7",
   "metadata": {},
   "outputs": [],
   "source": "for index, c in enumerate(reranked):\n    if \"몰디브\" in c.page_content:\n        print(f\"  {index}: 몰디브 관련 청크\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question):\n",
    "    chunks = fetch_context_unranked(question)\n",
    "    return rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547204c",
   "metadata": {},
   "outputs": [],
   "source": "SYSTEM_PROMPT = \"\"\"\nYou are a knowledgeable, friendly assistant representing the travel company 하늘여행사 (Sky Travel).\nYou are chatting with a user about 하늘여행사.\nYour answer will be evaluated for accuracy, relevance and completeness, so make sure it only answers the question and fully answers it.\nIf you don't know the answer, say so.\nFor context, here are specific extracts from the Knowledge Base that might be directly relevant to the user's question:\n{context}\n\nWith this context, please answer the user's question. Be accurate, relevant and complete.\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 메시지 생성 함수\n",
    "# 컨텍스트에 각 청크의 출처(source)를 포함하여 투명성 확보\n",
    "\n",
    "def make_rag_messages(question, history, chunks):\n",
    "    context = \"\\n\\n\".join(f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\" for chunk in chunks)\n",
    "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9491c",
   "metadata": {},
   "outputs": [],
   "source": "def rewrite_query(question, history=[]):\n    \"\"\"Rewrite the user's question to be a more specific question that is more likely to surface relevant content in the Knowledge Base.\"\"\"\n    message = f\"\"\"\nYou are in a conversation with a user, answering questions about the travel company 하늘여행사 (Sky Travel).\nYou are about to look up information in a Knowledge Base to answer the user's question.\n\nThis is the history of your conversation so far with the user:\n{history}\n\nAnd this is the user's current question:\n{question}\n\nRespond only with a single, refined question that you will use to search the Knowledge Base.\nIt should be a VERY short specific question most likely to surface content. Focus on the question details.\nDon't mention the company name unless it's a general question about the company.\nIMPORTANT: Respond ONLY with the knowledgebase query, nothing else.\n\"\"\"\n    response = completion(model=MODEL, messages=[{\"role\": \"system\", \"content\": message}])\n    return response.choices[0].message.content"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d050a1",
   "metadata": {},
   "outputs": [],
   "source": "rewrite_query(\"하늘여행사는 어떤 상을 수상했나요?\", [])"
  },
  {
   "cell_type": "markdown",
   "id": "yfsbnkgldqj",
   "metadata": {},
   "source": "### Query Rewriting의 주의점\n\nQuery Rewriting은 **양날의 검**입니다. 잘 작동하면 검색 품질이 향상되지만, 잘못되면 오히려 정확도가 떨어질 수 있습니다.\n\n**정확성을 떨어뜨리는 경우:**\n\n| 문제 | 예시 |\n|-----|------|\n| 고유명사 손실 | \"관광대상 수상\" → \"회사 수상 내역\" (관광대상이 사라짐) |\n| 과도한 일반화 | \"몰디브 허니문 가격\" → \"여행 상품 정보\" |\n| 의도 오해 | \"가격이 얼마야?\" → \"제품 정보\" (가격 키워드 누락) |\n| 환각 추가 | 없는 정보를 LLM이 추가 |\n| 맥락 오해석 | 대화 히스토리를 잘못 해석 |\n\n**해결 방법들:**\n\n1. **Multi-query RAG**: 원본 + 재작성 쿼리 둘 다로 검색\n2. **조건부 적용**: 단순 질문은 재작성 건너뛰기\n3. **품질 검증**: 재작성 결과가 원본과 너무 다르면 원본 사용\n4. **Fallback**: 검색 결과가 부실하면 원본으로 재시도\n5. **Query Expansion + Reranking 조합**: 원본 쿼리와 재작성된 쿼리 모두로 검색하여 청크를 수집한 뒤, 전체 결과를 Reranking으로 재정렬\n\n```\n┌─────────────────────────────────────────────────────────┐\n│  Query Expansion + Reranking 파이프라인                  │\n├─────────────────────────────────────────────────────────┤\n│                                                         │\n│  원본 쿼리: \"관광대상 수상 내역?\"                         │\n│       ↓                                                 │\n│  ┌─────────────┐     ┌─────────────┐                   │\n│  │ 원본 쿼리로  │     │ 재작성 쿼리로│                   │\n│  │ 검색        │     │ 검색        │                   │\n│  │ → 청크 A,B,C│     │ → 청크 D,E,F│                   │\n│  └─────────────┘     └─────────────┘                   │\n│       ↓                    ↓                           │\n│       └────────┬───────────┘                           │\n│                ↓                                        │\n│         중복 제거 후 병합                                │\n│         [A, B, C, D, E, F]                              │\n│                ↓                                        │\n│         Reranking (LLM)                                 │\n│                ↓                                        │\n│         최종 결과: [D, A, C, ...]                        │\n│                                                         │\n└─────────────────────────────────────────────────────────┘\n```\n\n이 방식은 원본 쿼리의 키워드 정확성과 재작성 쿼리의 의미 확장을 모두 활용할 수 있습니다.\n\n실무에서는 Query Rewriting을 **선택적으로** 적용하거나, A/B 테스트로 효과를 검증합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history: list[dict] = []) -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG and return the answer and the retrieved context\n",
    "    \"\"\"\n",
    "    query = rewrite_query(question, history)\n",
    "    print(query)\n",
    "    chunks = fetch_context(query)\n",
    "    messages = make_rag_messages(question, history, chunks)\n",
    "    response = completion(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab3e5f",
   "metadata": {},
   "outputs": [],
   "source": "answer_question(\"하늘여행사는 어떤 상을 수상했나요?\", [])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237722e",
   "metadata": {},
   "outputs": [],
   "source": "answer_question(\"몰디브 허니문 패키지의 가격은 얼마인가요?\", [])"
  },
  {
   "cell_type": "markdown",
   "id": "89trwwqv5rv",
   "source": "---\n\n## 07-1과 동일한 평가 프레임워크로 고급 RAG 평가\n\n이제 07-1에서 사용한 것과 **동일한 평가 지표**로 고급 RAG 시스템의 성능을 측정합니다.\n\n### 평가 구성\n\n| 평가 영역 | 지표 | 설명 |\n|-----------|------|------|\n| **검색 평가** | MRR | 첫 번째 정답 문서의 순위 역수 |\n| | nDCG | 전체 순위 품질 (상위에 정답 집중도) |\n| | Precision@K | K개 중 정답 문서 비율 |\n| | Recall@K | 전체 정답 중 K개 안에 찾은 비율 |\n| | Keyword Coverage | 핵심 키워드 포함 여부 |\n| **답변 평가** | Accuracy | 사실적 정확도 (1~5) |\n| | Completeness | 답변 완전성 (1~5) |\n| | Relevance | 질문 관련성 (1~5) |\n\n### 고급 RAG에서 달라지는 점\n\n07-1의 기본 RAG와 비교하여, 이 평가에서는 다음이 다릅니다:\n- **LLM 기반 청킹**: headline + summary가 추가되어 검색 매칭률 향상 기대\n- **Reranking**: 검색 결과 재정렬로 MRR/nDCG 향상 기대\n- **Query Rewriting**: 답변 생성 시 쿼리 최적화 (답변 품질 향상 기대)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tkz72oiysvm",
   "source": "# === 검색 평가 지표 함수 (07-1과 동일) ===\n\ndef calculate_mrr(source_docs: list[str], retrieved_docs: list) -> float:\n    \"\"\"문서 ID 기반 MRR(Reciprocal Rank) 계산\"\"\"\n    source_set = set(source_docs)\n    for rank, doc in enumerate(retrieved_docs, start=1):\n        if normalize_source(doc.metadata.get(\"source\", \"\")) in source_set:\n            return 1.0 / rank\n    return 0.0\n\n\ndef calculate_dcg(relevances: list[int], k: int) -> float:\n    \"\"\"Discounted Cumulative Gain 계산\"\"\"\n    dcg = 0.0\n    for i in range(min(k, len(relevances))):\n        dcg += relevances[i] / math.log2(i + 2)\n    return dcg\n\n\ndef calculate_ndcg(source_docs: list[str], retrieved_docs: list, k: int = 10) -> float:\n    \"\"\"문서 ID 기반 nDCG 계산\"\"\"\n    source_set = set(source_docs)\n    relevances = [\n        1 if normalize_source(doc.metadata.get(\"source\", \"\")) in source_set else 0\n        for doc in retrieved_docs[:k]\n    ]\n    dcg = calculate_dcg(relevances, k)\n    idcg = calculate_dcg(sorted(relevances, reverse=True), k)\n    return dcg / idcg if idcg > 0 else 0.0\n\n\ndef calculate_precision_at_k(source_docs: list[str], retrieved_docs: list, k: int) -> float:\n    \"\"\"Precision@K: 상위 K개 청크 중 정답 문서에서 온 청크의 비율\"\"\"\n    if k <= 0:\n        return 0.0\n    source_set = set(source_docs)\n    top_k = retrieved_docs[:k]\n    relevant = sum(\n        1 for doc in top_k\n        if normalize_source(doc.metadata.get(\"source\", \"\")) in source_set\n    )\n    return relevant / k\n\n\ndef calculate_recall_at_k(source_docs: list[str], retrieved_docs: list, k: int) -> float:\n    \"\"\"Recall@K: 정답 문서 중 상위 K개에서 1개라도 청크가 검색된 문서의 비율\"\"\"\n    if not source_docs:\n        return 1.0\n    source_set = set(source_docs)\n    found_sources = set(\n        normalize_source(doc.metadata.get(\"source\", \"\"))\n        for doc in retrieved_docs[:k]\n    )\n    found = len(source_set & found_sources)\n    return found / len(source_set)\n\n\nprint(\"검색 평가 지표 함수 정의 완료: MRR, nDCG, Precision@K, Recall@K\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "t5t7xa0zb1",
   "source": "# === 검색 평가 래퍼 + 답변 평가 (LLM-as-a-Judge) ===\n\nclass RetrievalEval(BaseModel):\n    \"\"\"검색 평가 결과 모델\"\"\"\n    mrr: float = Field(description=\"Mean Reciprocal Rank\")\n    ndcg: float = Field(description=\"Normalized DCG\")\n    precision_at_k: float = Field(description=\"Precision@K\")\n    recall_at_k: float = Field(description=\"Recall@K\")\n    keywords_found: int = Field(description=\"찾은 키워드 수\")\n    total_keywords: int = Field(description=\"전체 키워드 수\")\n    keyword_coverage: float = Field(description=\"키워드 커버리지 (0~1)\")\n\n\nclass AnswerEval(BaseModel):\n    \"\"\"LLM-as-a-Judge 답변 평가 결과\"\"\"\n    feedback: str = Field(description=\"답변 품질에 대한 피드백\")\n    accuracy: float = Field(description=\"사실적 정확도 (1~5점)\")\n    completeness: float = Field(description=\"답변의 완전성 (1~5점)\")\n    relevance: float = Field(description=\"질문과의 관련성 (1~5점)\")\n\n\ndef evaluate_retrieval(test: TestQuestion, k: int = RETRIEVAL_K) -> RetrievalEval:\n    \"\"\"\n    단일 테스트에 대한 검색 평가 수행 (Reranking 포함)\n    \n    검색 파이프라인: 벡터 유사도 검색 → Reranking → 평가\n    \"\"\"\n    retrieved_docs = fetch_context(test.question)\n\n    # 문서 ID 기반 지표\n    mrr = calculate_mrr(test.source_docs, retrieved_docs)\n    ndcg = calculate_ndcg(test.source_docs, retrieved_docs, k)\n    precision = calculate_precision_at_k(test.source_docs, retrieved_docs, k)\n    recall = calculate_recall_at_k(test.source_docs, retrieved_docs, k)\n\n    # 키워드 기반 보조 지표\n    all_content = \" \".join(doc.page_content.lower() for doc in retrieved_docs[:k])\n    keywords_found = sum(1 for kw in test.keywords if kw.lower() in all_content)\n    total_keywords = len(test.keywords)\n    coverage = keywords_found / total_keywords if total_keywords > 0 else 0.0\n\n    return RetrievalEval(\n        mrr=float(mrr), ndcg=float(ndcg),\n        precision_at_k=float(precision), recall_at_k=float(recall),\n        keywords_found=keywords_found, total_keywords=total_keywords,\n        keyword_coverage=coverage,\n    )\n\n\ndef evaluate_answer(test: TestQuestion) -> tuple[AnswerEval, str, list]:\n    \"\"\"LLM-as-a-Judge 방식으로 답변 품질 평가\"\"\"\n    generated_answer, retrieved_docs = answer_question(test.question)\n\n    judge_messages = [\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are an expert evaluator assessing the quality of answers.\\n\"\n                \"Evaluate the generated answer by comparing it to the reference answer.\\n\"\n                \"Only give 5/5 scores for perfect answers.\"\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Question:\\n{test.question}\\n\\n\"\n                f\"Generated Answer:\\n{generated_answer}\\n\\n\"\n                f\"Reference Answer:\\n{test.reference_answer}\\n\\n\"\n                \"Evaluate on three dimensions (1-5 each):\\n\"\n                \"1. Accuracy: factual correctness vs reference\\n\"\n                \"2. Completeness: covers all aspects of the question\\n\"\n                \"3. Relevance: directly answers the question\\n\"\n                \"If the answer is wrong, accuracy must be 1.\"\n            ),\n        },\n    ]\n\n    judge_response = completion(\n        model=MODEL, messages=judge_messages, response_format=AnswerEval,\n    )\n    answer_eval = AnswerEval.model_validate_json(judge_response.choices[0].message.content)\n    return answer_eval, generated_answer, retrieved_docs\n\n\nprint(\"평가 함수 정의 완료: evaluate_retrieval, evaluate_answer\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "mh6r7id0vi",
   "source": "# 단건 평가 테스트\ntest = tests[0]\n\n# 검색 평가\nr_eval = evaluate_retrieval(test)\nprint(f\"질문: {test.question}\")\nprint(f\"정답 문서: {test.source_docs}\")\nprint(f\"\\n[검색 평가 결과]\")\nprint(f\"  MRR: {r_eval.mrr:.4f}\")\nprint(f\"  nDCG: {r_eval.ndcg:.4f}\")\nprint(f\"  Precision@{RETRIEVAL_K}: {r_eval.precision_at_k:.4f}\")\nprint(f\"  Recall@{RETRIEVAL_K}: {r_eval.recall_at_k:.4f}\")\nprint(f\"  Keyword Coverage: {r_eval.keyword_coverage:.1%}\")\n\n# 답변 평가\nprint(f\"\\n--- 답변 평가 ---\")\na_eval, generated, _ = evaluate_answer(test)\nprint(f\"\\n정답: {test.reference_answer}\")\nprint(f\"생성된 답변: {generated}\")\nprint(f\"\\n[답변 평가 결과]\")\nprint(f\"  Accuracy: {a_eval.accuracy}/5\")\nprint(f\"  Completeness: {a_eval.completeness}/5\")\nprint(f\"  Relevance: {a_eval.relevance}/5\")\nprint(f\"  Feedback: {a_eval.feedback}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v1605zusaj",
   "source": "### 배치 평가 실행\n\n테스트 데이터 샘플에 대해 검색 + 답변 평가를 일괄 수행합니다.\n\n**주의**: 고급 RAG 파이프라인은 질문당 LLM 호출이 여러 번 발생합니다:\n1. Query Rewriting (1회)\n2. Reranking (1회)\n3. 답변 생성 (1회)\n4. LLM Judge 평가 (1회)\n\n따라서 07-1보다 평가 시간이 더 걸릴 수 있습니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8misl80gvme",
   "source": "def evaluate_all(\n    tests: list[TestQuestion],\n    run_answer_eval: bool = True,\n) -> list[dict]:\n    \"\"\"\n    전체 테스트에 대해 검색 + 답변 평가 수행\n\n    Args:\n        tests: 테스트 케이스 리스트\n        run_answer_eval: True면 LLM Judge도 실행\n\n    Returns:\n        결과 딕셔너리 리스트\n    \"\"\"\n    results = []\n\n    for test in tqdm(tests, desc=\"평가 중\"):\n        # 검색 평가 (Reranking 포함)\n        r_eval = evaluate_retrieval(test)\n\n        row = {\n            \"question\": test.question,\n            \"category\": test.category,\n            \"mrr\": r_eval.mrr,\n            \"ndcg\": r_eval.ndcg,\n            \"precision_at_k\": r_eval.precision_at_k,\n            \"recall_at_k\": r_eval.recall_at_k,\n            \"keyword_coverage\": r_eval.keyword_coverage,\n        }\n\n        # 답변 평가 (Query Rewriting + Reranking + LLM Judge)\n        if run_answer_eval:\n            a_eval, generated, _ = evaluate_answer(test)\n            row.update({\n                \"accuracy\": a_eval.accuracy,\n                \"completeness\": a_eval.completeness,\n                \"relevance\": a_eval.relevance,\n                \"feedback\": a_eval.feedback,\n                \"generated_answer\": generated,\n            })\n\n        results.append(row)\n\n    return results\n\n\n# 샘플 10건 평가 실행\nsample_tests = tests[:10]\nresults = evaluate_all(sample_tests)\n\nprint(f\"\\n평가 완료: {len(results)}건\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "udgeh7kqdv",
   "source": "import pandas as pd\n\n# 결과를 DataFrame으로 변환\ndf = pd.DataFrame(results)\n\n# 전체 평균\nmetrics = [\"mrr\", \"ndcg\", \"precision_at_k\", \"recall_at_k\", \"keyword_coverage\", \"accuracy\", \"completeness\", \"relevance\"]\navailable_metrics = [m for m in metrics if m in df.columns]\navgs = df[available_metrics].mean()\n\nprint(\"[고급 RAG 전체 평균]\")\nfor m in available_metrics:\n    if m in (\"accuracy\", \"completeness\", \"relevance\"):\n        print(f\"  {m:20s}: {avgs[m]:.2f}/5\")\n    else:\n        print(f\"  {m:20s}: {avgs[m]:.4f}\")\n\n# 바 차트\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\n# 검색 지표\nretrieval_metrics = [m for m in [\"mrr\", \"ndcg\", \"precision_at_k\", \"recall_at_k\", \"keyword_coverage\"] if m in avgs.index]\nbar_colors = [\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\", \"#CCB974\"]\naxes[0].bar(retrieval_metrics, [avgs[m] for m in retrieval_metrics], color=bar_colors[:len(retrieval_metrics)])\naxes[0].set_ylim(0, 1.1)\naxes[0].set_title(\"고급 RAG - 검색 평가 지표 평균\")\naxes[0].axhline(y=0.75, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"양호 기준 (0.75)\")\naxes[0].legend()\naxes[0].tick_params(axis='x', rotation=15)\nfor i, m in enumerate(retrieval_metrics):\n    axes[0].text(i, avgs[m] + 0.02, f\"{avgs[m]:.3f}\", ha=\"center\")\n\n# 답변 지표\nanswer_metrics = [m for m in [\"accuracy\", \"completeness\", \"relevance\"] if m in avgs.index]\nif answer_metrics:\n    axes[1].bar(answer_metrics, [avgs[m] for m in answer_metrics], color=[\"#4C72B0\", \"#55A868\", \"#C44E52\"])\n    axes[1].set_ylim(0, 5.5)\n    axes[1].set_title(\"고급 RAG - 답변 평가 지표 평균\")\n    axes[1].axhline(y=4.0, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"양호 기준 (4.0)\")\n    axes[1].legend()\n    for i, m in enumerate(answer_metrics):\n        axes[1].text(i, avgs[m] + 0.1, f\"{avgs[m]:.2f}\", ha=\"center\")\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3nqi0z7z0gk",
   "source": "# 카테고리별 분석\nprint(\"[카테고리별 평균 점수]\\n\")\n\ncat_group = df.groupby(\"category\")[available_metrics].mean()\n\nfor cat in cat_group.index:\n    row = cat_group.loc[cat]\n    print(f\"  {cat}:\")\n    print(f\"    MRR={row['mrr']:.3f}  nDCG={row['ndcg']:.3f}  P@K={row['precision_at_k']:.3f}  R@K={row['recall_at_k']:.3f}  Coverage={row['keyword_coverage']:.1%}\", end=\"\")\n    if \"accuracy\" in row.index:\n        print(f\"  Acc={row['accuracy']:.1f}  Comp={row['completeness']:.1f}  Rel={row['relevance']:.1f}\")\n    else:\n        print()\n\n# 카테고리별 MRR 바 차트\nfig, ax = plt.subplots(figsize=(8, 4))\ncat_mrr = df.groupby(\"category\")[\"mrr\"].mean().sort_values(ascending=True)\ncat_mrr.plot.barh(ax=ax, color=\"#4C72B0\")\nax.set_xlabel(\"평균 MRR\")\nax.set_title(\"고급 RAG - 카테고리별 평균 MRR\")\nax.axvline(x=0.75, color=\"gray\", linestyle=\"--\", alpha=0.5)\nax.set_xlim(0, 1.05)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "64c93b30",
   "metadata": {},
   "source": "## 정리\n\n이번 노트북에서 학습한 고급 RAG 기법들:\n\n### 1. LLM 기반 지능형 청킹\n- 고정 크기 분할 대신 LLM이 의미 단위로 분할\n- headline + summary + original_text 구조로 검색 품질 향상\n- Pydantic을 활용한 Structured Output\n\n### 2. Reranking (재순위화)\n- 벡터 유사도 검색 후 LLM이 재정렬\n- 의미적 관련성 기반 순위 부여\n- 검색 정확도 향상 (MRR, nDCG 개선)\n\n### 3. Query Rewriting (쿼리 재작성)\n- 사용자 질문을 검색 최적화 형태로 변환\n- 대화 맥락 고려\n- 검색 정확도 향상\n\n### 4. 07-1과 동일한 평가 프레임워크로 비교\n- 동일한 Knowledge Base (`00_test_data/knowledge_base`)와 테스트 데이터 (`tests.jsonl`) 사용\n- 동일한 평가 지표 (MRR, nDCG, Precision@K, Recall@K, LLM-as-a-Judge)\n- 기본 RAG 대비 고급 RAG의 성능 차이를 정량적으로 확인\n\n### 다음 단계\n- HyDE (Hypothetical Document Embeddings)\n- Multi-query RAG\n- RAG Fusion\n- Self-RAG"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}