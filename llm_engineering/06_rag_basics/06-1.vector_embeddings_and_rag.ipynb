{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Embeddings와 RAG 기초\n",
    "\n",
    "이번 노트북에서는 현대 AI 시스템의 핵심 기술인 **벡터 임베딩(Vector Embeddings)** 과 **RAG(Retrieval-Augmented Generation)** 에 대해 알아봅니다.\n",
    "\n",
    "## 개요\n",
    "\n",
    "| 주제 | 내용 |\n",
    "|------|------|\n",
    "| 벡터 임베딩 | 텍스트를 고차원 벡터로 변환하는 기술 |\n",
    "| 유사도 계산 | 벡터 간 의미적 유사성 측정 |\n",
    "| RAG | 외부 지식을 활용한 LLM 응답 생성 |\n",
    "| 실전 활용 | 문서 검색과 질의응답 시스템 구축 |\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "1. 벡터 임베딩의 개념과 작동 원리 이해하기\n",
    "2. OpenAI Embeddings API를 사용하여 텍스트를 벡터로 변환하기\n",
    "3. 코사인 유사도를 활용한 의미적 유사성 계산하기\n",
    "4. RAG의 개념과 필요성 이해하기\n",
    "5. 간단한 RAG 시스템 구현하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 벡터 임베딩(Vector Embeddings)이란?\n",
    "\n",
    "### 개념\n",
    "\n",
    "벡터 임베딩은 텍스트, 이미지, 오디오 등의 데이터를 **고차원 벡터 공간의 점**으로 표현하는 기술입니다. 의미가 비슷한 데이터는 벡터 공간에서 가까운 위치에 배치됩니다.\n",
    "\n",
    "### 왜 필요한가?\n",
    "\n",
    "- **의미적 검색**: 키워드가 아닌 의미로 검색 가능\n",
    "- **유사도 계산**: 두 텍스트의 의미적 유사성을 수치로 측정\n",
    "- **클러스터링**: 비슷한 내용을 자동으로 그룹화\n",
    "- **추천 시스템**: 사용자 취향과 유사한 콘텐츠 추천\n",
    "\n",
    "### 작동 원리\n",
    "\n",
    "```\n",
    "\"강아지가 공원에서 뛰어놀고 있다\" → [0.2, -0.5, 0.8, ..., 0.3]  (1536차원 벡터)\n",
    "\"개가 산책하고 있어요\"            → [0.21, -0.48, 0.82, ..., 0.29]\n",
    "\"주식 시장이 하락했다\"            → [-0.7, 0.3, -0.1, ..., 0.9]\n",
    "```\n",
    "\n",
    "위의 첫 두 문장은 의미가 비슷하므로 벡터 공간에서 가까운 위치에 놓입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 임베딩 모델(Embedding Model)의 이해\n",
    "\n",
    "### 임베딩 모델이란?\n",
    "\n",
    "임베딩 모델은 **텍스트를 벡터로 변환하는 신경망 모델**입니다. 대부분의 현대 임베딩 모델은 **Transformer 아키텍처** (BERT, RoBERTa 등)를 기반으로 합니다. 개발자 관점에서 임베딩 모델은 텍스트를 ‘의미 좌표’로 바꿔서\n",
    "검색, 비교, 분류를 가능하게 하는 핵심 요소입니다.\n",
    "\n",
    "```\n",
    "텍스트 입력 → [Transformer 인코더] → 고차원 벡터 출력\n",
    "                    ↓\n",
    "            문맥을 이해하고\n",
    "            의미를 압축하여\n",
    "            수치로 표현\n",
    "```\n",
    "\n",
    "### 임베딩 모델의 종류\n",
    "\n",
    "| 유형 | 설명 | 적합한 용도 | 예시 |\n",
    "|------|------|-------------|------|\n",
    "| **대칭형 (Symmetric)** | 입력 쌍이 동등한 형태 | 문장-문장 유사도, 중복 탐지 | sentence-transformers |\n",
    "| **비대칭형 (Asymmetric)** | 쿼리와 문서가 다른 형태 | 질문-문서 검색, RAG | E5, BGE 시리즈 |\n",
    "\n",
    "### 임베딩 모델 선택 시 고려사항\n",
    "\n",
    "1. **차원 수 (Dimensions)**\n",
    "   - 높은 차원: 더 많은 정보 표현 가능, 저장 공간 증가\n",
    "   - 낮은 차원: 빠른 검색, 저장 공간 절약\n",
    "   - 일반적으로 384 ~ 3072 차원 사용\n",
    "\n",
    "2. **다국어 지원**\n",
    "   - 영어 전용 모델: 영어 텍스트에서 최고 성능\n",
    "   - 다국어 모델: 한국어 포함 여러 언어 지원\n",
    "\n",
    "3. **속도 vs 품질**\n",
    "   - 작은 모델: 빠르지만 정확도 낮음\n",
    "   - 큰 모델: 느리지만 높은 품질\n",
    "\n",
    "4. **비용 (API vs 로컬)**\n",
    "   - API 모델: 쉬운 사용, 종량제 비용\n",
    "   - 로컬 모델: 초기 설정 필요, 무료 사용\n",
    "\n",
    "### 인기 임베딩 모델 비교\n",
    "\n",
    "| 모델 | 제공자 | 차원 | 다국어 | 특징 |\n",
    "|------|--------|------|--------|------|\n",
    "| text-embedding-3-small | OpenAI | 1536 | ✅ | 빠르고 저렴 |\n",
    "| text-embedding-3-large | OpenAI | 3072 | ✅ | 고품질 |\n",
    "| all-MiniLM-L6-v2 | HF | 384 | ❌ | 빠름, 무료 |\n",
    "| multilingual-e5-large | HF | 1024 | ✅ | 다국어 검색에 강함 |\n",
    "| BAAI/bge-m3 | HF | 1024 | ✅ | 최신 고성능 다국어 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 필요한 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai numpy python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 로드\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"❌ API key not found\")\n",
    "else:\n",
    "    print(\"✅ API key loaded\")\n",
    "    \n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OpenAI Embeddings API 사용하기\n",
    "\n",
    "### 임베딩 모델\n",
    "\n",
    "OpenAI는 여러 임베딩 모델을 제공합니다:\n",
    "\n",
    "| 모델 | 차원 | 성능 | 비용 |\n",
    "|------|------|------|------|\n",
    "| text-embedding-3-small | 1536 | 빠르고 저렴 | 낮음 |\n",
    "| text-embedding-3-large | 3072 | 높은 정확도 | 높음 |\n",
    "\n",
    "이번 실습에서는 `text-embedding-3-small`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트: 강아지가 공원에서 뛰어놀고 있다\n",
      "임베딩 벡터 차원: 1536\n",
      "벡터의 첫 10개 값: [0.018770838156342506, 0.010894719511270523, 0.0013781038578599691, 0.011866223067045212, 0.024235546588897705, 0.013375523500144482, 0.005152438767254353, 0.007425063289701939, -0.016654348000884056, -0.018128952011466026]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 간단한 텍스트를 벡터로 변환\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"텍스트를 벡터로 변환하는 함수\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 예시 텍스트\n",
    "text = \"강아지가 공원에서 뛰어놀고 있다\"\n",
    "embedding = get_embedding(text)\n",
    "\n",
    "print(f\"원본 텍스트: {text}\")\n",
    "print(f\"임베딩 벡터 차원: {len(embedding)}\")\n",
    "print(f\"벡터의 첫 10개 값: {embedding[:10]}\")\n",
    "print(type(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 텍스트 임베딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 6개의 문장을 임베딩했습니다.\n",
      "각 임베딩 벡터의 차원: 1536\n"
     ]
    }
   ],
   "source": [
    "# 여러 문장 임베딩\n",
    "sentences = [\n",
    "    \"강아지가 공원에서 뛰어놀고 있다\",\n",
    "    \"고양이가 공원에서 뛰어놀고 있다\",\n",
    "    \"개가 산책하고 있어요\",\n",
    "    \"고양이가 소파에서 자고 있다\",\n",
    "    \"주식 시장이 하락했다\",\n",
    "    \"경제 뉴스가 발표되었다\"\n",
    "]\n",
    "\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"총 {len(embeddings)}개의 문장을 임베딩했습니다.\")\n",
    "print(f\"각 임베딩 벡터의 차원: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Hugging Face 오픈소스 임베딩 모델\n",
    "\n",
    "OpenAI API 외에도 **무료 오픈소스 임베딩 모델**을 로컬에서 실행할 수 있습니다. `sentence-transformers` 라이브러리를 사용하면 Hugging Face의 다양한 모델을 쉽게 활용할 수 있습니다.\n",
    "\n",
    "### 인기 오픈소스 모델\n",
    "\n",
    "| 모델 | 차원 | 언어 | 특징 |\n",
    "|------|------|------|------|\n",
    "| `all-MiniLM-L6-v2` | 384 | 영어 | 빠르고 가벼움, 입문용으로 적합 |\n",
    "| `paraphrase-multilingual-MiniLM-L12-v2` | 384 | 다국어 | 50+ 언어 지원, 한국어 포함 |\n",
    "| `BAAI/bge-m3` | 1024 | 다국어 | 최신 고성능 모델, 검색에 강함 |\n",
    "| `intfloat/multilingual-e5-large` | 1024 | 다국어 | 다국어 검색 벤치마크 상위 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-transformers 라이브러리 설치\n",
    "%pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d5223339674cea90bd677c89c0e41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd67c310cd534caa813df7e7c0435c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd5ab54f77649619dc58bb325d01ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685f6d2738c94247bac2619fa3d97432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf00a3f03cc49f4ba7b19fccc41399e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6542012e2af34a3db83d0ec38c93973a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e1be5ff24848919de16c82e521f554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a83c6da0fc4afeae8370a4782d3060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62530580e3ca45288bf9bf90a97ee628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5758ca876344daf9c27e9de67e0e383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료!\n",
      "임베딩 차원: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 다국어 지원 모델 로드 (한국어 포함)\n",
    "hf_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "print(f\"모델 로드 완료!\")\n",
    "print(f\"임베딩 차원: {hf_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 shape: (3, 384)\n",
      "\n",
      "첫 번째 문장 벡터 (처음 10개):\n",
      "[ 0.36907932  0.13543867 -0.14175008  0.04047783 -0.16253845 -0.05617588\n",
      "  0.311258    0.00239587  0.09228859 -0.03149693]\n",
      "\n",
      "=== Hugging Face 모델 유사도 ===\n",
      "'강아지가 공원에서 뛰어놀고 ...' vs '고양이가 공원에서 뛰어놀고 ...': 0.6597\n",
      "'강아지가 공원에서 뛰어놀고 ...' vs '주식 시장이 하락했다...': 0.3356\n",
      "'고양이가 공원에서 뛰어놀고 ...' vs '주식 시장이 하락했다...': 0.2879\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face 모델로 텍스트 임베딩하기\n",
    "hf_sentences = [\n",
    "    \"강아지가 공원에서 뛰어놀고 있다\",\n",
    "    \"고양이가 공원에서 뛰어놀고 있다\",\n",
    "    \"주식 시장이 하락했다\"\n",
    "]\n",
    "\n",
    "# 여러 문장을 한 번에 임베딩 (배치 처리)\n",
    "hf_embeddings = hf_model.encode(hf_sentences)\n",
    "\n",
    "print(f\"임베딩 shape: {hf_embeddings.shape}\")\n",
    "print(f\"\\n첫 번째 문장 벡터 (처음 10개):\")\n",
    "print(hf_embeddings[0][:10])\n",
    "\n",
    "# 유사도 계산\n",
    "from sentence_transformers import util\n",
    "\n",
    "print(\"\\n=== Hugging Face 모델 유사도 ===\")\n",
    "for i in range(len(hf_sentences)):\n",
    "    for j in range(i+1, len(hf_sentences)):\n",
    "        sim = util.cos_sim(hf_embeddings[i], hf_embeddings[j]).item()\n",
    "        print(f\"'{hf_sentences[i][:15]}...' vs '{hf_sentences[j][:15]}...': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI vs Hugging Face 임베딩 모델 비교\n",
    "\n",
    "| 항목 | OpenAI API | Hugging Face (로컬) |\n",
    "|------|------------|---------------------|\n",
    "| **비용** | 사용량에 따라 과금 | 무료 (컴퓨팅 비용만) |\n",
    "| **속도** | 네트워크 지연 있음 | 로컬 실행으로 빠름 |\n",
    "| **프라이버시** | 데이터가 서버로 전송됨 | 데이터가 로컬에 유지 |\n",
    "| **오프라인** | 인터넷 필요 | 오프라인 사용 가능 |\n",
    "| **품질** | 일반적으로 높은 품질 | 모델에 따라 다양 |\n",
    "| **설정** | API 키만 필요 | 모델 다운로드 필요 |\n",
    "| **확장성** | 무제한 확장 가능 | 하드웨어 제약 |\n",
    "\n",
    "**언제 무엇을 선택할까요?**\n",
    "- **OpenAI**: 프로덕션 서비스, 높은 품질이 필요할 때, 인프라 관리 부담을 줄이고 싶을 때\n",
    "- **Hugging Face**: 비용 절감, 데이터 프라이버시가 중요할 때, 오프라인 환경, 커스터마이징이 필요할 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 코사인 유사도(Cosine Similarity)\n",
    "\n",
    "### 개념\n",
    "\n",
    "코사인 유사도는 두 벡터 사이의 각도를 이용해 유사성을 측정합니다.\n",
    "\n",
    "- **1에 가까울수록**: 매우 유사함 (같은 방향)\n",
    "- **0에 가까울수록**: 무관함 (직각)\n",
    "- **-1에 가까울수록**: 반대됨 (정반대 방향)\n",
    "\n",
    "### 계산 공식\n",
    "\n",
    "```\n",
    "cosine_similarity = (A · B) / (||A|| × ||B||)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'강아지가 공원에서 뛰어놀고 있다'\n",
      "'고양이가 공원에서 뛰어놀고 있다'\n",
      "유사도: 0.6046\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"두 벡터 간의 코사인 유사도 계산\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# 테스트\n",
    "similarity = cosine_similarity(embeddings[0], embeddings[1])\n",
    "print(f\"'{sentences[0]}'\")\n",
    "print(f\"'{sentences[1]}'\")\n",
    "print(f\"유사도: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 문장 간 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 문장 간 유사도 매트릭스 ===\n",
      "\n",
      "                              문장 1  문장 2  문장 3  문장 4  문장 5  문장 6  \n",
      "문장1 (강아지가 공원에서 뛰어...)  1.000   0.605   0.158   0.339   0.085   0.097  \n",
      "문장2 (고양이가 공원에서 뛰어...)  0.605   1.000   0.164   0.709   0.150   0.086  \n",
      "문장3 (개가 산책하고 있어요...)  0.158   0.164   1.000   0.162   0.159   0.168  \n",
      "문장4 (고양이가 소파에서 자고...)  0.339   0.709   0.162   1.000   0.139   0.105  \n",
      "문장5 (주식 시장이 하락했다...)  0.085   0.150   0.159   0.139   1.000   0.348  \n",
      "문장6 (경제 뉴스가 발표되었다...)  0.097   0.086   0.168   0.105   0.348   1.000  \n"
     ]
    }
   ],
   "source": [
    "# 유사도 매트릭스 생성\n",
    "print(\"\\n=== 문장 간 유사도 매트릭스 ===\")\n",
    "print(\"\\n\" + \" \" * 30, end=\"\")\n",
    "for i, _ in enumerate(sentences):\n",
    "    print(f\"문장{i+1:2d}\", end=\"  \")\n",
    "print()\n",
    "\n",
    "for i, sent1 in enumerate(sentences):\n",
    "    print(f\"문장{i+1} ({sent1[:12]}...)\", end=\" \")\n",
    "    for j, sent2 in enumerate(sentences):\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        print(f\"{sim:6.3f}\", end=\"  \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가장 유사한 문장 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: '애완동물이 놀고 있어요'\n",
      "\n",
      "가장 유사한 문장들:\n",
      "1. 고양이가 소파에서 자고 있다\n",
      "   유사도: 0.3647\n",
      "\n",
      "2. 강아지가 공원에서 뛰어놀고 있다\n",
      "   유사도: 0.3520\n",
      "\n",
      "3. 고양이가 공원에서 뛰어놀고 있다\n",
      "   유사도: 0.3506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(query, sentences, embeddings, top_k=3):\n",
    "    \"\"\"쿼리와 가장 유사한 문장 찾기\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        sim = cosine_similarity(query_embedding, embedding)\n",
    "        similarities.append((sentences[i], sim))\n",
    "    \n",
    "    # 유사도 순으로 정렬\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# 테스트\n",
    "query = \"애완동물이 놀고 있어요\"\n",
    "results = find_most_similar(query, sentences, embeddings)\n",
    "\n",
    "print(f\"질문: '{query}'\\n\")\n",
    "print(\"가장 유사한 문장들:\")\n",
    "for i, (sentence, similarity) in enumerate(results, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "    print(f\"   유사도: {similarity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG (Retrieval-Augmented Generation)란?\n",
    "\n",
    "### 개념\n",
    "\n",
    "RAG는 외부 지식을 검색하여 LLM의 응답에 활용하는 기술입니다.\n",
    "\n",
    "### 왜 필요한가?\n",
    "\n",
    "LLM의 한계:\n",
    "- **지식 차단**: 학습 데이터의 시점까지만 알고 있음\n",
    "- **환각(Hallucination)**: 모르는 내용을 그럴듯하게 지어냄\n",
    "- **도메인 지식 부족**: 특정 회사나 제품의 최신 정보 모름\n",
    "\n",
    "RAG의 해결책:\n",
    "- 실시간으로 최신 정보를 검색하여 제공\n",
    "- 신뢰할 수 있는 출처 기반 답변\n",
    "- 도메인 특화 지식베이스 활용\n",
    "\n",
    "### RAG 파이프라인\n",
    "\n",
    "```\n",
    "1. 문서 준비\n",
    "   └→ 문서들을 임베딩하여 벡터 DB에 저장\n",
    "   \n",
    "2. 질문 받기\n",
    "   └→ 사용자 질문을 임베딩\n",
    "   \n",
    "3. 관련 문서 검색\n",
    "   └→ 유사도가 높은 문서들 찾기\n",
    "   \n",
    "4. 컨텍스트 주입\n",
    "   └→ 검색된 문서와 질문을 함께 LLM에 전달\n",
    "   \n",
    "5. 답변 생성\n",
    "   └→ LLM이 문서를 참고하여 답변\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 간단한 RAG 시스템 구현\n",
    "\n",
    "벡터 DB 없이 numpy만으로 간단한 RAG를 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지식베이스 문서 수: 7\n",
      "\n",
      "문서 목록:\n",
      "1. 우리 회사의 연차 휴가는 입사 1년 후부터 연 15일이 제공됩니다.\n",
      "2. 재택근무는 주 2회까지 가능하며, 사전에 팀장의 승인을 받아야 합니다.\n",
      "3. 점심시간은 12시부터 1시까지이며, 구내식당을 무료로 이용할 수 있습니다.\n",
      "4. 회사 건물은 오전 8시에 개방되고 오후 10시에 폐쇄됩니다.\n",
      "5. 신입사원 교육은 입사 첫 주에 3일간 진행되며, 필수 참석입니다.\n",
      "6. 경조사 휴가는 경조사 종류에 따라 1일에서 5일까지 제공됩니다.\n",
      "7. 복지포인트는 매년 100만원이 지급되며, 자유롭게 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 1. 지식베이스 준비 (예: 회사 정책 문서)\n",
    "knowledge_base = [\n",
    "    \"우리 회사의 연차 휴가는 입사 1년 후부터 연 15일이 제공됩니다.\",\n",
    "    \"재택근무는 주 2회까지 가능하며, 사전에 팀장의 승인을 받아야 합니다.\",\n",
    "    \"점심시간은 12시부터 1시까지이며, 구내식당을 무료로 이용할 수 있습니다.\",\n",
    "    \"회사 건물은 오전 8시에 개방되고 오후 10시에 폐쇄됩니다.\",\n",
    "    \"신입사원 교육은 입사 첫 주에 3일간 진행되며, 필수 참석입니다.\",\n",
    "    \"경조사 휴가는 경조사 종류에 따라 1일에서 5일까지 제공됩니다.\",\n",
    "    \"복지포인트는 매년 100만원이 지급되며, 자유롭게 사용할 수 있습니다.\"\n",
    "]\n",
    "\n",
    "print(\"지식베이스 문서 수:\", len(knowledge_base))\n",
    "print(\"\\n문서 목록:\")\n",
    "for i, doc in enumerate(knowledge_base, 1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서들을 임베딩하는 중...\n",
      "✅ 7개 문서 임베딩 완료\n"
     ]
    }
   ],
   "source": [
    "# 2. 모든 문서를 임베딩\n",
    "print(\"문서들을 임베딩하는 중...\")\n",
    "kb_embeddings = [get_embedding(doc) for doc in knowledge_base]\n",
    "print(f\"✅ {len(kb_embeddings)}개 문서 임베딩 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RAG 함수 구현\n",
    "def rag_query(question, knowledge_base, kb_embeddings, top_k=2):\n",
    "    \"\"\"\n",
    "    RAG를 사용하여 질문에 답변\n",
    "    \n",
    "    Args:\n",
    "        question: 사용자 질문\n",
    "        knowledge_base: 문서 리스트\n",
    "        kb_embeddings: 문서 임베딩 리스트\n",
    "        top_k: 검색할 문서 개수\n",
    "    \"\"\"\n",
    "    # Step 1: 질문 임베딩, 질문을 벡터로 변환\n",
    "    print(f\"질문: {question}\\n\")\n",
    "    question_embedding = get_embedding(question)\n",
    "    \n",
    "    # Step 2: 유사한 문서 검색\n",
    "    print(\"📚 관련 문서 검색 중...\")\n",
    "    similarities = []\n",
    "    for i, doc_embedding in enumerate(kb_embeddings):\n",
    "        sim = cosine_similarity(question_embedding, doc_embedding)\n",
    "        similarities.append((i, knowledge_base[i], sim))\n",
    "    \n",
    "    # 유사도 순 정렬\n",
    "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "    top_docs = similarities[:top_k]\n",
    "    \n",
    "    print(f\"\\n가장 관련있는 {top_k}개 문서:\")\n",
    "    for i, (idx, doc, sim) in enumerate(top_docs, 1):\n",
    "        print(f\"  {i}. (유사도: {sim:.4f}) {doc}\")\n",
    "    \n",
    "    # Step 3: 컨텍스트 구성\n",
    "    context = \"\\n\".join([doc for _, doc, _ in top_docs])\n",
    "    \n",
    "    # Step 4: LLM에 컨텍스트와 질문 전달\n",
    "    print(\"\\n🤖 LLM 응답 생성 중...\\n\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"당신은 회사 정책에 대해 정확하게 답변하는 HR 어시스턴트입니다. 주어진 문서 정보만을 바탕으로 답변하세요.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"다음은 관련 문서입니다:\n",
    "\n",
    "            {context}\n",
    "\n",
    "            질문: {question}\n",
    "\n",
    "            위 문서를 참고하여 질문에 답변해주세요.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"답변:\")\n",
    "    print(answer)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return answer, top_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG 시스템 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 입사하면 휴가를 몇 일이나 쓸 수 있나요?\n",
      "\n",
      "📚 관련 문서 검색 중...\n",
      "\n",
      "가장 관련있는 2개 문서:\n",
      "  1. (유사도: 0.5321) 경조사 휴가는 경조사 종류에 따라 1일에서 5일까지 제공됩니다.\n",
      "  2. (유사도: 0.5179) 우리 회사의 연차 휴가는 입사 1년 후부터 연 15일이 제공됩니다.\n",
      "\n",
      "🤖 LLM 응답 생성 중...\n",
      "\n",
      "============================================================\n",
      "답변:\n",
      "입사 1년 후부터 연차 휴가로 15일을 사용하실 수 있습니다. 또한, 경조사에 따라 추가로 1일에서 5일까지의 휴가를 제공받을 수 있습니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 테스트 1: 연차 관련 질문\n",
    "answer, docs = rag_query(\n",
    "    \"입사하면 휴가를 몇 일이나 쓸 수 있나요?\",\n",
    "    knowledge_base,\n",
    "    kb_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 집에서 일하고 싶은데 가능한가요?\n",
      "\n",
      "📚 관련 문서 검색 중...\n",
      "\n",
      "가장 관련있는 2개 문서:\n",
      "  1. (유사도: 0.2738) 재택근무는 주 2회까지 가능하며, 사전에 팀장의 승인을 받아야 합니다.\n",
      "  2. (유사도: 0.2500) 점심시간은 12시부터 1시까지이며, 구내식당을 무료로 이용할 수 있습니다.\n",
      "\n",
      "🤖 LLM 응답 생성 중...\n",
      "\n",
      "============================================================\n",
      "답변:\n",
      "네, 가능합니다. 하지만 주 2회까지만 재택근무가 가능하며, 꼭 사전에 팀장의 승인을 받아야 합니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 테스트 2: 재택근무 관련 질문\n",
    "answer, docs = rag_query(\n",
    "    \"집에서 일하고 싶은데 가능한가요?\",\n",
    "    knowledge_base,\n",
    "    kb_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 신입사원이 알아야 할 중요한 정보는 무엇인가요?\n",
      "\n",
      "📚 관련 문서 검색 중...\n",
      "\n",
      "가장 관련있는 3개 문서:\n",
      "  1. (유사도: 0.5923) 신입사원 교육은 입사 첫 주에 3일간 진행되며, 필수 참석입니다.\n",
      "  2. (유사도: 0.2266) 재택근무는 주 2회까지 가능하며, 사전에 팀장의 승인을 받아야 합니다.\n",
      "  3. (유사도: 0.2229) 우리 회사의 연차 휴가는 입사 1년 후부터 연 15일이 제공됩니다.\n",
      "\n",
      "🤖 LLM 응답 생성 중...\n",
      "\n",
      "============================================================\n",
      "답변:\n",
      "신입사원이 알아야 할 중요한 정보는 다음과 같습니다. 첫째, 입사 첫 주에는 3일간 필수 교육이 있습니다. 둘째, 재택근무는 주 2회까지 가능하나, 이는 팀장의 사전 승인을 받아야 합니다. 셋째, 연차 휴가는 입사 1년 후부터 연 15일이 제공됩니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 테스트 3: 복합 질문\n",
    "answer, docs = rag_query(\n",
    "    \"신입사원이 알아야 할 중요한 정보는 무엇인가요?\",\n",
    "    knowledge_base,\n",
    "    kb_embeddings,\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RAG vs 일반 LLM 비교\n",
    "\n",
    "지식베이스에 없는 정보를 물어보면 어떻게 될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 RAG 사용 (지식베이스 참고):\n",
      "============================================================\n",
      "질문: 우리 회사 연차는 며칠인가요?\n",
      "\n",
      "📚 관련 문서 검색 중...\n",
      "\n",
      "가장 관련있는 2개 문서:\n",
      "  1. (유사도: 0.5281) 우리 회사의 연차 휴가는 입사 1년 후부터 연 15일이 제공됩니다.\n",
      "  2. (유사도: 0.2932) 회사 건물은 오전 8시에 개방되고 오후 10시에 폐쇄됩니다.\n",
      "\n",
      "🤖 LLM 응답 생성 중...\n",
      "\n",
      "============================================================\n",
      "답변:\n",
      "우리 회사의 연차는 연 15일입니다.\n",
      "============================================================\n",
      "\n",
      "\n",
      "❌ RAG 미사용 (LLM 지식만 사용):\n",
      "============================================================\n",
      "회사의 정확한 연차 정책을 알려드리기 위해서는, 저에게 회사의 이름이나 특정 세부사항이 필요합니다. 그러나 일반적으로, 대부분의 회사는 1년 동안 근무한 직원에게 10일의 유급 연차를 제공합니다. 이는 근무 연수에 따라 증가할 수 있습니다. 단, 이는 회사마다 다르며, 일부 회사는 더 많은 연차를 제공하거나, 일부는 더 적게 제공할 수 있습니다. 따라서 귀하의 회사의 구체적인 연차 정책을 확인하려면 인사팀에 문의하시는 것이 가장 좋습니다.\n",
      "============================================================\n",
      "\n",
      "💡 차이점:\n",
      "- RAG: 정확한 회사 정책(15일)을 제공\n",
      "- No RAG: 일반적인 답변이거나 정확하지 않을 수 있음\n"
     ]
    }
   ],
   "source": [
    "# RAG 없이 직접 질문\n",
    "def ask_without_rag(question):\n",
    "    \"\"\"RAG 없이 LLM에 직접 질문\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"당신은 회사 정책에 대해 답변하는 HR 어시스턴트입니다.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 비교 테스트\n",
    "question = \"우리 회사 연차는 며칠인가요?\"\n",
    "\n",
    "print(\"🔍 RAG 사용 (지식베이스 참고):\")\n",
    "print(\"=\"*60)\n",
    "rag_answer, _ = rag_query(question, knowledge_base, kb_embeddings)\n",
    "\n",
    "print(\"\\n\\n❌ RAG 미사용 (LLM 지식만 사용):\")\n",
    "print(\"=\"*60)\n",
    "no_rag_answer = ask_without_rag(question)\n",
    "print(no_rag_answer)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n💡 차이점:\")\n",
    "print(\"- RAG: 정확한 회사 정책(15일)을 제공\")\n",
    "print(\"- No RAG: 일반적인 답변이거나 정확하지 않을 수 있음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실전 팁\n",
    "\n",
    "### 성능 향상 방법\n",
    "\n",
    "1. **청크 크기 조정**: 문서를 적절한 크기로 분할\n",
    "2. **하이브리드 검색**: 키워드 + 벡터 검색 병행\n",
    "3. **재순위화(Re-ranking)**: 검색 결과를 다시 정렬\n",
    "4. **메타데이터 활용**: 날짜, 출처 등 추가 정보 활용\n",
    "\n",
    "### 벡터 데이터베이스\n",
    "\n",
    "실전에서는 numpy 대신 전문 벡터 DB를 사용합니다:\n",
    "\n",
    "| 벡터 DB | 특징 | 추천 용도 |\n",
    "|---------|------|----------|\n",
    "| **Pinecone** | 완전 관리형, 확장성 | 프로덕션 서비스 |\n",
    "| **ChromaDB** | 오픈소스, 간단 | 프로토타입, 소규모 |\n",
    "| **Weaviate** | 오픈소스, 풍부한 기능 | 복잡한 검색 |\n",
    "| **Qdrant** | 오픈소스, 빠른 성능 | 대용량 데이터 |\n",
    "\n",
    "### 비용 최적화\n",
    "\n",
    "- 임베딩 캐싱: 동일한 텍스트는 재사용\n",
    "- 배치 처리: 여러 텍스트를 한 번에 임베딩\n",
    "- 작은 모델 사용: `text-embedding-3-small` 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 요약\n",
    "\n",
    "이번 노트북에서 다룬 내용:\n",
    "\n",
    "### 핵심 포인트\n",
    "\n",
    "1. **벡터 임베딩**: 텍스트를 고차원 벡터로 변환하여 의미를 수치화\n",
    "2. **코사인 유사도**: 두 벡터의 의미적 유사성을 -1~1 범위로 측정\n",
    "3. **RAG**: 외부 지식을 검색하여 LLM 응답의 정확성 향상\n",
    "4. **RAG 파이프라인**: 임베딩 → 검색 → 컨텍스트 주입 → 생성\n",
    "5. **실전 활용**: 문서 검색, 질의응답, 추천 시스템 등\n",
    "\n",
    "### RAG의 장점\n",
    "\n",
    "- ✅ 최신 정보 활용 가능\n",
    "- ✅ 도메인 특화 지식 제공\n",
    "- ✅ 환각(Hallucination) 감소\n",
    "- ✅ 출처 추적 가능\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "다음 학습에서는:\n",
    "- ChromaDB, Pinecone 등 벡터 DB 활용\n",
    "- 대용량 문서 처리 (청킹 전략)\n",
    "- 고급 RAG 기법 (하이브리드 검색, 재순위화)\n",
    "- LangChain을 활용한 RAG 파이프라인\n",
    "\n",
    "---\n",
    "\n",
    "**참고 자료**\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [Vector Database Comparison](https://github.com/erikbern/ann-benchmarks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
