{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG ê¸°ë°˜ ê³ ê° ìƒë‹´ ì±—ë´‡ ë§Œë“¤ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” LangChainê³¼ ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ **RAG(Retrieval-Augmented Generation)** ê¸°ë°˜ì˜ ê³ ê° ìƒë‹´ ì±—ë´‡ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "| ì£¼ì œ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| ë¬¸ì„œ ë¡œë”© | ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ LangChain Documentë¡œ ë¡œë“œ |\n",
    "| ì²­í‚¹ | ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• í•˜ëŠ” ì „ëµ |\n",
    "| ì„ë² ë”© | í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ |\n",
    "| ê²€ìƒ‰ | ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ |\n",
    "| RAG ì²´ì¸ | ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í™œìš©í•œ LLM ë‹µë³€ ìƒì„± |\n",
    "| ì›¹ UI | Gradioë¡œ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶• |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í”„ë¡œì íŠ¸ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "example/\n",
    "â”œâ”€â”€ app.py                    # Gradio ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ embed_documents.py    # ë¬¸ì„œ ì„ë² ë”© ëª¨ë“ˆ\n",
    "â”‚   â””â”€â”€ rag_chain.py          # RAG ì²´ì¸ ëª¨ë“ˆ\n",
    "â”œâ”€â”€ knowledge_base/           # ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ\n",
    "â”‚   â”œâ”€â”€ products/             # ì œí’ˆ ì •ë³´\n",
    "â”‚   â”œâ”€â”€ policies/             # ì •ì±… ë¬¸ì„œ (ë°°ì†¡, êµí™˜/í™˜ë¶ˆ, ë³´ì¦)\n",
    "â”‚   â””â”€â”€ faq/                  # FAQ\n",
    "â””â”€â”€ chroma_db/                # ë²¡í„° DB (ìë™ ìƒì„±)\n",
    "```\n",
    "\n",
    "### ë°ì´í„° íë¦„\n",
    "\n",
    "```\n",
    "1. ë¬¸ì„œ ì¤€ë¹„ (knowledge_base/)\n",
    "       â†“\n",
    "2. ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹ (embed_documents.py)\n",
    "       â†“\n",
    "3. ì„ë² ë”© & ë²¡í„° DB ì €ì¥ (chroma_db/)\n",
    "       â†“\n",
    "4. ì‚¬ìš©ì ì§ˆë¬¸ â†’ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (rag_chain.py)\n",
    "       â†“\n",
    "5. ê²€ìƒ‰ ê²°ê³¼ + ì§ˆë¬¸ â†’ LLM ë‹µë³€ ìƒì„±\n",
    "       â†“\n",
    "6. Gradio UIë¡œ ì‘ë‹µ í‘œì‹œ (app.py)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´ ìƒëµ)\n",
    "# !pip install langchain langchain-openai langchain-chroma langchain-huggingface langchain-community gradio python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API í‚¤ í™•ì¸\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"âœ… OpenAI API í‚¤ ë¡œë“œë¨: {api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²½ë¡œ ì„¤ì •\n",
    "EXAMPLE_DIR = Path(\"./example\")\n",
    "KNOWLEDGE_BASE_DIR = EXAMPLE_DIR / \"knowledge_base\"\n",
    "CHROMA_DB_DIR = EXAMPLE_DIR / \"chroma_db\"\n",
    "\n",
    "print(f\"ì˜ˆì œ í´ë”: {EXAMPLE_DIR.absolute()}\")\n",
    "print(f\"ì§€ì‹ë² ì´ìŠ¤: {KNOWLEDGE_BASE_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ì‚´í´ë³´ê¸°\n",
    "\n",
    "ìš°ë¦¬ê°€ ì¤€ë¹„í•œ ì§€ì‹ë² ì´ìŠ¤ì—ëŠ” ê°€ìƒì˜ ì‡¼í•‘ëª° \"TechMall\"ì˜ ì •ë³´ê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¡° í™•ì¸\n",
    "print(\"ğŸ“ ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¡°:\\n\")\n",
    "\n",
    "for category_dir in sorted(KNOWLEDGE_BASE_DIR.iterdir()):\n",
    "    if category_dir.is_dir():\n",
    "        files = list(category_dir.glob(\"*.md\"))\n",
    "        print(f\"ğŸ“‚ {category_dir.name}/\")\n",
    "        for f in files:\n",
    "            print(f\"   â””â”€ {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ë¬¸ì„œ ë‚´ìš© í™•ì¸\n",
    "sample_file = KNOWLEDGE_BASE_DIR / \"products\" / \"smartwatch_pro.md\"\n",
    "\n",
    "print(f\"ğŸ“„ {sample_file.name} ë‚´ìš©:\\n\")\n",
    "print(sample_file.read_text(encoding=\"utf-8\")[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë¬¸ì„œ ë¡œë”© (Document Loading)\n",
    "\n",
    "LangChainì˜ `DirectoryLoader`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "def load_all_documents(base_path: Path) -> list:\n",
    "    \"\"\"\n",
    "    ì§€ì‹ë² ì´ìŠ¤ì˜ ëª¨ë“  ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    ê° ë¬¸ì„œì— ì¹´í…Œê³ ë¦¬ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for category_folder in base_path.iterdir():\n",
    "        if not category_folder.is_dir():\n",
    "            continue\n",
    "        \n",
    "        category = category_folder.name\n",
    "        \n",
    "        # DirectoryLoaderë¡œ í•´ë‹¹ í´ë”ì˜ .md íŒŒì¼ ë¡œë“œ\n",
    "        loader = DirectoryLoader(\n",
    "            str(category_folder),\n",
    "            glob=\"**/*.md\",\n",
    "            loader_cls=TextLoader,\n",
    "            loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "        )\n",
    "        \n",
    "        docs = loader.load()\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„°ì— ì¹´í…Œê³ ë¦¬ ì¶”ê°€\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"category\"] = category\n",
    "            doc.metadata[\"filename\"] = Path(doc.metadata[\"source\"]).name\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "documents = load_all_documents(KNOWLEDGE_BASE_DIR)\n",
    "\n",
    "print(f\"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œë¨\\n\")\n",
    "\n",
    "# ë¡œë“œëœ ë¬¸ì„œ ì •ë³´ ì¶œë ¥\n",
    "for doc in documents:\n",
    "    content_preview = doc.page_content[:50].replace(\"\\n\", \" \") + \"...\"\n",
    "    print(f\"ğŸ“„ [{doc.metadata['category']}] {doc.metadata['filename']}\")\n",
    "    print(f\"   ë‚´ìš©: {content_preview}\")\n",
    "    print(f\"   ê¸¸ì´: {len(doc.page_content):,}ì\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì²­í‚¹ (Chunking)\n",
    "\n",
    "ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ê²€ìƒ‰ íš¨ìœ¨ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì²­í‚¹ ì „ëµ\n",
    "\n",
    "| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¶Œì¥ê°’ |\n",
    "|---------|------|-------|\n",
    "| chunk_size | ì²­í¬ì˜ ìµœëŒ€ ë¬¸ì ìˆ˜ | 300-1000 |\n",
    "| chunk_overlap | ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ | chunk_sizeì˜ 10-20% |\n",
    "| separators | ë¶„í•  ê¸°ì¤€ ë¬¸ìì—´ | í—¤ë”, ë‹¨ë½ ìˆœì„œë¡œ |\n",
    "\n",
    "### RecursiveCharacterTextSplitter\n",
    "\n",
    "ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸°ì…ë‹ˆë‹¤. ì§€ì •ëœ separators ìˆœì„œëŒ€ë¡œ ë¶„í• ì„ ì‹œë„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í• \n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}\")\n",
    "print(f\"ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
    "print(f\"í‰ê·  ì²­í¬ í¬ê¸°: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f}ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²­í¬ ìƒ˜í”Œ í™•ì¸\n",
    "print(\"=\" * 60)\n",
    "print(\"ì²­í¬ ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- ì²­í¬ {i+1} ---\")\n",
    "    print(f\"ì¹´í…Œê³ ë¦¬: {chunk.metadata['category']}\")\n",
    "    print(f\"íŒŒì¼: {chunk.metadata['filename']}\")\n",
    "    print(f\"ê¸¸ì´: {len(chunk.page_content)}ì\")\n",
    "    print(f\"ë‚´ìš©:\\n{chunk.page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì„ë² ë”© & ë²¡í„° ì €ì¥ (Embedding & Vector Store)\n",
    "\n",
    "ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì„ë² ë”© ëª¨ë¸ ì„ íƒ\n",
    "\n",
    "| ëª¨ë¸ | ì°¨ì› | íŠ¹ì§• |\n",
    "|------|-----|------|\n",
    "| all-MiniLM-L6-v2 | 384 | ë¬´ë£Œ, ë¹ ë¦„, ë¡œì»¬ ì‹¤í–‰ |\n",
    "| text-embedding-3-small | 1536 | OpenAI, ìœ ë£Œ, ê³ ì„±ëŠ¥ |\n",
    "| text-embedding-3-large | 3072 | OpenAI, ìœ ë£Œ, ìµœê³  ì„±ëŠ¥ |\n",
    "\n",
    "ì´ë²ˆ ì˜ˆì œì—ì„œëŠ” ë¬´ë£Œë¡œ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ HuggingFace ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import shutil\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {EMBEDDING_MODEL}\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "print(\"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© í…ŒìŠ¤íŠ¸\n",
    "test_text = \"ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ ë°°í„°ë¦¬ ìˆ˜ëª…\"\n",
    "test_embedding = embedding_model.embed_query(test_text)\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: '{test_text}'\")\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {len(test_embedding)}\")\n",
    "print(f\"ì„ë² ë”© ìƒ˜í”Œ: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ DB ì‚­ì œ (ì¬ìƒì„±ì„ ìœ„í•´)\n",
    "if CHROMA_DB_DIR.exists():\n",
    "    shutil.rmtree(CHROMA_DB_DIR)\n",
    "    print(\"ê¸°ì¡´ ë²¡í„° DB ì‚­ì œë¨\")\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "print(\"\\në²¡í„° DB ìƒì„± ì¤‘...\")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=str(CHROMA_DB_DIR)\n",
    ")\n",
    "\n",
    "# ì €ì¥ ì •ë³´ í™•ì¸\n",
    "collection = vector_store._collection\n",
    "print(f\"\\nâœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   ì €ì¥ëœ ë²¡í„° ìˆ˜: {collection.count():,}ê°œ\")\n",
    "print(f\"   ì €ì¥ ê²½ë¡œ: {CHROMA_DB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (Retrieval)\n",
    "\n",
    "ë²¡í„° DBì—ì„œ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever ìƒì„±\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  # ìƒìœ„ 5ê°œ ë¬¸ì„œ ë°˜í™˜\n",
    ")\n",
    "\n",
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "test_query = \"ë°°ì†¡ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"ì§ˆë¬¸: {test_query}\")\n",
    "print(f\"\\nê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n[{i}] {doc.metadata['category']} / {doc.metadata['filename']}\")\n",
    "    print(f\"{doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "queries = [\n",
    "    \"í™˜ë¶ˆí•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\",\n",
    "    \"ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ ë°©ìˆ˜ ë˜ë‚˜ìš”?\",\n",
    "    \"í¬ì¸íŠ¸ëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    docs = retriever.invoke(query)\n",
    "    print(f\"\\nğŸ” '{query}'\")\n",
    "    print(f\"   â†’ [{docs[0].metadata['category']}] {docs[0].page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG ì²´ì¸ êµ¬í˜„\n",
    "\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "SYSTEM_PROMPT = \"\"\"ë‹¹ì‹ ì€ TechMall ì‡¼í•‘ëª°ì˜ ì¹œì ˆí•œ ê³ ê° ìƒë‹´ì›ì…ë‹ˆë‹¤.\n",
    "ê³ ê°ì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "\n",
    "## ë‹µë³€ ì§€ì¹¨\n",
    "- ì•„ë˜ ì°¸ê³  ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "- ì°¸ê³  ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ \"í™•ì¸ í›„ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤\"ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”.\n",
    "- ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "## ì°¸ê³  ìë£Œ\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "print(\"LLM ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_rag(question: str) -> str:\n",
    "    \"\"\"\n",
    "    RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "    context_parts = []\n",
    "    for doc in docs:\n",
    "        category = doc.metadata.get(\"category\", \"\")\n",
    "        context_parts.append(f\"[{category}]\\n{doc.page_content}\")\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    system_message = SYSTEM_PROMPT.format(context=context)\n",
    "    \n",
    "    # 4. LLM í˜¸ì¶œ\n",
    "    messages = [\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=question)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG í…ŒìŠ¤íŠ¸\n",
    "test_questions = [\n",
    "    \"SmartWatch Pro X1 ê°€ê²©ì´ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "    \"í™˜ë¶ˆì€ ì–´ë–»ê²Œ ì‹ ì²­í•˜ë‚˜ìš”?\",\n",
    "    \"ë³´ì¦ ê¸°ê°„ì„ ì—°ì¥í•  ìˆ˜ ìˆë‚˜ìš”?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    answer = ask_with_rag(q)\n",
    "    print(f\"\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RAG vs ì¼ë°˜ LLM ë¹„êµ\n",
    "\n",
    "RAGë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_without_rag(question: str) -> str:\n",
    "    \"\"\"RAG ì—†ì´ LLMì— ì§ì ‘ ì§ˆë¬¸\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"ë‹¹ì‹ ì€ TechMall ì‡¼í•‘ëª°ì˜ ê³ ê° ìƒë‹´ì›ì…ë‹ˆë‹¤.\"),\n",
    "        HumanMessage(content=question)\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "# ë¹„êµ\n",
    "comparison_question = \"SmartWatch Pro X1ì˜ ë°°í„°ë¦¬ ìˆ˜ëª…ì€ ì–¼ë§ˆì¸ê°€ìš”?\"\n",
    "\n",
    "print(\"ì§ˆë¬¸:\", comparison_question)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“š RAG ì‚¬ìš© (ì§€ì‹ë² ì´ìŠ¤ ì°¸ì¡°)\")\n",
    "print(\"=\"*60)\n",
    "print(ask_with_rag(comparison_question))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âŒ RAG ë¯¸ì‚¬ìš© (LLM ì§€ì‹ë§Œ)\")\n",
    "print(\"=\"*60)\n",
    "print(ask_without_rag(comparison_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gradio UIë¡œ ì±—ë´‡ ì‹¤í–‰\n",
    "\n",
    "ì´ì œ ì™„ì„±ëœ RAG ì±—ë´‡ì„ Gradio UIë¡œ ì‹¤í–‰í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "cd basic/day6/example\n",
    "python app.py\n",
    "```\n",
    "\n",
    "ë˜ëŠ” ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ê°„ë‹¨í•œ ì±— ì¸í„°í˜ì´ìŠ¤ë¥¼ ë„ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"Gradioìš© ì±„íŒ… í•¨ìˆ˜\"\"\"\n",
    "    return ask_with_rag(message)\n",
    "\n",
    "# ê°„ë‹¨í•œ ì±„íŒ… UI\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"TechMall ê³ ê° ìƒë‹´ ì±—ë´‡\",\n",
    "    description=\"ì œí’ˆ, ë°°ì†¡, êµí™˜/í™˜ë¶ˆ, ë³´ì¦ ë“±ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”!\",\n",
    "    examples=[\n",
    "        \"SmartWatch Pro X1 ê°€ê²©ì´ ì–¼ë§ˆì¸ê°€ìš”?\",\n",
    "        \"í™˜ë¶ˆì€ ì–´ë–»ê²Œ ì‹ ì²­í•˜ë‚˜ìš”?\",\n",
    "        \"ë°°ì†¡ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ìš”ì•½\n",
    "\n",
    "### RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„±ìš”ì†Œ\n",
    "\n",
    "| ë‹¨ê³„ | ëª¨ë“ˆ | ì—­í•  |\n",
    "|------|------|------|\n",
    "| 1. ë¬¸ì„œ ë¡œë”© | DirectoryLoader | íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë¬¸ì„œ ì½ê¸° |\n",
    "| 2. ì²­í‚¹ | RecursiveCharacterTextSplitter | ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í•  |\n",
    "| 3. ì„ë² ë”© | HuggingFaceEmbeddings | í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ |\n",
    "| 4. ì €ì¥ | ChromaDB | ë²¡í„° ì €ì¥ ë° ê²€ìƒ‰ |\n",
    "| 5. ê²€ìƒ‰ | Retriever | ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ |\n",
    "| 6. ìƒì„± | ChatOpenAI | ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„± |\n",
    "| 7. UI | Gradio | ëŒ€í™”í˜• ì›¹ ì¸í„°í˜ì´ìŠ¤ |\n",
    "\n",
    "### í•µì‹¬ í¬ì¸íŠ¸\n",
    "\n",
    "1. **ì²­í‚¹ ì „ëµì´ ì¤‘ìš”**: chunk_sizeì™€ overlapì„ ë¬¸ì„œ íŠ¹ì„±ì— ë§ê²Œ ì¡°ì •\n",
    "2. **ë©”íƒ€ë°ì´í„° í™œìš©**: ì¹´í…Œê³ ë¦¬, ì¶œì²˜ ë“±ì„ ì €ì¥í•˜ì—¬ í•„í„°ë§/ë””ë²„ê¹…ì— í™œìš©\n",
    "3. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ LLMì˜ ë™ì‘ ë°©ì‹ ì œì–´\n",
    "4. **RAGì˜ ì¥ì **: ìµœì‹  ì •ë³´ ë°˜ì˜, í™˜ê° ê°ì†Œ, ì¶œì²˜ ì¶”ì  ê°€ëŠ¥\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ë²¡í„°)\n",
    "- Re-rankingìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ\n",
    "- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ\n",
    "- ëŒ€í™” ì´ë ¥ ê´€ë¦¬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
