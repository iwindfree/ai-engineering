{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b155564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/windfree/.pyenv/versions/3.13.2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geqqqaflryq",
   "metadata": {},
   "source": [
    "# LLM Tokenizer ì¶”ê°€ í•™ìŠµ\n",
    "\n",
    "## ì†Œê°œ\n",
    "\n",
    "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì´í•´í•˜ëŠ” ë° ìˆì–´ **í† í¬ë‚˜ì´ì €(Tokenizer)** ëŠ” ê°€ì¥ ê¸°ë³¸ì ì´ë©´ì„œë„ ì¤‘ìš”í•œ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì¸ê°„ì´ ì´í•´í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í† í¬ë‚˜ì´ì €ê°€ í•˜ëŠ” ì¼\n",
    "\n",
    "1. **í…ìŠ¤íŠ¸ â†’ í† í°**: ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„(í† í°)ë¡œ ë¶„í• \n",
    "2. **í† í° â†’ ID**: ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ IDë¡œ ë³€í™˜\n",
    "3. **ID â†’ í† í°**: ëª¨ë¸ ì¶œë ¥(ID)ì„ ë‹¤ì‹œ í† í°ìœ¼ë¡œ ë³€í™˜\n",
    "4. **í† í° â†’ í…ìŠ¤íŠ¸**: í† í°ì„ ë‹¤ì‹œ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³µì›\n",
    "\n",
    "### ì™œ í† í¬ë‚˜ì´ì €ê°€ ì¤‘ìš”í•œê°€?\n",
    "\n",
    "- **íš¨ìœ¨ì„±**: ë‹¨ì–´ ë‹¨ìœ„ë³´ë‹¤ ë” íš¨ìœ¨ì ì¸ í•˜ìœ„ ë‹¨ì–´(subword) ë‹¨ìœ„ ì²˜ë¦¬\n",
    "- **ì¼ê´€ì„±**: í•™ìŠµê³¼ ì¶”ë¡  ì‹œ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬\n",
    "- **ì–´íœ˜ ê´€ë¦¬**: ì œí•œëœ vocabularyë¡œ ë¬´í•œí•œ í…ìŠ¤íŠ¸ í‘œí˜„\n",
    "- **ë‹¤êµ­ì–´ ì§€ì›**: ë‹¤ì–‘í•œ ì–¸ì–´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5feb4168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36c25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a2e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40078d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace ë¡œê·¸ì¸ (ì„ íƒì‚¬í•­ - ì¼ë¶€ ì œí•œëœ ëª¨ë¸ì—ë§Œ í•„ìš”)\n",
    "# ê³µê°œ ëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš° ì´ ë‹¨ê³„ëŠ” ê±´ë„ˆë›°ì–´ë„ ë©ë‹ˆë‹¤\n",
    "#hf_token = \"enter your token here\"\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "load_dotenv(override=True)\n",
    "from huggingface_hub import login\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v885j52juma",
   "metadata": {},
   "source": [
    "## ì‹¤ìŠµ ì¤€ë¹„\n",
    "\n",
    "ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•˜ê³  í† í¬ë‚˜ì´ì €ë¥¼ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b31c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# Llama 3.1 í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)\n",
    "print(\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71khkbv295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ í…ìŠ¤íŠ¸: Hello, how are you today?\n",
      "í† í° ê°œìˆ˜: 7\n",
      "í† í° ë¦¬ìŠ¤íŠ¸: [9906, 11, 1268, 527, 499, 3432, 30]\n"
     ]
    }
   ],
   "source": [
    "# ê°„ë‹¨í•œ ì˜ì–´ ë¬¸ì¥ í† í°í™”\n",
    "text = \"Hello, how are you today?\"\n",
    "#tokens = tokenizer.tokenize(text)\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: {text}\")\n",
    "print(f\"í† í° ê°œìˆ˜: {len(tokens)}\")\n",
    "print(f\"í† í° ë¦¬ìŠ¤íŠ¸: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ypdxn2vaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ì½”ë“œ:\n",
      "def hello_world():\n",
      "    print('Hello, World!')\n",
      "\n",
      "í† í° ê°œìˆ˜: 12\n",
      "í† í° ë¦¬ìŠ¤íŠ¸: ['def', 'Ä hello', '_world', '():ÄŠ', 'Ä Ä Ä ', 'Ä print', \"('\", 'Hello', ',', 'Ä World', '!', \"')\"]\n"
     ]
    }
   ],
   "source": [
    "# ë³µì¡í•œ ì˜ˆì œ: ì½”ë“œì™€ íŠ¹ìˆ˜ë¬¸ì\n",
    "code_text = \"def hello_world():\\n    print('Hello, World!')\"\n",
    "code_tokens = tokenizer.tokenize(code_text)\n",
    "\n",
    "print(f\"ì›ë³¸ ì½”ë“œ:\\n{code_text}\\n\")\n",
    "print(f\"í† í° ê°œìˆ˜: {len(code_tokens)}\")\n",
    "print(f\"í† í° ë¦¬ìŠ¤íŠ¸: {code_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66vrkt8gf",
   "metadata": {},
   "source": [
    "## íŠ¹ìˆ˜ í† í° (Special Tokens)\n",
    "\n",
    "LLMì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì—¬ëŸ¬ íŠ¹ìˆ˜ í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:\n",
    "\n",
    "- **BOS (Beginning of Sequence)**: ì‹œí€€ìŠ¤ì˜ ì‹œì‘ì„ í‘œì‹œ\n",
    "- **EOS (End of Sequence)**: ì‹œí€€ìŠ¤ì˜ ëì„ í‘œì‹œ  \n",
    "- **PAD (Padding)**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•œ íŒ¨ë”©\n",
    "- **UNK (Unknown)**: ì–´íœ˜ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ í‘œì‹œ\n",
    "\n",
    "ì´ëŸ¬í•œ íŠ¹ìˆ˜ í† í°ë“¤ì€ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f305a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== íŠ¹ìˆ˜ í† í° ì •ë³´ ===\n",
      "BOS í† í°: <|begin_of_text|> (ID: 128000)\n",
      "EOS í† í°: <|end_of_text|> (ID: 128001)\n",
      "PAD í† í°: None (ID: None)\n",
      "UNK í† í°: None (ID: None)\n",
      "\n",
      "ëª¨ë“  íŠ¹ìˆ˜ í† í°: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}\n"
     ]
    }
   ],
   "source": [
    "# íŠ¹ìˆ˜ í† í° í™•ì¸\n",
    "print(\"=== íŠ¹ìˆ˜ í† í° ì •ë³´ ===\")\n",
    "print(f\"BOS í† í°: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS í† í°: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD í† í°: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"UNK í† í°: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
    "print(f\"\\nëª¨ë“  íŠ¹ìˆ˜ í† í°: {tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dilobaymx9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ í…ìŠ¤íŠ¸: Hello, World!\n",
      "\n",
      "íŠ¹ìˆ˜ í† í° í¬í•¨ (add_special_tokens=True):\n",
      "  í† í° ID: [128000, 9906, 11, 4435, 0]\n",
      "  í† í° ê°œìˆ˜: 5\n",
      "\n",
      "íŠ¹ìˆ˜ í† í° ì œì™¸ (add_special_tokens=False):\n",
      "  í† í° ID: [9906, 11, 4435, 0]\n",
      "  í† í° ê°œìˆ˜: 4\n"
     ]
    }
   ],
   "source": [
    "# íŠ¹ìˆ˜ í† í°ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬\n",
    "text_with_special = \"Hello, World!\"\n",
    "# add_special_tokens=Trueë¡œ BOS/EOS í† í° ìë™ ì¶”ê°€\n",
    "encoded_with_special = tokenizer.encode(text_with_special, add_special_tokens=True)\n",
    "encoded_without_special = tokenizer.encode(text_with_special, add_special_tokens=False)\n",
    "\n",
    "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: {text_with_special}\")\n",
    "print(f\"\\níŠ¹ìˆ˜ í† í° í¬í•¨ (add_special_tokens=True):\")\n",
    "print(f\"  í† í° ID: {encoded_with_special}\")\n",
    "print(f\"  í† í° ê°œìˆ˜: {len(encoded_with_special)}\")\n",
    "print(f\"\\níŠ¹ìˆ˜ í† í° ì œì™¸ (add_special_tokens=False):\")\n",
    "print(f\"  í† í° ID: {encoded_without_special}\")\n",
    "print(f\"  í† í° ê°œìˆ˜: {len(encoded_without_special)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wpanep8q3a",
   "metadata": {},
   "source": [
    "## ì¸ì½”ë”©ê³¼ ë””ì½”ë”©\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ì˜ í•µì‹¬ ê¸°ëŠ¥ì€ í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ë³€í™˜(ì¸ì½”ë”©)í•˜ê³ , IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³µì›(ë””ì½”ë”©)í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dnj0t30v5t",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 3404, 2065, 374, 279, 1176, 3094, 304, 445, 11237, 8863, 13]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __call__ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ì¸ì½”ë”©\n",
    "# ì´ ë°©ë²•ì´ ë” ë§ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤\n",
    "text = \"Tokenization is the first step in LLM processing.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6659c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49 characters, 8 words and 12 tokens\n"
     ]
    }
   ],
   "source": [
    "character_count = len(text)\n",
    "word_count = len(text.split(' '))\n",
    "token_count = len(tokens)\n",
    "print(f\"There are {character_count} characters, {word_count} words and {token_count} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "uz1vj5xcxen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Tokenization is the first step in LLM processing.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21c54e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Tokenization is the first step in LLM processing.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ahyy1zgdwr",
   "metadata": {},
   "source": [
    "## Vocabularyì™€ ê³ ê¸‰ ê°œë…\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ì˜ vocabularyëŠ” ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª¨ë“  í† í°ì˜ ì§‘í•©ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "oaq1xuochv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í† í° ID â†” í…ìŠ¤íŠ¸ ë³€í™˜ ===\n",
      "ID 128000 â†’ í† í°: '<|begin_of_text|>' â†’ í…ìŠ¤íŠ¸: '<|begin_of_text|>'\n",
      "ID 9906 â†’ í† í°: 'Hello' â†’ í…ìŠ¤íŠ¸: 'Hello'\n",
      "ID 11 â†’ í† í°: ',' â†’ í…ìŠ¤íŠ¸: ','\n",
      "ID 1917 â†’ í† í°: 'Ä world' â†’ í…ìŠ¤íŠ¸: ' world'\n"
     ]
    }
   ],
   "source": [
    "# íŠ¹ì • í† í° IDì™€ í…ìŠ¤íŠ¸ ê°„ ë³€í™˜\n",
    "token_ids = [128000, 9906, 11, 1917]  # ì„ì˜ì˜ í† í° IDë“¤\n",
    "\n",
    "print(\"=== í† í° ID â†” í…ìŠ¤íŠ¸ ë³€í™˜ ===\")\n",
    "for token_id in token_ids:\n",
    "    # ID â†’ í† í°\n",
    "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "    # ID â†’ í…ìŠ¤íŠ¸ (ë””ì½”ë”©)\n",
    "    text = tokenizer.decode([token_id])\n",
    "    print(f\"ID {token_id} â†’ í† í°: '{token}' â†’ í…ìŠ¤íŠ¸: '{text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "zzsi1qaxz6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì„œë¸Œì›Œë“œ í† í°í™” ì˜ˆì œ ===\n",
      "\n",
      "ë‹¨ì–´: 'tokenization'\n",
      "  í† í° ê°œìˆ˜: 2\n",
      "  í† í°: ['token', 'ization']\n",
      "\n",
      "ë‹¨ì–´: 'antidisestablishmentarianism'\n",
      "  í† í° ê°œìˆ˜: 6\n",
      "  í† í°: ['ant', 'idis', 'establish', 'ment', 'arian', 'ism']\n",
      "\n",
      "ë‹¨ì–´: 'AI'\n",
      "  í† í° ê°œìˆ˜: 1\n",
      "  í† í°: ['AI']\n",
      "\n",
      "ë‹¨ì–´: 'ğŸ¤–'\n",
      "  í† í° ê°œìˆ˜: 3\n",
      "  í† í°: ['Ã°Å', 'Â¤', 'Ä¸']\n",
      "\n",
      "ë‹¨ì–´: 'cafÃ©'\n",
      "  í† í° ê°œìˆ˜: 2\n",
      "  í† í°: ['ca', 'fÃƒÂ©']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì„œë¸Œì›Œë“œ í† í°í™” ì›ë¦¬ ì´í•´\n",
    "words = [\"tokenization\", \"antidisestablishmentarianism\", \"AI\", \"ğŸ¤–\", \"cafÃ©\"]\n",
    "\n",
    "print(\"=== ì„œë¸Œì›Œë“œ í† í°í™” ì˜ˆì œ ===\\n\")\n",
    "for word in words:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    print(f\"ë‹¨ì–´: '{word}'\")\n",
    "    print(f\"  í† í° ê°œìˆ˜: {len(tokens)}\")\n",
    "    print(f\"  í† í°: {tokens}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45484506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|begin_of_text|>': 128000,\n",
       " '<|end_of_text|>': 128001,\n",
       " '<|reserved_special_token_0|>': 128002,\n",
       " '<|reserved_special_token_1|>': 128003,\n",
       " '<|finetune_right_pad_id|>': 128004,\n",
       " '<|reserved_special_token_2|>': 128005,\n",
       " '<|start_header_id|>': 128006,\n",
       " '<|end_header_id|>': 128007,\n",
       " '<|eom_id|>': 128008,\n",
       " '<|eot_id|>': 128009,\n",
       " '<|python_tag|>': 128010,\n",
       " '<|reserved_special_token_3|>': 128011,\n",
       " '<|reserved_special_token_4|>': 128012,\n",
       " '<|reserved_special_token_5|>': 128013,\n",
       " '<|reserved_special_token_6|>': 128014,\n",
       " '<|reserved_special_token_7|>': 128015,\n",
       " '<|reserved_special_token_8|>': 128016,\n",
       " '<|reserved_special_token_9|>': 128017,\n",
       " '<|reserved_special_token_10|>': 128018,\n",
       " '<|reserved_special_token_11|>': 128019,\n",
       " '<|reserved_special_token_12|>': 128020,\n",
       " '<|reserved_special_token_13|>': 128021,\n",
       " '<|reserved_special_token_14|>': 128022,\n",
       " '<|reserved_special_token_15|>': 128023,\n",
       " '<|reserved_special_token_16|>': 128024,\n",
       " '<|reserved_special_token_17|>': 128025,\n",
       " '<|reserved_special_token_18|>': 128026,\n",
       " '<|reserved_special_token_19|>': 128027,\n",
       " '<|reserved_special_token_20|>': 128028,\n",
       " '<|reserved_special_token_21|>': 128029,\n",
       " '<|reserved_special_token_22|>': 128030,\n",
       " '<|reserved_special_token_23|>': 128031,\n",
       " '<|reserved_special_token_24|>': 128032,\n",
       " '<|reserved_special_token_25|>': 128033,\n",
       " '<|reserved_special_token_26|>': 128034,\n",
       " '<|reserved_special_token_27|>': 128035,\n",
       " '<|reserved_special_token_28|>': 128036,\n",
       " '<|reserved_special_token_29|>': 128037,\n",
       " '<|reserved_special_token_30|>': 128038,\n",
       " '<|reserved_special_token_31|>': 128039,\n",
       " '<|reserved_special_token_32|>': 128040,\n",
       " '<|reserved_special_token_33|>': 128041,\n",
       " '<|reserved_special_token_34|>': 128042,\n",
       " '<|reserved_special_token_35|>': 128043,\n",
       " '<|reserved_special_token_36|>': 128044,\n",
       " '<|reserved_special_token_37|>': 128045,\n",
       " '<|reserved_special_token_38|>': 128046,\n",
       " '<|reserved_special_token_39|>': 128047,\n",
       " '<|reserved_special_token_40|>': 128048,\n",
       " '<|reserved_special_token_41|>': 128049,\n",
       " '<|reserved_special_token_42|>': 128050,\n",
       " '<|reserved_special_token_43|>': 128051,\n",
       " '<|reserved_special_token_44|>': 128052,\n",
       " '<|reserved_special_token_45|>': 128053,\n",
       " '<|reserved_special_token_46|>': 128054,\n",
       " '<|reserved_special_token_47|>': 128055,\n",
       " '<|reserved_special_token_48|>': 128056,\n",
       " '<|reserved_special_token_49|>': 128057,\n",
       " '<|reserved_special_token_50|>': 128058,\n",
       " '<|reserved_special_token_51|>': 128059,\n",
       " '<|reserved_special_token_52|>': 128060,\n",
       " '<|reserved_special_token_53|>': 128061,\n",
       " '<|reserved_special_token_54|>': 128062,\n",
       " '<|reserved_special_token_55|>': 128063,\n",
       " '<|reserved_special_token_56|>': 128064,\n",
       " '<|reserved_special_token_57|>': 128065,\n",
       " '<|reserved_special_token_58|>': 128066,\n",
       " '<|reserved_special_token_59|>': 128067,\n",
       " '<|reserved_special_token_60|>': 128068,\n",
       " '<|reserved_special_token_61|>': 128069,\n",
       " '<|reserved_special_token_62|>': 128070,\n",
       " '<|reserved_special_token_63|>': 128071,\n",
       " '<|reserved_special_token_64|>': 128072,\n",
       " '<|reserved_special_token_65|>': 128073,\n",
       " '<|reserved_special_token_66|>': 128074,\n",
       " '<|reserved_special_token_67|>': 128075,\n",
       " '<|reserved_special_token_68|>': 128076,\n",
       " '<|reserved_special_token_69|>': 128077,\n",
       " '<|reserved_special_token_70|>': 128078,\n",
       " '<|reserved_special_token_71|>': 128079,\n",
       " '<|reserved_special_token_72|>': 128080,\n",
       " '<|reserved_special_token_73|>': 128081,\n",
       " '<|reserved_special_token_74|>': 128082,\n",
       " '<|reserved_special_token_75|>': 128083,\n",
       " '<|reserved_special_token_76|>': 128084,\n",
       " '<|reserved_special_token_77|>': 128085,\n",
       " '<|reserved_special_token_78|>': 128086,\n",
       " '<|reserved_special_token_79|>': 128087,\n",
       " '<|reserved_special_token_80|>': 128088,\n",
       " '<|reserved_special_token_81|>': 128089,\n",
       " '<|reserved_special_token_82|>': 128090,\n",
       " '<|reserved_special_token_83|>': 128091,\n",
       " '<|reserved_special_token_84|>': 128092,\n",
       " '<|reserved_special_token_85|>': 128093,\n",
       " '<|reserved_special_token_86|>': 128094,\n",
       " '<|reserved_special_token_87|>': 128095,\n",
       " '<|reserved_special_token_88|>': 128096,\n",
       " '<|reserved_special_token_89|>': 128097,\n",
       " '<|reserved_special_token_90|>': 128098,\n",
       " '<|reserved_special_token_91|>': 128099,\n",
       " '<|reserved_special_token_92|>': 128100,\n",
       " '<|reserved_special_token_93|>': 128101,\n",
       " '<|reserved_special_token_94|>': 128102,\n",
       " '<|reserved_special_token_95|>': 128103,\n",
       " '<|reserved_special_token_96|>': 128104,\n",
       " '<|reserved_special_token_97|>': 128105,\n",
       " '<|reserved_special_token_98|>': 128106,\n",
       " '<|reserved_special_token_99|>': 128107,\n",
       " '<|reserved_special_token_100|>': 128108,\n",
       " '<|reserved_special_token_101|>': 128109,\n",
       " '<|reserved_special_token_102|>': 128110,\n",
       " '<|reserved_special_token_103|>': 128111,\n",
       " '<|reserved_special_token_104|>': 128112,\n",
       " '<|reserved_special_token_105|>': 128113,\n",
       " '<|reserved_special_token_106|>': 128114,\n",
       " '<|reserved_special_token_107|>': 128115,\n",
       " '<|reserved_special_token_108|>': 128116,\n",
       " '<|reserved_special_token_109|>': 128117,\n",
       " '<|reserved_special_token_110|>': 128118,\n",
       " '<|reserved_special_token_111|>': 128119,\n",
       " '<|reserved_special_token_112|>': 128120,\n",
       " '<|reserved_special_token_113|>': 128121,\n",
       " '<|reserved_special_token_114|>': 128122,\n",
       " '<|reserved_special_token_115|>': 128123,\n",
       " '<|reserved_special_token_116|>': 128124,\n",
       " '<|reserved_special_token_117|>': 128125,\n",
       " '<|reserved_special_token_118|>': 128126,\n",
       " '<|reserved_special_token_119|>': 128127,\n",
       " '<|reserved_special_token_120|>': 128128,\n",
       " '<|reserved_special_token_121|>': 128129,\n",
       " '<|reserved_special_token_122|>': 128130,\n",
       " '<|reserved_special_token_123|>': 128131,\n",
       " '<|reserved_special_token_124|>': 128132,\n",
       " '<|reserved_special_token_125|>': 128133,\n",
       " '<|reserved_special_token_126|>': 128134,\n",
       " '<|reserved_special_token_127|>': 128135,\n",
       " '<|reserved_special_token_128|>': 128136,\n",
       " '<|reserved_special_token_129|>': 128137,\n",
       " '<|reserved_special_token_130|>': 128138,\n",
       " '<|reserved_special_token_131|>': 128139,\n",
       " '<|reserved_special_token_132|>': 128140,\n",
       " '<|reserved_special_token_133|>': 128141,\n",
       " '<|reserved_special_token_134|>': 128142,\n",
       " '<|reserved_special_token_135|>': 128143,\n",
       " '<|reserved_special_token_136|>': 128144,\n",
       " '<|reserved_special_token_137|>': 128145,\n",
       " '<|reserved_special_token_138|>': 128146,\n",
       " '<|reserved_special_token_139|>': 128147,\n",
       " '<|reserved_special_token_140|>': 128148,\n",
       " '<|reserved_special_token_141|>': 128149,\n",
       " '<|reserved_special_token_142|>': 128150,\n",
       " '<|reserved_special_token_143|>': 128151,\n",
       " '<|reserved_special_token_144|>': 128152,\n",
       " '<|reserved_special_token_145|>': 128153,\n",
       " '<|reserved_special_token_146|>': 128154,\n",
       " '<|reserved_special_token_147|>': 128155,\n",
       " '<|reserved_special_token_148|>': 128156,\n",
       " '<|reserved_special_token_149|>': 128157,\n",
       " '<|reserved_special_token_150|>': 128158,\n",
       " '<|reserved_special_token_151|>': 128159,\n",
       " '<|reserved_special_token_152|>': 128160,\n",
       " '<|reserved_special_token_153|>': 128161,\n",
       " '<|reserved_special_token_154|>': 128162,\n",
       " '<|reserved_special_token_155|>': 128163,\n",
       " '<|reserved_special_token_156|>': 128164,\n",
       " '<|reserved_special_token_157|>': 128165,\n",
       " '<|reserved_special_token_158|>': 128166,\n",
       " '<|reserved_special_token_159|>': 128167,\n",
       " '<|reserved_special_token_160|>': 128168,\n",
       " '<|reserved_special_token_161|>': 128169,\n",
       " '<|reserved_special_token_162|>': 128170,\n",
       " '<|reserved_special_token_163|>': 128171,\n",
       " '<|reserved_special_token_164|>': 128172,\n",
       " '<|reserved_special_token_165|>': 128173,\n",
       " '<|reserved_special_token_166|>': 128174,\n",
       " '<|reserved_special_token_167|>': 128175,\n",
       " '<|reserved_special_token_168|>': 128176,\n",
       " '<|reserved_special_token_169|>': 128177,\n",
       " '<|reserved_special_token_170|>': 128178,\n",
       " '<|reserved_special_token_171|>': 128179,\n",
       " '<|reserved_special_token_172|>': 128180,\n",
       " '<|reserved_special_token_173|>': 128181,\n",
       " '<|reserved_special_token_174|>': 128182,\n",
       " '<|reserved_special_token_175|>': 128183,\n",
       " '<|reserved_special_token_176|>': 128184,\n",
       " '<|reserved_special_token_177|>': 128185,\n",
       " '<|reserved_special_token_178|>': 128186,\n",
       " '<|reserved_special_token_179|>': 128187,\n",
       " '<|reserved_special_token_180|>': 128188,\n",
       " '<|reserved_special_token_181|>': 128189,\n",
       " '<|reserved_special_token_182|>': 128190,\n",
       " '<|reserved_special_token_183|>': 128191,\n",
       " '<|reserved_special_token_184|>': 128192,\n",
       " '<|reserved_special_token_185|>': 128193,\n",
       " '<|reserved_special_token_186|>': 128194,\n",
       " '<|reserved_special_token_187|>': 128195,\n",
       " '<|reserved_special_token_188|>': 128196,\n",
       " '<|reserved_special_token_189|>': 128197,\n",
       " '<|reserved_special_token_190|>': 128198,\n",
       " '<|reserved_special_token_191|>': 128199,\n",
       " '<|reserved_special_token_192|>': 128200,\n",
       " '<|reserved_special_token_193|>': 128201,\n",
       " '<|reserved_special_token_194|>': 128202,\n",
       " '<|reserved_special_token_195|>': 128203,\n",
       " '<|reserved_special_token_196|>': 128204,\n",
       " '<|reserved_special_token_197|>': 128205,\n",
       " '<|reserved_special_token_198|>': 128206,\n",
       " '<|reserved_special_token_199|>': 128207,\n",
       " '<|reserved_special_token_200|>': 128208,\n",
       " '<|reserved_special_token_201|>': 128209,\n",
       " '<|reserved_special_token_202|>': 128210,\n",
       " '<|reserved_special_token_203|>': 128211,\n",
       " '<|reserved_special_token_204|>': 128212,\n",
       " '<|reserved_special_token_205|>': 128213,\n",
       " '<|reserved_special_token_206|>': 128214,\n",
       " '<|reserved_special_token_207|>': 128215,\n",
       " '<|reserved_special_token_208|>': 128216,\n",
       " '<|reserved_special_token_209|>': 128217,\n",
       " '<|reserved_special_token_210|>': 128218,\n",
       " '<|reserved_special_token_211|>': 128219,\n",
       " '<|reserved_special_token_212|>': 128220,\n",
       " '<|reserved_special_token_213|>': 128221,\n",
       " '<|reserved_special_token_214|>': 128222,\n",
       " '<|reserved_special_token_215|>': 128223,\n",
       " '<|reserved_special_token_216|>': 128224,\n",
       " '<|reserved_special_token_217|>': 128225,\n",
       " '<|reserved_special_token_218|>': 128226,\n",
       " '<|reserved_special_token_219|>': 128227,\n",
       " '<|reserved_special_token_220|>': 128228,\n",
       " '<|reserved_special_token_221|>': 128229,\n",
       " '<|reserved_special_token_222|>': 128230,\n",
       " '<|reserved_special_token_223|>': 128231,\n",
       " '<|reserved_special_token_224|>': 128232,\n",
       " '<|reserved_special_token_225|>': 128233,\n",
       " '<|reserved_special_token_226|>': 128234,\n",
       " '<|reserved_special_token_227|>': 128235,\n",
       " '<|reserved_special_token_228|>': 128236,\n",
       " '<|reserved_special_token_229|>': 128237,\n",
       " '<|reserved_special_token_230|>': 128238,\n",
       " '<|reserved_special_token_231|>': 128239,\n",
       " '<|reserved_special_token_232|>': 128240,\n",
       " '<|reserved_special_token_233|>': 128241,\n",
       " '<|reserved_special_token_234|>': 128242,\n",
       " '<|reserved_special_token_235|>': 128243,\n",
       " '<|reserved_special_token_236|>': 128244,\n",
       " '<|reserved_special_token_237|>': 128245,\n",
       " '<|reserved_special_token_238|>': 128246,\n",
       " '<|reserved_special_token_239|>': 128247,\n",
       " '<|reserved_special_token_240|>': 128248,\n",
       " '<|reserved_special_token_241|>': 128249,\n",
       " '<|reserved_special_token_242|>': 128250,\n",
       " '<|reserved_special_token_243|>': 128251,\n",
       " '<|reserved_special_token_244|>': 128252,\n",
       " '<|reserved_special_token_245|>': 128253,\n",
       " '<|reserved_special_token_246|>': 128254,\n",
       " '<|reserved_special_token_247|>': 128255}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.vocab\n",
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "febb4bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vocabulary ì •ë³´ ===\n",
      "Vocabulary í¬ê¸°: 128,000\n",
      "ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: 131,072 í† í°\n",
      "\n",
      "ì´ í† í¬ë‚˜ì´ì €ëŠ” 128,000ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary í¬ê¸° í™•ì¸\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"=== Vocabulary ì •ë³´ ===\")\n",
    "print(f\"Vocabulary í¬ê¸°: {vocab_size:,}\")\n",
    "print(f\"ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: {tokenizer.model_max_length:,} í† í°\")\n",
    "print(f\"\\nì´ í† í¬ë‚˜ì´ì €ëŠ” {vocab_size:,}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nzui99xm9ri",
   "metadata": {},
   "source": [
    "## ê²°ë¡ \n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œ ë‹¤ë£¬ ë‚´ìš©:\n",
    "\n",
    "1. âœ… **í† í¬ë‚˜ì´ì €ì˜ ê¸°ë³¸ ê°œë…**: í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ì—­í• \n",
    "2. âœ… **í† í°í™” ê³¼ì •**: tokenize, encode, decode ë©”ì„œë“œ ì‚¬ìš©ë²•\n",
    "3. âœ… **íŠ¹ìˆ˜ í† í°**: BOS, EOS, PAD, UNKì˜ ì—­í• ê³¼ ì‚¬ìš©\n",
    "4. âœ… **ë°°ì¹˜ ì²˜ë¦¬**: paddingê³¼ attention maskë¥¼ í™œìš©í•œ íš¨ìœ¨ì ì¸ ì²˜ë¦¬\n",
    "5. âœ… **ê³ ê¸‰ ê°œë…**: vocabulary, truncation, ì„œë¸Œì›Œë“œ í† í°í™”\n",
    "6. âœ… **ëª¨ë¸ ë¹„êµ**: ì„œë¡œ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ì˜ íŠ¹ì„± ë¹„êµ\n",
    "7. âœ… **ì‹¤ì „ í™œìš©**: API ë¹„ìš© ê³„ì‚°, ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬\n",
    "\n",
    "### í•µì‹¬ í¬ì¸íŠ¸\n",
    "\n",
    "- í† í¬ë‚˜ì´ì €ëŠ” LLMì˜ \"ì–¸ì–´\"ë¥¼ ì •ì˜í•©ë‹ˆë‹¤\n",
    "- ê°™ì€ í…ìŠ¤íŠ¸ë„ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í† í°í™”ë©ë‹ˆë‹¤\n",
    "- í† í° ìˆ˜ëŠ” ë¹„ìš©ê³¼ ì„±ëŠ¥ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤\n",
    "- ì‹¤ë¬´ì—ì„œëŠ” í•­ìƒ í† í° ìˆ˜ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dkb2g0drbbu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬\n",
    "def check_context_limit(text, max_tokens=4096):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ê°€ ì»¨í…ìŠ¤íŠ¸ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸\n",
    "    \n",
    "    Args:\n",
    "        text: í™•ì¸í•  í…ìŠ¤íŠ¸\n",
    "        max_tokens: ìµœëŒ€ í† í° ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        ì´ˆê³¼ ì—¬ë¶€ì™€ ì •ë³´\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    num_tokens = len(tokens)\n",
    "    is_over = num_tokens > max_tokens\n",
    "    \n",
    "    return {\n",
    "        'num_tokens': num_tokens,\n",
    "        'max_tokens': max_tokens,\n",
    "        'is_over_limit': is_over,\n",
    "        'remaining': max_tokens - num_tokens,\n",
    "        'percentage': (num_tokens / max_tokens) * 100\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "long_text = \"This is a test sentence. \" * 200\n",
    "result = check_context_limit(long_text, max_tokens=128)\n",
    "\n",
    "print(\"=== ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì²´í¬ ===\")\n",
    "print(f\"í† í° ìˆ˜: {result['num_tokens']}\")\n",
    "print(f\"ìµœëŒ€ í—ˆìš©: {result['max_tokens']}\")\n",
    "print(f\"ì œí•œ ì´ˆê³¼: {'ì˜ˆ' if result['is_over_limit'] else 'ì•„ë‹ˆì˜¤'}\")\n",
    "print(f\"ì‚¬ìš©ë¥ : {result['percentage']:.1f}%\")\n",
    "\n",
    "if result['is_over_limit']:\n",
    "    print(f\"âš ï¸  {abs(result['remaining'])} í† í° ì´ˆê³¼! í…ìŠ¤íŠ¸ë¥¼ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"âœ“ {result['remaining']} í† í° ì—¬ìœ  ìˆìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r6v511j4ry8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncation ì˜ˆì œ: ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬\n",
    "long_text = \"Large Language Models \" * 100  # ë§¤ìš° ê¸´ ë°˜ë³µ í…ìŠ¤íŠ¸\n",
    "\n",
    "# truncation ì—†ì´ (ê²½ê³  ë°œìƒ ê°€ëŠ¥)\n",
    "encoded_no_trunc = tokenizer(long_text, truncation=False)\n",
    "print(f\"=== Truncation í…ŒìŠ¤íŠ¸ ===\")\n",
    "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(long_text)} ë¬¸ì\")\n",
    "print(f\"Truncation ì—†ìŒ: {len(encoded_no_trunc['input_ids'])} í† í°\")\n",
    "\n",
    "# truncation ì‚¬ìš© (ìµœëŒ€ ê¸¸ì´ë¡œ ìë¦„)\n",
    "encoded_with_trunc = tokenizer(long_text, truncation=True, max_length=50)\n",
    "print(f\"Truncation ì‚¬ìš© (max_length=50): {len(encoded_with_trunc['input_ids'])} í† í°\")\n",
    "print(f\"\\nëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ë‚´ì•¼ í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kfure9l0h2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vocabulary ì •ë³´ ===\n",
      "Vocabulary í¬ê¸°: 128,000\n",
      "ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: 131,072 í† í°\n",
      "\n",
      "ì´ í† í¬ë‚˜ì´ì €ëŠ” 128,000ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary í¬ê¸° í™•ì¸\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"=== Vocabulary ì •ë³´ ===\")\n",
    "print(f\"Vocabulary í¬ê¸°: {vocab_size:,}\")\n",
    "print(f\"ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: {tokenizer.model_max_length:,} í† í°\")\n",
    "print(f\"\\nì´ í† í¬ë‚˜ì´ì €ëŠ” {vocab_size:,}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dqbubdqivw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ í…ìŠ¤íŠ¸: Large Language Models are transforming AI!\n",
      "ì¸ì½”ë”© ê²°ê³¼ (í† í° ID): [128000, 35353, 11688, 27972, 527, 46890, 15592, 0]\n",
      "ë””ì½”ë”© ê²°ê³¼: <|begin_of_text|>Large Language Models are transforming AI!\n",
      "\n",
      "ì›ë³¸ê³¼ ë™ì¼?: False\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ ì¸ì½”ë”©/ë””ì½”ë”©\n",
    "text = \"Large Language Models are transforming AI!\"\n",
    "\n",
    "# ì¸ì½”ë”©: í…ìŠ¤íŠ¸ â†’ í† í° ID\n",
    "encoded = tokenizer.encode(text)\n",
    "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: {text}\")\n",
    "print(f\"ì¸ì½”ë”© ê²°ê³¼ (í† í° ID): {encoded}\")\n",
    "\n",
    "# ë””ì½”ë”©: í† í° ID â†’ í…ìŠ¤íŠ¸\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"ë””ì½”ë”© ê²°ê³¼: {decoded}\")\n",
    "print(f\"\\nì›ë³¸ê³¼ ë™ì¼?: {text == decoded.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989157a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
